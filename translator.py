# translator.py - æ ¸å¿ƒç¿»è¯‘å™¨ç±»

import re
import time
import difflib
import requests
import pandas as pd
import streamlit as st
import jieba


class MultiAPIExcelTranslator:
    def __init__(self, api_key, api_provider, api_url, model, context_size=10, max_retries=10):
        self.api_key = api_key
        self.api_provider = api_provider
        self.api_url = api_url
        self.model = model
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.context_history = {}  # æŒ‰è¯­è¨€åˆ†åˆ«å­˜å‚¨ä¸Šä¸‹æ–‡
        self.term_dict = {}
        self.role_column = None
        self.context_size = context_size
        self.max_retries = max_retries

        # æ”¹ä¸ºæŒ‰è¯­è¨€å­˜å‚¨æœ¯è¯­åº“
        self.term_base_dict = {}  # {è¯­è¨€: [{source: xxx, target: xxx}]}
        self.term_base_list = []  # å•è¯­è¨€æœ¯è¯­åº“åˆ—è¡¨

        self.role_personality_dict = {}
        self.current_text_terms = {}
        self.current_role_personality = None
        self.target_languages = ["è‹±æ–‡"]
        self.language_column_names = {"è‹±æ–‡": "è‹±æ–‡ç¿»è¯‘ç»“æœ"}
        self.target_language = "è‹±æ–‡"

        # æ–°å¢ï¼šè§’è‰²æ˜ å°„è¡¨
        self.role_mapping = {}
        self.enable_fuzzy_match = False
        self.fuzzy_threshold = 0.6

        self.init_chinese_tokenizer()

    def init_chinese_tokenizer(self):
        try:
            self.chinese_tokenizer = jieba
            st.success("âœ… ä¸­æ–‡åˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            st.warning(f"âš ï¸ ä¸­æ–‡åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.chinese_tokenizer = None

    def tokenize_chinese_text(self, text):
        if not text or pd.isna(text):
            return []

        text = str(text).strip()
        if not text:
            return []

        try:
            if self.chinese_tokenizer:
                words = self.chinese_tokenizer.cut(text)
                return [word for word in words if word.strip() and re.search(r'[\w\u4e00-\u9fa5]', word)]
            else:
                return [char for char in text if char.strip() and re.search(r'[\w\u4e00-\u9fa5]', char)]
        except Exception as e:
            st.warning(f"âš ï¸ ä¸­æ–‡åˆ†è¯å¤±è´¥: {e}")
            return [char for char in text if char.strip()]

    def clean_role_name(self, role_name):
        """æ¸…ç†è§’è‰²åç§°ï¼šå»é™¤é¢å¤–æ ‡è®°ã€ç©ºæ ¼ç­‰"""
        if not role_name or pd.isna(role_name):
            return ""

        role_name = str(role_name).strip()
        role_name = re.sub(r'\|.*$', '', role_name)
        role_name = re.sub(r'[\s\u3000]+', '', role_name)
        role_name = role_name.strip()

        return role_name

    def fuzzy_match_role(self, role_name, threshold=None):
        """æ¨¡ç³ŠåŒ¹é…è§’è‰²åç§°"""
        if not role_name or not self.role_personality_dict:
            return None, 0

        if threshold is None:
            threshold = self.fuzzy_threshold

        cleaned_role = self.clean_role_name(role_name)
        if not cleaned_role:
            return None, 0

        if role_name in self.role_mapping:
            return self.role_mapping[role_name], 1.0

        best_match = None
        best_score = 0

        for official_role in self.role_personality_dict.keys():
            cleaned_official = self.clean_role_name(official_role)

            if cleaned_role == cleaned_official:
                return official_role, 1.0

            score = difflib.SequenceMatcher(None, cleaned_role, cleaned_official).ratio()

            if cleaned_role in cleaned_official or cleaned_official in cleaned_role:
                score = max(score, 0.8)

            if score > best_score:
                best_score = score
                best_match = official_role

        if best_score >= threshold:
            return best_match, best_score

        return None, best_score

    def analyze_role_matches(self, df, role_col):
        """åˆ†ææ•°æ®ä¸­çš„æ‰€æœ‰è§’è‰²åç§°"""
        if not role_col or role_col not in df.columns:
            return {}

        unique_roles = df[role_col].dropna().unique()
        fuzzy_matches = {}

        for role in unique_roles:
            role_str = str(role).strip()
            if not role_str:
                continue

            if role_str in self.role_mapping:
                continue

            if role_str in self.role_personality_dict:
                continue

            matched_role, score = self.fuzzy_match_role(role_str)

            if matched_role:
                if role_str not in fuzzy_matches:
                    fuzzy_matches[role_str] = []
                fuzzy_matches[role_str].append((matched_role, score))

                if score < 1.0:
                    for official_role in self.role_personality_dict.keys():
                        if official_role == matched_role:
                            continue
                        alt_score = difflib.SequenceMatcher(
                            None,
                            self.clean_role_name(role_str),
                            self.clean_role_name(official_role)
                        ).ratio()

                        if alt_score >= self.fuzzy_threshold * 0.8:
                            fuzzy_matches[role_str].append((official_role, alt_score))

                fuzzy_matches[role_str].sort(key=lambda x: x[1], reverse=True)

        return fuzzy_matches

    def find_role_personality(self, role_name):
        """æŸ¥æ‰¾è§’è‰²æ€§æ ¼æè¿°"""
        if not role_name or not self.role_personality_dict:
            return None

        role_name = str(role_name).strip()
        if not role_name:
            return None

        if role_name in self.role_mapping:
            mapped_role = self.role_mapping[role_name]
            return self.role_personality_dict.get(mapped_role)

        if role_name in self.role_personality_dict:
            return self.role_personality_dict[role_name]

        if self.enable_fuzzy_match:
            matched_role, score = self.fuzzy_match_role(role_name)
            if matched_role:
                return self.role_personality_dict[matched_role]

        return None

    def add_to_context(self, original, translation, role=None, language="è‹±æ–‡"):
        """ä¸ºæŒ‡å®šè¯­è¨€æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆç¡®ä¿è¯­è¨€ç‹¬ç«‹ï¼‰"""
        if language not in self.context_history:
            self.context_history[language] = []

        self.context_history[language].append((original, translation, role))
        if len(self.context_history[language]) > self.context_size:
            self.context_history[language].pop(0)

    def build_context_prompt(self, language="è‹±æ–‡"):
        """ä¸ºæŒ‡å®šè¯­è¨€æ„å»ºä¸Šä¸‹æ–‡æç¤ºï¼ˆç¡®ä¿è¯­è¨€ç‹¬ç«‹ï¼‰"""
        if language not in self.context_history or not self.context_history[language]:
            return ""

        context_str = f"\n\n### é‡è¦ä¸Šä¸‹æ–‡å‚è€ƒï¼ˆ{language}ç¿»è¯‘ï¼‰ï¼š\n"
        for i, (orig, trans, role) in enumerate(self.context_history[language], 1):
            role_info = f" [{role}]" if role else ""
            context_str += f"å‰æ–‡{i}{role_info}:\nåŸæ–‡: {orig}\n{language}è¯‘æ–‡: {trans}\n\n"
        return context_str

    def find_matched_terms(self, text, language):
        """ä¸ºæŒ‡å®šè¯­è¨€æŸ¥æ‰¾åŒ¹é…çš„æœ¯è¯­"""
        if language not in self.term_base_dict or not self.term_base_dict[language]:
            return {}

        words = self.tokenize_chinese_text(text)
        matched_terms = {}

        # è¯çº§åˆ«åŒ¹é…
        for word in words:
            for term_entry in self.term_base_dict[language]:
                if term_entry['source'] == word:
                    if word not in matched_terms:
                        matched_terms[word] = []
                    if term_entry['target'] not in matched_terms[word]:
                        matched_terms[word].append(term_entry['target'])

        # çŸ­è¯­çº§åˆ«åŒ¹é…
        for term_entry in self.term_base_dict[language]:
            term = term_entry['source']
            if term in text:
                if term not in matched_terms:
                    matched_terms[term] = []
                if term_entry['target'] not in matched_terms[term]:
                    matched_terms[term].append(term_entry['target'])

        return matched_terms

    def build_term_base_prompt(self, text, language="è‹±æ–‡"):
        """ä¸ºæŒ‡å®šè¯­è¨€æ„å»ºæœ¯è¯­åº“æç¤º"""
        # å°è¯•ä½¿ç”¨å¤šè¯­è¨€æœ¯è¯­åº“
        matched_terms = self.find_matched_terms(text, language)

        # å¦‚æœæ²¡æœ‰å¤šè¯­è¨€æœ¯è¯­åº“ï¼Œä½¿ç”¨å•è¯­è¨€æœ¯è¯­åº“
        if not matched_terms and self.term_base_list:
            words = self.tokenize_chinese_text(text)
            matched_terms = {}

            for word in words:
                for term_entry in self.term_base_list:
                    if term_entry['source'] == word:
                        if word not in matched_terms:
                            matched_terms[word] = []
                        if term_entry['target'] not in matched_terms[word]:
                            matched_terms[word].append(term_entry['target'])

            for term_entry in self.term_base_list:
                term = term_entry['source']
                if term in text:
                    if term not in matched_terms:
                        matched_terms[term] = []
                    if term_entry['target'] not in matched_terms[term]:
                        matched_terms[term].append(term_entry['target'])

        if not matched_terms:
            return ""

        term_base_str = f"\n\n### æœ¯è¯­åº“åŒ¹é…ï¼š\n"

        for orig, trans_list in matched_terms.items():
            if len(trans_list) == 1:
                term_base_str += f"- ã€Œ{orig}ã€ â†’ {language}è¯‘åï¼šã€Œ{trans_list[0]}ã€\n"
            else:
                term_base_str += f"- ã€Œ{orig}ã€ â†’ {language}è¯‘åå€™é€‰ï¼š{' / '.join([f'ã€Œ{t}ã€' for t in trans_list])} ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„ï¼‰\n"

        return term_base_str

    def build_role_personality_prompt(self, role_name):
        if not role_name:
            return ""

        personality = self.find_role_personality(role_name)
        self.current_role_personality = personality

        if not personality:
            return ""

        mapped_role = self.role_mapping.get(str(role_name).strip())
        if mapped_role and mapped_role != str(role_name).strip():
            role_personality_str = f"\n\n### è§’è‰²æ€§æ ¼æè¿°ï¼š\nè§’è‰²ã€Œ{role_name}ã€(æ˜ å°„ä¸ºã€Œ{mapped_role}ã€)çš„æ€§æ ¼ç‰¹ç‚¹ï¼š{personality}\n"
        else:
            role_personality_str = f"\n\n### è§’è‰²æ€§æ ¼æè¿°ï¼š\nè§’è‰²ã€Œ{role_name}ã€çš„æ€§æ ¼ç‰¹ç‚¹ï¼š{personality}\n"

        return role_personality_str

    def set_target_languages(self, languages, column_names):
        """è®¾ç½®ç›®æ ‡è¯­è¨€åˆ—è¡¨å’Œå¯¹åº”çš„åˆ—å"""
        self.target_languages = languages
        self.language_column_names = column_names
        # ä¸ºæ¯ç§è¯­è¨€åˆå§‹åŒ–ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡å†å²
        for lang in languages:
            if lang not in self.context_history:
                self.context_history[lang] = []

    def set_target_language(self, language):
        self.target_language = language

    def get_language_specific_requirements(self, language):
        language_requirements = {
            "è‹±æ–‡": """
è‹±æ–‡ç¿»è¯‘è¦æ±‚ï¼š
- ä¿æŒè‡ªç„¶æµç•…ï¼Œç¬¦åˆè‹±è¯­æ¯è¯­è€…çš„è¡¨è¾¾ä¹ æƒ¯
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
""",
            "æ—¥æ–‡": """
æ—¥æ–‡ç¿»è¯‘è¦æ±‚ï¼š
- æ³¨æ„æ•¬ä½“å’Œå¸¸ä½“çš„ä½¿ç”¨ï¼Œæ ¹æ®è§’è‰²å…³ç³»å’Œåœºæ™¯é€‰æ‹©åˆé€‚çš„è¯­ä½“
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œç¬¦åˆæ—¥è¯­è¡¨è¾¾ä¹ æƒ¯
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”å’Œè¯­å°¾
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
""",
            "éŸ©æ–‡": """
éŸ©æ–‡ç¿»è¯‘è¦æ±‚ï¼š
- æ³¨æ„å°Šæ•¬è¯­å’Œéå°Šæ•¬è¯­çš„ä½¿ç”¨ï¼Œæ ¹æ®è§’è‰²å…³ç³»å’Œåœºæ™¯é€‰æ‹©åˆé€‚çš„è¯­ä½“
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œç¬¦åˆéŸ©è¯­è¡¨è¾¾ä¹ æƒ¯
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
"""
        }

        if language in language_requirements:
            return language_requirements[language]
        else:
            return f"""
{language}ç¿»è¯‘è¦æ±‚ï¼š
- æ³¨æ„æ­£å¼å’Œéæ­£å¼è¯­ä½“çš„ä½¿ç”¨ï¼Œæ ¹æ®è§’è‰²å…³ç³»å’Œåœºæ™¯é€‰æ‹©åˆé€‚çš„è¯­ä½“
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œç¬¦åˆ{language}è¡¨è¾¾ä¹ æƒ¯
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
"""

    def is_translation_error(self, response_text, original_text):
        if not response_text or response_text.strip() == "":
            return True
        if len(response_text) < len(original_text) * 0.1:
            return True
        return False

    def translate_text_with_retry(self, text, target_language, custom_requirements="", role=None):
        if not text or pd.isna(text) or str(text).strip() == "":
            return text

        last_exception = None

        for attempt in range(self.max_retries):
            try:
                translated_text = self._translate_single_attempt(text, target_language, custom_requirements, role)

                if not self.is_translation_error(translated_text, text):
                    return translated_text
                else:
                    st.warning(f"âš ï¸ [{target_language}] ç¬¬ {attempt + 1} æ¬¡ç¿»è¯‘ç»“æœå¼‚å¸¸ï¼Œå‡†å¤‡é‡è¯•...")

            except requests.exceptions.RequestException as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] ç½‘ç»œé”™è¯¯ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            except requests.exceptions.Timeout as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] è¯·æ±‚è¶…æ—¶ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            except requests.exceptions.ConnectionError as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] è¿æ¥é”™è¯¯ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            except Exception as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] APIé”™è¯¯ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            if attempt < self.max_retries - 1:
                wait_time = min(2 ** attempt, 60)
                time.sleep(wait_time)

        st.error(f"âŒ [{target_language}] ç¿»è¯‘å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.max_retries}")
        if last_exception:
            st.error(f"æœ€åé”™è¯¯: {last_exception}")

        return text

    def _translate_single_attempt(self, text, target_language, custom_requirements="", role=None):
        # ä¸ºå½“å‰è¯­è¨€æ„å»ºç‹¬ç«‹çš„ä¸Šä¸‹æ–‡å’Œæœ¯è¯­æç¤º
        context_prompt = self.build_context_prompt(target_language)
        term_base_prompt = self.build_term_base_prompt(text, target_language)
        role_personality_prompt = self.build_role_personality_prompt(role) if role else ""
        language_requirements = self.get_language_specific_requirements(target_language)

        role_prompt = ""
        if role and not pd.isna(role) and str(role).strip() != "":
            role_prompt = f"\nå½“å‰æ–‡æœ¬çš„è¯´è¯äºº: {role}\n"

        prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ã€‚

## è§’è‰²ä¿¡æ¯ï¼š
{role_prompt}{role_personality_prompt}

## {target_language}ç¿»è¯‘è§„èŒƒï¼ˆä¼˜å…ˆçº§æœ€ä½ï¼‰ï¼š
{language_requirements}

## ç”¨æˆ·è‡ªå®šä¹‰è¦æ±‚ï¼ˆä¼˜å…ˆçº§ç¬¬ä¸€é«˜ï¼‰ï¼š
{custom_requirements}

{context_prompt}

{term_base_prompt}

## å¾…ç¿»è¯‘æ–‡æœ¬ï¼š
{text}

## é‡è¦è¯´æ˜ï¼ˆä¼˜å…ˆçº§ç¬¬äºŒé«˜ï¼‰ï¼š
1. è¯·åªè¿”å›{target_language}ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å¤‡æ³¨
2. æœ¯è¯­åº“ä¸­çš„ç‰¹å®šè¯æ±‡ç¿»è¯‘ï¼Œå¦‚æœæ˜¯äººåæˆ–è€…å›ºå®šç‰¹æ®Šåç§°éœ€è¦ä¸¥æ ¼é‡‡ç”¨ç›¸åŒçš„ç¿»è¯‘,ä½†æ³¨æ„å¦‚æœæ˜¯ä¸€äº›æ™®é€šçš„è¯æ±‡åˆ™çœ‹å¥å­ç¿»è¯‘ä¸å¿…ä¸€å®šæŒ‰ç…§æœ¯è¯­åº“æ¥
3. å¦‚æœä¸€ä¸ªæœ¯è¯­æœ‰å¤šä¸ªå€™é€‰è¯‘åï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„
4. è¯·æ ¹æ®è§’è‰²æ€§æ ¼æè¿°è°ƒæ•´ç¿»è¯‘é£æ ¼å’Œè¯­æ°”
5. å‚è€ƒä¸Šä¸‹æ–‡ä¸­çš„{target_language}è¯‘æ–‡é£æ ¼ï¼Œä¿æŒç¿»è¯‘ä¸€è‡´æ€§ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™ä¸Šä¸‹æ–‡è§’è‰²å¯èƒ½ä¸æ˜¯ä¸€ä¸ªäººæˆ–è€…åªæ˜¯UIç¿»è¯‘ï¼Œä½ è¿˜æ˜¯éœ€è¦å‚è€ƒåŸæ–‡åˆ¤æ–­æ˜¯å¦å‚è€ƒä¸Šä¸‹æ–‡

{target_language}ç¿»è¯‘ç»“æœï¼š
"""

        data = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": f"ä½ æ˜¯ä¸€åä¸“ä¸šçš„{target_language}ç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿æ¸¸æˆæœ¬åœ°åŒ–ã€UIç•Œé¢ç¿»è¯‘å’Œè§’è‰²æ–‡æ¡ˆç¿»è¯‘ã€‚ä½ æ­£åœ¨è¿›è¡Œä¸­æ–‡åˆ°{target_language}çš„ç¿»è¯‘å·¥ä½œã€‚è¯·ç¡®ä¿æœ¯è¯­ä¸€è‡´æ€§å’Œé£æ ¼ç»Ÿä¸€ï¼Œå¹¶æ ¹æ®è§’è‰²ç‰¹ç‚¹è°ƒæ•´ç¿»è¯‘é£æ ¼ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.2,
            "max_tokens": 4000
        }
        print(str(prompt))
        response = requests.post(self.api_url, headers=self.headers, json=data, timeout=60)
        response.raise_for_status()
        result = response.json()
        translated_text = result["choices"][0]["message"]["content"].strip()

        translated_text = self.clean_translation(translated_text)
        # å°†ç¿»è¯‘ç»“æœæ·»åŠ åˆ°è¯¥è¯­è¨€çš„ç‹¬ç«‹ä¸Šä¸‹æ–‡ä¸­
        self.add_to_context(text, translated_text, role, target_language)

        return translated_text

    def translate_text(self, text, target_language, custom_requirements="", role=None):
        return self.translate_text_with_retry(text, target_language, custom_requirements, role)

    def clean_translation(self, text):
        if text.startswith('"') and text.endswith('"'):
            text = text[1:-1]
        elif text.startswith("'") and text.endswith("'"):
            text = text[1:-1]

        prefixes = [
            "moniorï¼š", "monior:", "è§’è‰²ï¼š", "è§’è‰²:",
            "ç¿»è¯‘ï¼š", "ç¿»è¯‘:", "è¯‘æ–‡ï¼š", "è¯‘æ–‡:",
            "ç»“æœï¼š", "ç»“æœ:", "ç¿»è¯‘ç»“æœï¼š", "ç¿»è¯‘ç»“æœ:",
            "è‹±æ–‡ç¿»è¯‘ç»“æœï¼š", "æ—¥æ–‡ç¿»è¯‘ç»“æœï¼š", "éŸ©æ–‡ç¿»è¯‘ç»“æœï¼š",
            "è‹±æ–‡ï¼š", "æ—¥æ–‡ï¼š", "éŸ©æ–‡ï¼š", "æ³•æ–‡ï¼š", "å¾·æ–‡ï¼š",
        ]
        for prefix in prefixes:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()

        return text

    def reset_context(self):
        self.context_history = {}
        self.term_dict = {}
        self.term_base_dict = {}
        self.role_column = None
        self.current_text_terms = {}
        self.current_role_personality = None
        self.role_mapping = {}

    def load_term_base(self, df, source_col, target_col):
        """åŠ è½½æœ¯è¯­åº“ - æ”¯æŒé‡å¤æœ¯è¯­"""
        try:
            # æ„å»ºæœ¯è¯­åº“åˆ—è¡¨ï¼Œæ”¯æŒé‡å¤æœ¯è¯­
            self.term_base_list = []
            missing_count = 0

            for _, row in df.iterrows():
                source = row[source_col]
                target = row[target_col]

                if pd.isna(source) or pd.isna(target):
                    missing_count += 1
                    continue

                source = str(source).strip()
                target = str(target).strip()

                if source and target:
                    # ä¸å†æ£€æŸ¥é‡å¤ï¼Œç›´æ¥æ·»åŠ åˆ°åˆ—è¡¨
                    self.term_base_list.append({
                        'source': source,
                        'target': target
                    })

            st.success(f"âœ… æˆåŠŸåŠ è½½æœ¯è¯­: {len(self.term_base_list)} æ¡")
            if missing_count > 0:
                st.warning(f"âš ï¸ è·³è¿‡ {missing_count} æ¡ä¸å®Œæ•´çš„è®°å½•")

            return True
        except Exception as e:
            st.error(f"âŒâŒ åŠ è½½æœ¯è¯­åº“å¤±è´¥: {e}")
            return False

    def load_term_base_multilang(self, df, source_col, target_cols_dict):
        """
        åŠ è½½å¤šè¯­è¨€æœ¯è¯­åº“
        df: æœ¯è¯­åº“DataFrame
        source_col: åŸæ–‡åˆ—å
        target_cols_dict: {è¯­è¨€: åˆ—å} ä¾‹å¦‚ {"è‹±æ–‡": "English", "æ—¥æ–‡": "Japanese"}
        """
        try:
            self.term_base_dict = {}

            for language, target_col in target_cols_dict.items():
                if target_col not in df.columns:
                    st.warning(f"âš ï¸ æœ¯è¯­åº“ä¸­æœªæ‰¾åˆ° {language} å¯¹åº”çš„åˆ—: {target_col}")
                    continue

                self.term_base_dict[language] = []
                missing_count = 0

                for _, row in df.iterrows():
                    source = row[source_col]
                    target = row[target_col]

                    if pd.isna(source) or pd.isna(target):
                        missing_count += 1
                        continue

                    source = str(source).strip()
                    target = str(target).strip()

                    if source and target:
                        # å°†åŸæ–‡æ·»åŠ åˆ°åˆ†è¯è¯å…¸
                        try:
                            self.chinese_tokenizer.add_word(source)
                        except:
                            pass

                        self.term_base_dict[language].append({
                            'source': source,
                            'target': target
                        })

                st.success(f"âœ… {language} æœ¯è¯­åŠ è½½æˆåŠŸ: {len(self.term_base_dict[language])} æ¡")
                if missing_count > 0:
                    st.info(f"   è·³è¿‡ {missing_count} æ¡ä¸å®Œæ•´çš„ {language} æœ¯è¯­")

            # æ˜¾ç¤ºæœ¯è¯­åº“ç»Ÿè®¡
            total_terms = sum(len(terms) for terms in self.term_base_dict.values())
            st.success(f"ğŸ“Š æ€»è®¡åŠ è½½æœ¯è¯­: {total_terms} æ¡ï¼Œè¦†ç›– {len(self.term_base_dict)} ç§è¯­è¨€")

            # æ˜¾ç¤ºæœ¯è¯­ç¤ºä¾‹
            with st.expander("ğŸ“‹ æŸ¥çœ‹æœ¯è¯­åº“ç¤ºä¾‹"):
                for language, terms in self.term_base_dict.items():
                    if terms:
                        st.write(f"**{language}æœ¯è¯­ç¤ºä¾‹ï¼š**")
                        for i, term in enumerate(terms[:5]):
                            st.write(f"  {i+1}. {term['source']} â†’ {term['target']}")
                        if len(terms) > 5:
                            st.write(f"  ... è¿˜æœ‰ {len(terms)-5} æ¡")

            return True

        except Exception as e:
            st.error(f"âŒ åŠ è½½å¤šè¯­è¨€æœ¯è¯­åº“å¤±è´¥: {e}")
            import traceback
            st.error(traceback.format_exc())
            return False

    def load_role_personality(self, df, role_col, personality_col):
        try:
            self.role_personality_dict = {}
            missing_count = 0

            for _, row in df.iterrows():
                role = row[role_col]
                personality = row[personality_col]

                if pd.isna(role) or pd.isna(personality):
                    missing_count += 1
                    continue

                role = str(role).strip()
                personality = str(personality).strip()

                if role and personality:
                    self.role_personality_dict[role] = personality

            st.success(f"âœ… æˆåŠŸåŠ è½½è§’è‰²æ€§æ ¼: {len(self.role_personality_dict)} æ¡")
            if missing_count > 0:
                st.warning(f"âš ï¸ è·³è¿‡ {missing_count} æ¡ä¸å®Œæ•´çš„è®°å½•")

            self.analyze_role_personality()

            return True
        except Exception as e:
            st.error(f"âŒ åŠ è½½è§’è‰²æ€§æ ¼åº“å¤±è´¥: {e}")
            return False

    def analyze_role_personality(self):
        if not self.role_personality_dict:
            return

        st.write(f"ğŸ“Š è§’è‰²æ€§æ ¼åº“ç»Ÿè®¡: {len(self.role_personality_dict)} ä¸ªè§’è‰²")

        st.write("ğŸ“‹ éƒ¨åˆ†è§’è‰²æ€§æ ¼é¢„è§ˆ:")
        count = 0
        for role, personality in list(self.role_personality_dict.items())[:5]:
            st.write(f"  - {role}: {personality[:50]}..." if len(personality) > 50 else f"  - {role}: {personality}")
            count += 1
            if count >= 5:
                break
