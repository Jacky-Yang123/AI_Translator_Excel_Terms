import os
import time
import threading
from pathlib import Path
from io import BytesIO
import zipfile
import concurrent.futures

import pandas as pd
import streamlit as st


progress_lock = threading.Lock()


def load_single_file(file_path):
    """åŠ è½½å•ä¸ªæ–‡ä»¶ï¼ˆExcelæˆ–CSVï¼‰"""
    try:
        if file_path.suffix.lower() in ['.xlsx', '.xls', '.xlsm']:
            excel_file = pd.read_excel(file_path, sheet_name=None)
            results = {}
            for sheet_name, df in excel_file.items():
                if not df.empty:
                    key = f"{file_path.name} - {sheet_name}"
                    results[key] = {
                        'dataframe': df,
                        'file_path': file_path,
                        'sheet_name': sheet_name,
                        'file_type': 'excel'
                    }
            return results
        elif file_path.suffix.lower() == '.csv':
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is not None and not df.empty:
                return {
                    file_path.name: {
                        'dataframe': df,
                        'file_path': file_path,
                        'sheet_name': 'CSV',
                        'file_type': 'csv'
                    }
                }
    except Exception as e:
        st.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
    
    return {}


def load_all_files_parallel(folder_path, max_workers=4):
    """å¹¶è¡ŒåŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰Excelå’ŒCSVæ–‡ä»¶"""
    all_files = {}
    folder_path = Path(folder_path)
    
    file_paths = []
    for pattern in ['*.xlsx', '*.xls', '*.xlsm', '*.csv']:
        file_paths.extend(folder_path.rglob(pattern))
    
    if not file_paths:
        return all_files
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_path = {executor.submit(load_single_file, path): path for path in file_paths}
        
        for future in concurrent.futures.as_completed(future_to_path):
            try:
                result = future.result()
                all_files.update(result)
            except Exception as e:
                path = future_to_path[future]
                st.warning(f"å¤„ç†æ–‡ä»¶ {path} æ—¶å‡ºé”™: {str(e)}")
    
    return all_files


def load_source_files_parallel(folder_path, max_workers=4):
    """å¹¶è¡ŒåŠ è½½æºæ–‡ä»¶å¤¹ä¸­çš„Excelå’ŒCSVæ–‡ä»¶"""
    source_files = {}
    folder_path = Path(folder_path)
    
    file_paths = []
    for pattern in ['*.xlsx', '*.xls', '*.xlsm', '*.csv']:
        file_paths.extend(folder_path.glob(pattern))
    
    if not file_paths:
        return source_files
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_path = {executor.submit(load_single_file, path): path for path in file_paths}
        
        for future in concurrent.futures.as_completed(future_to_path):
            try:
                result = future.result()
                source_files.update(result)
            except Exception as e:
                path = future_to_path[future]
                st.error(f"å¤„ç†æ–‡ä»¶ {path} æ—¶å‡ºé”™: {str(e)}")
    
    return source_files


def find_matching_text(search_text, files_dict, source_col, target_col, match_strategy, similarity_threshold):
    """åœ¨æ–‡ä»¶å­—å…¸ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æœ¬"""
    if pd.isna(search_text) or search_text == '':
        return None, None, 0
        
    search_text = str(search_text).strip()
    
    best_match = None
    best_similarity = 0
    best_source = None
    
    for file_info in files_dict.values():
        df = file_info['dataframe']
        
        if source_col in df.columns and target_col in df.columns:
            if match_strategy == "ç²¾ç¡®åŒ¹é…":
                matches = df[df[source_col].astype(str).str.strip() == search_text]
                if not matches.empty:
                    return matches[target_col].iloc[0], matches[source_col].iloc[0], 1.0
            else:
                for idx, row in df.iterrows():
                    source_text = str(row[source_col])
                    if pd.isna(source_text) or source_text == '':
                        continue
                    
                    if match_strategy == "åŒ…å«åŒ¹é…":
                        if search_text in source_text or source_text in search_text:
                            similarity = similar(search_text, source_text)
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_match = row[target_col]
                                best_source = source_text
                    else:
                        similarity = similar(search_text, source_text)
                        if similarity > best_similarity and similarity >= similarity_threshold:
                            best_similarity = similarity
                            best_match = row[target_col]
                            best_source = source_text
    
    if match_strategy != "ç²¾ç¡®åŒ¹é…" and best_match is not None:
        return best_match, best_source, best_similarity
    
    return None, None, 0


def similar(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰"""
    if text1 == text2:
        return 1.0
    
    set1 = set(text1)
    set2 = set(text2)
    
    if not set1 and not set2:
        return 1.0
    
    if not set1 or not set2:
        return 0.0
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return intersection / union if union > 0 else 0.0


def process_single_row(args):
    """å¤„ç†å•è¡Œæ•°æ®çš„åŒ¹é…"""
    index, row, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled = args
    
    if skip_filled and not pd.isna(row.get(folder1_fill_col, None)) and str(row[folder1_fill_col]).strip() != '':
        return index, None, None, None, 0, "è·³è¿‡å·²å¡«å……"
    
    search_text = row[folder1_match_col]
    
    if pd.isna(search_text) or search_text == '':
        return index, None, None, None, 0, "ç©ºå€¼"
    
    matched_text, matched_source, similarity = find_matching_text(
        search_text, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold
    )
    
    match_status = "åŒ¹é…æˆåŠŸ" if matched_text is not None else "æœªåŒ¹é…"
    
    return index, matched_text, matched_source, search_text, similarity, match_status


def process_single_file(args):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„åŒ¹é…"""
    filename, file_info, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled, thread_id = args
    
    df = file_info['dataframe'].copy()
    file_matches = 0
    file_total = 0
    file_skipped = 0
    
    if folder1_match_col not in df.columns:
        return filename, None, {"error": f"æ–‡ä»¶ {filename} ä¸­æ‰¾ä¸åˆ°åˆ— '{folder1_match_col}'"}
        
    if folder1_fill_col not in df.columns:
        return filename, None, {"error": f"æ–‡ä»¶ {filename} ä¸­æ‰¾ä¸åˆ°åˆ— '{folder1_fill_col}'"}
    
    rows_to_process = []
    for index, row in df.iterrows():
        file_total += 1
        rows_to_process.append((index, row, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled))
    
    matched_results = {}
    match_details = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        future_to_index = {executor.submit(process_single_row, args): args[0] for args in rows_to_process}
        
        for future in concurrent.futures.as_completed(future_to_index):
            try:
                index, matched_text, matched_source, search_text, similarity, match_status = future.result()
                
                match_details.append({
                    'index': index,
                    'search_text': search_text,
                    'matched_text': matched_text,
                    'matched_source': matched_source,
                    'similarity': similarity,
                    'status': match_status
                })
                
                if match_status == "è·³è¿‡å·²å¡«å……":
                    file_skipped += 1
                elif matched_text is not None:
                    matched_results[index] = matched_text
                    file_matches += 1
            except Exception as e:
                index = future_to_index[future]
                st.warning(f"å¤„ç†æ–‡ä»¶ {filename} çš„ç¬¬ {index} è¡Œæ—¶å‡ºé”™: {str(e)}")
    
    for index, matched_text in matched_results.items():
        df.at[index, folder1_fill_col] = matched_text
    
    report = {
        'total_rows': file_total,
        'matched_rows': file_matches,
        'unmatched_rows': file_total - file_matches - file_skipped,
        'skipped_rows': file_skipped,
        'match_details': match_details
    }
    
    return filename, df, report


def process_file_matching_parallel(folder1_path, folder2_path, folder1_match_col, folder1_fill_col, 
                                  folder2_source_col, folder2_target_col, match_strategy, 
                                  similarity_threshold, skip_filled, max_workers=4):
    """å¹¶è¡Œå¤„ç†æ–‡ä»¶åŒ¹é…"""
    
    st.info("æ­£åœ¨åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶...")
    folder1_files = load_source_files_parallel(folder1_path, max_workers)
    
    if not folder1_files:
        st.error("åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæˆ–CSVæ–‡ä»¶")
        return None, None
    
    st.info("æ­£åœ¨åŠ è½½ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶...")
    folder2_files = load_all_files_parallel(folder2_path, max_workers)
    
    if not folder2_files:
        st.error("åœ¨ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæˆ–CSVæ–‡ä»¶")
        return None, None
    
    st.success(f"åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(folder1_files)} ä¸ªæ–‡ä»¶")
    st.success(f"åœ¨ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(folder2_files)} ä¸ªæ–‡ä»¶")
    
    st.info("å¼€å§‹åŒ¹é…å¤„ç†...")
    results = {}
    match_report = {
        'total_files': len(folder1_files),
        'total_rows': 0,
        'matched_rows': 0,
        'unmatched_rows': 0,
        'skipped_rows': 0,
        'file_details': {}
    }
    
    files_to_process = []
    for i, (filename, file_info) in enumerate(folder1_files.items()):
        files_to_process.append((
            filename, file_info, folder1_match_col, folder1_fill_col,
            folder2_files, folder2_source_col, folder2_target_col, 
            match_strategy, similarity_threshold, skip_filled, i % max_workers
        ))
    
    progress_bar = st.progress(0)
    processed_count = 0
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_filename = {executor.submit(process_single_file, args): args[0] for args in files_to_process}
        
        for future in concurrent.futures.as_completed(future_to_filename):
            try:
                filename, processed_df, report = future.result()
                if processed_df is not None:
                    results[filename] = processed_df
                    match_report['file_details'][filename] = report
                    match_report['total_rows'] += report['total_rows']
                    match_report['matched_rows'] += report['matched_rows']
                    match_report['unmatched_rows'] += report['unmatched_rows']
                    match_report['skipped_rows'] += report['skipped_rows']
                elif "error" in report:
                    st.error(report["error"])
            except Exception as e:
                filename = future_to_filename[future]
                st.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
            
            with progress_lock:
                processed_count += 1
                progress_bar.progress(processed_count / len(files_to_process))
    
    progress_bar.empty()
    return results, match_report


def save_processed_files(processed_files):
    """ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ°ZIPåŒ…"""
    zip_buffer = BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for filename, df in processed_files.items():
            if filename.lower().endswith('.csv'):
                csv_buffer = BytesIO()
                df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                csv_buffer.seek(0)
                zip_file.writestr(filename, csv_buffer.getvalue())
            else:
                excel_buffer = BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
                    df.to_excel(writer, index=False, sheet_name='Sheet1')
                excel_buffer.seek(0)
                zip_file.writestr(filename, excel_buffer.getvalue())
    
    return zip_buffer.getvalue()


def excel_matchpro_page():
    st.title("âš¡ Excel/CSVæ–‡ä»¶åŒ¹é…å·¥å…·(å¢å¼ºç‰ˆ)")
    st.markdown("""
    è¿™ä¸ªå·¥å…·ä½¿ç”¨å¤šçº¿ç¨‹æŠ€æœ¯åŠ é€Ÿå¤„ç†ï¼Œå¯ä»¥å¿«é€ŸåŒ¹é…ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„Excelå’ŒCSVæ–‡ä»¶å†…å®¹ã€‚  
    æ”¯æŒç²¾ç¡®åŒ¹é…ã€åŒ…å«åŒ¹é…å’Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œå¯è·³è¿‡å·²æœ‰ç¿»è¯‘æ–‡æœ¬çš„è¡Œã€‚
    """)
    
    st.sidebar.header("é…ç½®å‚æ•°")
    
    with st.sidebar.expander("æ–‡ä»¶å¤¹è®¾ç½®", expanded=True):
        folder1_path = st.text_input("ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå›ºå®šæ ¼å¼æ–‡ä»¶ï¼‰", value="./folder1")
        folder2_path = st.text_input("ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç¿»è¯‘æ–‡ä»¶ï¼‰", value="./folder2")
        max_workers = st.slider("çº¿ç¨‹æ•°", min_value=1, max_value=16, value=4, step=1)
    
    with st.sidebar.expander("åŒ¹é…ç­–ç•¥è®¾ç½®", expanded=True):
        match_strategy = st.selectbox(
            "åŒ¹é…ç­–ç•¥",
            ["ç²¾ç¡®åŒ¹é…", "åŒ…å«åŒ¹é…", "ç›¸ä¼¼åº¦åŒ¹é…"],
            help="ç²¾ç¡®åŒ¹é…: å®Œå…¨ç›¸åŒçš„æ–‡æœ¬; åŒ…å«åŒ¹é…: æ–‡æœ¬äº’ç›¸åŒ…å«; ç›¸ä¼¼åº¦åŒ¹é…: åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦"
        )
        
        similarity_threshold = st.slider(
            "ç›¸ä¼¼åº¦é˜ˆå€¼(ä»…å¯¹ç›¸ä¼¼åº¦åŒ¹é…æœ‰æ•ˆ)",
            min_value=0.1,
            max_value=1.0,
            value=0.8,
            step=0.05,
            help="ç›¸ä¼¼åº¦é«˜äºæ­¤é˜ˆå€¼çš„æ–‡æœ¬å°†è¢«è§†ä¸ºåŒ¹é…"
        )
        
        skip_filled = st.checkbox(
            "è·³è¿‡å·²æœ‰ç¿»è¯‘æ–‡æœ¬çš„è¡Œ",
            value=True,
            help="å¦‚æœç›®æ ‡åˆ—å·²æœ‰å†…å®¹ï¼Œåˆ™è·³è¿‡è¯¥è¡Œä¸è¿›è¡ŒåŒ¹é…"
        )
    
    with st.sidebar.expander("åˆ—æ˜ å°„è®¾ç½®", expanded=True):
        st.markdown("**ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹åˆ—è®¾ç½®**")
        folder1_match_col = st.text_input("åŒ¹é…åˆ—åï¼ˆç”¨äºæŸ¥æ‰¾çš„åˆ—ï¼‰", value="ä¸­æ–‡æ–‡æœ¬")
        folder1_fill_col = st.text_input("å¡«å……åˆ—åï¼ˆè¦å¡«å…¥ç¿»è¯‘çš„åˆ—ï¼‰", value="è‹±æ–‡æ–‡æœ¬")
        
        st.markdown("**ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹åˆ—è®¾ç½®**")
        folder2_source_col = st.text_input("åŸæ–‡åˆ—å", value="åŸæ–‡")
        folder2_target_col = st.text_input("ç¿»è¯‘åˆ—å", value="ç¿»è¯‘ç»“æœ")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if st.button("å¼€å§‹å¤„ç†", type="primary", use_container_width=True):
            if not folder1_path or not folder2_path:
                st.error("è¯·å¡«å†™ä¸¤ä¸ªæ–‡ä»¶å¤¹çš„è·¯å¾„")
                return
                
            if not os.path.exists(folder1_path):
                st.error(f"ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder1_path}")
                return
                
            if not os.path.exists(folder2_path):
                st.error(f"ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder2_path}")
                return
            
            start_time = time.time()
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶åŒ¹é…..."):
                processed_files, match_report = process_file_matching_parallel(
                    folder1_path, folder2_path, 
                    folder1_match_col, folder1_fill_col,
                    folder2_source_col, folder2_target_col,
                    match_strategy, similarity_threshold, skip_filled, max_workers
                )
            
            end_time = time.time()
            
            if processed_files is not None:
                st.success(f"å¤„ç†å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
                
                st.subheader("åŒ¹é…æŠ¥å‘Š")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("æ€»æ–‡ä»¶æ•°", match_report['total_files'])
                    st.metric("æ€»è¡Œæ•°", match_report['total_rows'])
                
                with col2:
                    st.metric("åŒ¹é…è¡Œæ•°", match_report['matched_rows'])
                    match_rate = (match_report['matched_rows'] / (match_report['total_rows'] - match_report['skipped_rows']) * 100) if (match_report['total_rows'] - match_report['skipped_rows']) > 0 else 0
                    st.metric("åŒ¹é…ç‡", f"{match_rate:.1f}%")
                
                with col3:
                    st.metric("æœªåŒ¹é…è¡Œæ•°", match_report['unmatched_rows'])
                    st.metric("è·³è¿‡è¡Œæ•°", match_report['skipped_rows'])
                
                with col4:
                    st.metric("å¤„ç†é€Ÿåº¦", f"{match_report['total_rows'] / (end_time - start_time):.1f} è¡Œ/ç§’")
                
                st.subheader("æ–‡ä»¶è¯¦æƒ…")
                for filename, details in match_report['file_details'].items():
                    with st.expander(f"ğŸ“„ {filename}", expanded=False):
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("æ€»è¡Œæ•°", details['total_rows'])
                        col2.metric("åŒ¹é…", details['matched_rows'])
                        col3.metric("æœªåŒ¹é…", details['unmatched_rows'])
                        col4.metric("è·³è¿‡", details['skipped_rows'])
                
                st.subheader("ä¸‹è½½ç»“æœ")
                zip_data = save_processed_files(processed_files)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼ˆZIPï¼‰",
                    data=zip_data,
                    file_name="processed_files.zip",
                    mime="application/zip",
                    use_container_width=True
                )
