import os
import sys
import re
import json
import time
import shutil
import subprocess
import threading
from datetime import datetime
from pathlib import Path
from io import BytesIO
import zipfile

import pandas as pd
import streamlit as st


def translation_result_processor_page():
    st.title("ğŸ“Š ç¿»è¯‘ç»“æœå¤„ç†å·¥å…·")
    st.markdown("### å°†AIç¿»è¯‘ç»“æœä¸åŸå§‹Excelæ–‡ä»¶åˆå¹¶")
    
    st.header("ğŸ“ ä¸Šä¼ æ–‡ä»¶")
    
    col1, col2 = st.columns(2)
    
    with col1:
        uploaded_original = st.file_uploader(
            "ğŸ“„ ä¸Šä¼ åŸå§‹Excelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="processor_original_file"
        )
        
        if uploaded_original is not None:
            try:
                df_original = pd.read_excel(uploaded_original)
                df_original.columns = df_original.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.success(f"âœ… æˆåŠŸè¯»å–åŸå§‹æ–‡ä»¶ï¼Œå…± {len(df_original)} è¡Œæ•°æ®")
                
                with st.expander("ğŸ“Š åŸå§‹æ–‡ä»¶é¢„è§ˆ"):
                    st.dataframe(df_original.head(10))
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    with col2:
        st.markdown("### ğŸ“ ç²˜è´´AIç¿»è¯‘ç»“æœ")
        st.markdown("è¯·å°†AIè¿”å›çš„ç¿»è¯‘ç»“æœç²˜è´´åˆ°ä¸‹æ–¹æ–‡æœ¬æ¡†ä¸­")
        
        translation_result = st.text_area(
            "ç¿»è¯‘ç»“æœ",
            height=300,
            placeholder="è¯·ç²˜è´´AIç¿»è¯‘ç»“æœ...",
            key="processor_translation_result"
        )
    
    if uploaded_original is not None and translation_result:
        st.header("ğŸ”„ å¤„ç†ç¿»è¯‘ç»“æœ")
        
        try:
            lines = translation_result.strip().split('\n')
            translations = []
            
            for line in lines:
                line = line.strip()
                if line and '|' in line:
                    parts = [p.strip() for p in line.split('|')]
                    if len(parts) >= 2:
                        translations.append({
                            'åŸæ–‡': parts[0],
                            'ç¿»è¯‘': parts[1]
                        })
            
            if translations:
                st.success(f"âœ… æˆåŠŸè§£æ {len(translations)} æ¡ç¿»è¯‘ç»“æœ")
                
                df_translations = pd.DataFrame(translations)
                
                with st.expander("ğŸ“Š ç¿»è¯‘ç»“æœé¢„è§ˆ"):
                    st.dataframe(df_translations.head(10))
                
                st.header("ğŸ“Š åˆå¹¶ç»“æœ")
                
                cols = df_original.columns.tolist()
                target_col = st.selectbox(
                    "é€‰æ‹©è¦æ·»åŠ ç¿»è¯‘ç»“æœçš„åˆ—:",
                    options=cols,
                    index=0,
                    key="processor_target_col"
                )
                
                if st.button("ğŸš€ åˆå¹¶ç¿»è¯‘ç»“æœ", key="merge_btn", use_container_width=True):
                    df_merged = df_original.copy()
                    df_merged['ç¿»è¯‘ç»“æœ'] = ''
                    
                    for idx, row in df_merged.iterrows():
                        original_text = str(row[target_col]).strip()
                        
                        for trans in translations:
                            if trans['åŸæ–‡'] == original_text:
                                df_merged.at[idx, 'ç¿»è¯‘ç»“æœ'] = trans['ç¿»è¯‘']
                                break
                    
                    st.success("âœ… åˆå¹¶å®Œæˆï¼")
                    
                    with st.expander("ğŸ“Š åˆå¹¶ç»“æœé¢„è§ˆ"):
                        st.dataframe(df_merged.head(20))
                    
                    output = pd.ExcelWriter('translation_result_merged.xlsx', engine='openpyxl')
                    df_merged.to_excel(output, index=False)
                    output.close()
                    
                    with open('translation_result_merged.xlsx', 'rb') as f:
                        st.download_button(
                            label="ğŸ’¾ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
                            data=f.read(),
                            file_name="translation_result_merged.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
            else:
                st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç¿»è¯‘ç»“æœ")
        except Exception as e:
            st.error(f"âŒ å¤„ç†å¤±è´¥: {e}")


def excel_comparison_page():
    st.title("ğŸ” Excelè¡¨æ ¼å¯¹æ¯”å·¥å…·")
    st.markdown("### å¯¹æ¯”ä¸¤ä¸ªExcelæ–‡ä»¶çš„å·®å¼‚")
    
    col1, col2 = st.columns(2)
    
    with col1:
        uploaded_file1 = st.file_uploader(
            "ğŸ“„ ä¸Šä¼ ç¬¬ä¸€ä¸ªExcelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="comparison_file1"
        )
        
        if uploaded_file1 is not None:
            try:
                df1 = pd.read_excel(uploaded_file1)
                df1.columns = df1.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.success(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶1ï¼Œå…± {len(df1)} è¡Œæ•°æ®")
                
                with st.expander("ğŸ“Š æ–‡ä»¶1é¢„è§ˆ"):
                    st.dataframe(df1.head(10))
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    with col2:
        uploaded_file2 = st.file_uploader(
            "ğŸ“„ ä¸Šä¼ ç¬¬äºŒä¸ªExcelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="comparison_file2"
        )
        
        if uploaded_file2 is not None:
            try:
                df2 = pd.read_excel(uploaded_file2)
                df2.columns = df2.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.success(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶2ï¼Œå…± {len(df2)} è¡Œæ•°æ®")
                
                with st.expander("ğŸ“Š æ–‡ä»¶2é¢„è§ˆ"):
                    st.dataframe(df2.head(10))
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    if uploaded_file1 is not None and uploaded_file2 is not None:
        st.header("ğŸ”„ å¯¹æ¯”è®¾ç½®")
        
        cols1 = df1.columns.tolist()
        cols2 = df2.columns.tolist()
        
        key_col1 = st.selectbox(
            "é€‰æ‹©æ–‡ä»¶1çš„å…³é”®åˆ—:",
            options=cols1,
            index=0,
            key="comparison_key_col1"
        )
        
        key_col2 = st.selectbox(
            "é€‰æ‹©æ–‡ä»¶2çš„å…³é”®åˆ—:",
            options=cols2,
            index=0,
            key="comparison_key_col2"
        )
        
        if st.button("ğŸš€ å¼€å§‹å¯¹æ¯”", key="compare_btn", use_container_width=True):
            try:
                merged = pd.merge(df1, df2, left_on=key_col1, right_on=key_col2, how='outer', indicator=True, suffixes=('_file1', '_file2'))
                
                only_in_file1 = merged[merged['_merge'] == 'left_only']
                only_in_file2 = merged[merged['_merge'] == 'right_only']
                in_both = merged[merged['_merge'] == 'both']
                
                st.success("âœ… å¯¹æ¯”å®Œæˆï¼")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("ä»…åœ¨æ–‡ä»¶1ä¸­", len(only_in_file1))
                
                with col2:
                    st.metric("ä»…åœ¨æ–‡ä»¶2ä¸­", len(only_in_file2))
                
                with col3:
                    st.metric("ä¸¤ä¸ªæ–‡ä»¶éƒ½æœ‰", len(in_both))
                
                if not only_in_file1.empty:
                    st.subheader("ğŸ“‹ ä»…åœ¨æ–‡ä»¶1ä¸­çš„è®°å½•")
                    st.dataframe(only_in_file1.head(20))
                
                if not only_in_file2.empty:
                    st.subheader("ğŸ“‹ ä»…åœ¨æ–‡ä»¶2ä¸­çš„è®°å½•")
                    st.dataframe(only_in_file2.head(20))
                
                if not in_both.empty:
                    st.subheader("ğŸ“‹ ä¸¤ä¸ªæ–‡ä»¶éƒ½æœ‰çš„è®°å½•")
                    st.dataframe(in_both.head(20))
                
            except Exception as e:
                st.error(f"âŒ å¯¹æ¯”å¤±è´¥: {e}")


def term_lookup_page():
    st.title("æœ¯è¯­æŸ¥æ‰¾")
    st.markdown("### åœ¨æœ¯è¯­åº“ä¸­æŸ¥æ‰¾æœ¯è¯­")
    
    uploaded_file = st.file_uploader(
        "ğŸ“„ ä¸Šä¼ æœ¯è¯­åº“æ–‡ä»¶ (Excel)",
        type=['xlsx', 'xls'],
        key="term_lookup_file"
    )
    
    if uploaded_file is not None:
        try:
            df = pd.read_excel(uploaded_file)
            df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
            st.success(f"âœ… æˆåŠŸè¯»å–æœ¯è¯­åº“ï¼Œå…± {len(df)} æ¡è®°å½•")
            
            with st.expander("ğŸ“Š æœ¯è¯­åº“é¢„è§ˆ"):
                st.dataframe(df.head(10))
            
            cols = df.columns.tolist()
            search_col = st.selectbox(
                "é€‰æ‹©æœç´¢åˆ—:",
                options=cols,
                index=0,
                key="term_lookup_col"
            )
            
            search_term = st.text_input(
                "è¾“å…¥æœç´¢æœ¯è¯­:",
                placeholder="è¯·è¾“å…¥è¦æŸ¥æ‰¾çš„æœ¯è¯­...",
                key="term_lookup_search"
            )
            
            if st.button("ğŸ” æŸ¥æ‰¾", key="term_lookup_btn", use_container_width=True):
                if search_term:
                    results = df[df[search_col].astype(str).str.contains(search_term, case=False, na=False)]
                    
                    if not results.empty:
                        st.success(f"âœ… æ‰¾åˆ° {len(results)} æ¡åŒ¹é…è®°å½•")
                        st.dataframe(results)
                    else:
                        st.warning("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æœ¯è¯­")
        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")


progress_lock = threading.Lock()


def load_single_file(file_path):
    """åŠ è½½å•ä¸ªæ–‡ä»¶ï¼ˆExcelæˆ–CSVï¼‰"""
    try:
        if file_path.suffix.lower() in ['.xlsx', '.xls', '.xlsm']:
            excel_file = pd.read_excel(file_path, sheet_name=None)
            results = {}
            for sheet_name, df in excel_file.items():
                if not df.empty:
                    key = f"{file_path.name} - {sheet_name}"
                    results[key] = {
                        'dataframe': df,
                        'file_path': file_path,
                        'sheet_name': sheet_name,
                        'file_type': 'excel'
                    }
            return results
        elif file_path.suffix.lower() == '.csv':
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is not None and not df.empty:
                return {
                    file_path.name: {
                        'dataframe': df,
                        'file_path': file_path,
                        'sheet_name': 'CSV',
                        'file_type': 'csv'
                    }
                }
    except Exception as e:
        st.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
    
    return {}


def load_all_files_parallel(folder_path, max_workers=4):
    """å¹¶è¡ŒåŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰Excelå’ŒCSVæ–‡ä»¶"""
    all_files = {}
    folder_path = Path(folder_path)
    
    file_paths = []
    for pattern in ['*.xlsx', '*.xls', '*.xlsm', '*.csv']:
        file_paths.extend(folder_path.rglob(pattern))
    
    if not file_paths:
        return all_files
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_path = {executor.submit(load_single_file, path): path for path in file_paths}
        
        for future in concurrent.futures.as_completed(future_to_path):
            try:
                result = future.result()
                all_files.update(result)
            except Exception as e:
                path = future_to_path[future]
                st.warning(f"å¤„ç†æ–‡ä»¶ {path} æ—¶å‡ºé”™: {str(e)}")
    
    return all_files


def load_source_files_parallel(folder_path, max_workers=4):
    """å¹¶è¡ŒåŠ è½½æºæ–‡ä»¶å¤¹ä¸­çš„Excelå’ŒCSVæ–‡ä»¶"""
    source_files = {}
    folder_path = Path(folder_path)
    
    file_paths = []
    for pattern in ['*.xlsx', '*.xls', '*.xlsm', '*.csv']:
        file_paths.extend(folder_path.glob(pattern))
    
    if not file_paths:
        return source_files
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_path = {executor.submit(load_single_file, path): path for path in file_paths}
        
        for future in concurrent.futures.as_completed(future_to_path):
            try:
                result = future.result()
                source_files.update(result)
            except Exception as e:
                path = future_to_path[future]
                st.error(f"å¤„ç†æ–‡ä»¶ {path} æ—¶å‡ºé”™: {str(e)}")
    
    return source_files


def find_matching_text(search_text, files_dict, source_col, target_col, match_strategy, similarity_threshold):
    """åœ¨æ–‡ä»¶å­—å…¸ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æœ¬"""
    if pd.isna(search_text) or search_text == '':
        return None, None, 0
        
    search_text = str(search_text).strip()
    
    best_match = None
    best_similarity = 0
    best_source = None
    
    for file_info in files_dict.values():
        df = file_info['dataframe']
        
        if source_col in df.columns and target_col in df.columns:
            if match_strategy == "ç²¾ç¡®åŒ¹é…":
                matches = df[df[source_col].astype(str).str.strip() == search_text]
                if not matches.empty:
                    return matches[target_col].iloc[0], matches[source_col].iloc[0], 1.0
            else:
                for idx, row in df.iterrows():
                    source_text = str(row[source_col])
                    if pd.isna(source_text) or source_text == '':
                        continue
                    
                    if match_strategy == "åŒ…å«åŒ¹é…":
                        if search_text in source_text or source_text in search_text:
                            similarity = similar(search_text, source_text)
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_match = row[target_col]
                                best_source = source_text
                    else:
                        similarity = similar(search_text, source_text)
                        if similarity > best_similarity and similarity >= similarity_threshold:
                            best_similarity = similarity
                            best_match = row[target_col]
                            best_source = source_text
    
    if match_strategy != "ç²¾ç¡®åŒ¹é…" and best_match is not None:
        return best_match, best_source, best_similarity
    
    return None, None, 0


def similar(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰"""
    if text1 == text2:
        return 1.0
    
    set1 = set(text1)
    set2 = set(text2)
    
    if not set1 and not set2:
        return 1.0
    
    if not set1 or not set2:
        return 0.0
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return intersection / union if union > 0 else 0.0


def process_single_row(args):
    """å¤„ç†å•è¡Œæ•°æ®çš„åŒ¹é…"""
    index, row, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled = args
    
    if skip_filled and not pd.isna(row.get(folder1_fill_col, None)) and str(row[folder1_fill_col]).strip() != '':
        return index, None, None, None, 0, "è·³è¿‡å·²å¡«å……"
    
    search_text = row[folder1_match_col]
    
    if pd.isna(search_text) or search_text == '':
        return index, None, None, None, 0, "ç©ºå€¼"
    
    matched_text, matched_source, similarity = find_matching_text(
        search_text, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold
    )
    
    match_status = "åŒ¹é…æˆåŠŸ" if matched_text is not None else "æœªåŒ¹é…"
    
    return index, matched_text, matched_source, search_text, similarity, match_status


def process_single_file(args):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„åŒ¹é…"""
    filename, file_info, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled, thread_id = args
    
    df = file_info['dataframe'].copy()
    file_matches = 0
    file_total = 0
    file_skipped = 0
    
    if folder1_match_col not in df.columns:
        return filename, None, {"error": f"æ–‡ä»¶ {filename} ä¸­æ‰¾ä¸åˆ°åˆ— '{folder1_match_col}'"}
        
    if folder1_fill_col not in df.columns:
        return filename, None, {"error": f"æ–‡ä»¶ {filename} ä¸­æ‰¾ä¸åˆ°åˆ— '{folder1_fill_col}'"}
    
    rows_to_process = []
    for index, row in df.iterrows():
        file_total += 1
        rows_to_process.append((index, row, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled))
    
    matched_results = {}
    match_details = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        future_to_index = {executor.submit(process_single_row, args): args[0] for args in rows_to_process}
        
        for future in concurrent.futures.as_completed(future_to_index):
            try:
                index, matched_text, matched_source, search_text, similarity, match_status = future.result()
                
                match_details.append({
                    'index': index,
                    'search_text': search_text,
                    'matched_text': matched_text,
                    'matched_source': matched_source,
                    'similarity': similarity,
                    'status': match_status
                })
                
                if match_status == "è·³è¿‡å·²å¡«å……":
                    file_skipped += 1
                elif matched_text is not None:
                    matched_results[index] = matched_text
                    file_matches += 1
            except Exception as e:
                index = future_to_index[future]
                st.warning(f"å¤„ç†æ–‡ä»¶ {filename} çš„ç¬¬ {index} è¡Œæ—¶å‡ºé”™: {str(e)}")
    
    for index, matched_text in matched_results.items():
        df.at[index, folder1_fill_col] = matched_text
    
    report = {
        'total_rows': file_total,
        'matched_rows': file_matches,
        'unmatched_rows': file_total - file_matches - file_skipped,
        'skipped_rows': file_skipped,
        'match_details': match_details
    }
    
    return filename, df, report


def process_file_matching_parallel(folder1_path, folder2_path, folder1_match_col, folder1_fill_col, 
                                  folder2_source_col, folder2_target_col, match_strategy, 
                                  similarity_threshold, skip_filled, max_workers=4):
    """å¹¶è¡Œå¤„ç†æ–‡ä»¶åŒ¹é…"""
    
    st.info("æ­£åœ¨åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶...")
    folder1_files = load_source_files_parallel(folder1_path, max_workers)
    
    if not folder1_files:
        st.error("åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæˆ–CSVæ–‡ä»¶")
        return None, None
    
    st.info("æ­£åœ¨åŠ è½½ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶...")
    folder2_files = load_all_files_parallel(folder2_path, max_workers)
    
    if not folder2_files:
        st.error("åœ¨ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæˆ–CSVæ–‡ä»¶")
        return None, None
    
    st.success(f"åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(folder1_files)} ä¸ªæ–‡ä»¶")
    st.success(f"åœ¨ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(folder2_files)} ä¸ªæ–‡ä»¶")
    
    st.info("å¼€å§‹åŒ¹é…å¤„ç†...")
    results = {}
    match_report = {
        'total_files': len(folder1_files),
        'total_rows': 0,
        'matched_rows': 0,
        'unmatched_rows': 0,
        'skipped_rows': 0,
        'file_details': {}
    }
    
    files_to_process = []
    for i, (filename, file_info) in enumerate(folder1_files.items()):
        files_to_process.append((
            filename, file_info, folder1_match_col, folder1_fill_col,
            folder2_files, folder2_source_col, folder2_target_col, 
            match_strategy, similarity_threshold, skip_filled, i % max_workers
        ))
    
    progress_bar = st.progress(0)
    processed_count = 0
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_filename = {executor.submit(process_single_file, args): args[0] for args in files_to_process}
        
        for future in concurrent.futures.as_completed(future_to_filename):
            try:
                filename, processed_df, report = future.result()
                if processed_df is not None:
                    results[filename] = processed_df
                    match_report['file_details'][filename] = report
                    match_report['total_rows'] += report['total_rows']
                    match_report['matched_rows'] += report['matched_rows']
                    match_report['unmatched_rows'] += report['unmatched_rows']
                    match_report['skipped_rows'] += report['skipped_rows']
                elif "error" in report:
                    st.error(report["error"])
            except Exception as e:
                filename = future_to_filename[future]
                st.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
            
            with progress_lock:
                processed_count += 1
                progress_bar.progress(processed_count / len(files_to_process))
    
    progress_bar.empty()
    return results, match_report


def save_processed_files(processed_files):
    """ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ°ZIPåŒ…"""
    zip_buffer = BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for filename, df in processed_files.items():
            if filename.lower().endswith('.csv'):
                csv_buffer = BytesIO()
                df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                csv_buffer.seek(0)
                zip_file.writestr(filename, csv_buffer.getvalue())
            else:
                excel_buffer = BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
                    df.to_excel(writer, index=False, sheet_name='Sheet1')
                excel_buffer.seek(0)
                zip_file.writestr(filename, excel_buffer.getvalue())
    
    return zip_buffer.getvalue()


def excel_matchpro_page():
    st.title("âš¡ Excel/CSVæ–‡ä»¶åŒ¹é…å·¥å…·(å¢å¼ºç‰ˆ)")
    st.markdown("""
    è¿™ä¸ªå·¥å…·ä½¿ç”¨å¤šçº¿ç¨‹æŠ€æœ¯åŠ é€Ÿå¤„ç†ï¼Œå¯ä»¥å¿«é€ŸåŒ¹é…ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„Excelå’ŒCSVæ–‡ä»¶å†…å®¹ã€‚  
    æ”¯æŒç²¾ç¡®åŒ¹é…ã€åŒ…å«åŒ¹é…å’Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œå¯è·³è¿‡å·²æœ‰ç¿»è¯‘æ–‡æœ¬çš„è¡Œã€‚
    """)
    
    st.sidebar.header("é…ç½®å‚æ•°")
    
    with st.sidebar.expander("æ–‡ä»¶å¤¹è®¾ç½®", expanded=True):
        folder1_path = st.text_input("ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå›ºå®šæ ¼å¼æ–‡ä»¶ï¼‰", value="./folder1")
        folder2_path = st.text_input("ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç¿»è¯‘æ–‡ä»¶ï¼‰", value="./folder2")
        max_workers = st.slider("çº¿ç¨‹æ•°", min_value=1, max_value=16, value=4, step=1)
    
    with st.sidebar.expander("åŒ¹é…ç­–ç•¥è®¾ç½®", expanded=True):
        match_strategy = st.selectbox(
            "åŒ¹é…ç­–ç•¥",
            ["ç²¾ç¡®åŒ¹é…", "åŒ…å«åŒ¹é…", "ç›¸ä¼¼åº¦åŒ¹é…"],
            help="ç²¾ç¡®åŒ¹é…: å®Œå…¨ç›¸åŒçš„æ–‡æœ¬; åŒ…å«åŒ¹é…: æ–‡æœ¬äº’ç›¸åŒ…å«; ç›¸ä¼¼åº¦åŒ¹é…: åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦"
        )
        
        similarity_threshold = st.slider(
            "ç›¸ä¼¼åº¦é˜ˆå€¼(ä»…å¯¹ç›¸ä¼¼åº¦åŒ¹é…æœ‰æ•ˆ)",
            min_value=0.1,
            max_value=1.0,
            value=0.8,
            step=0.05,
            help="ç›¸ä¼¼åº¦é«˜äºæ­¤é˜ˆå€¼çš„æ–‡æœ¬å°†è¢«è§†ä¸ºåŒ¹é…"
        )
        
        skip_filled = st.checkbox(
            "è·³è¿‡å·²æœ‰ç¿»è¯‘æ–‡æœ¬çš„è¡Œ",
            value=True,
            help="å¦‚æœç›®æ ‡åˆ—å·²æœ‰å†…å®¹ï¼Œåˆ™è·³è¿‡è¯¥è¡Œä¸è¿›è¡ŒåŒ¹é…"
        )
    
    with st.sidebar.expander("åˆ—æ˜ å°„è®¾ç½®", expanded=True):
        st.markdown("**ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹åˆ—è®¾ç½®**")
        folder1_match_col = st.text_input("åŒ¹é…åˆ—åï¼ˆç”¨äºæŸ¥æ‰¾çš„åˆ—ï¼‰", value="ä¸­æ–‡æ–‡æœ¬")
        folder1_fill_col = st.text_input("å¡«å……åˆ—åï¼ˆè¦å¡«å…¥ç¿»è¯‘çš„åˆ—ï¼‰", value="è‹±æ–‡æ–‡æœ¬")
        
        st.markdown("**ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹åˆ—è®¾ç½®**")
        folder2_source_col = st.text_input("åŸæ–‡åˆ—å", value="åŸæ–‡")
        folder2_target_col = st.text_input("ç¿»è¯‘åˆ—å", value="ç¿»è¯‘ç»“æœ")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if st.button("å¼€å§‹å¤„ç†", type="primary", use_container_width=True):
            if not folder1_path or not folder2_path:
                st.error("è¯·å¡«å†™ä¸¤ä¸ªæ–‡ä»¶å¤¹çš„è·¯å¾„")
                return
                
            if not os.path.exists(folder1_path):
                st.error(f"ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder1_path}")
                return
                
            if not os.path.exists(folder2_path):
                st.error(f"ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder2_path}")
                return
            
            start_time = time.time()
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶åŒ¹é…..."):
                processed_files, match_report = process_file_matching_parallel(
                    folder1_path, folder2_path, 
                    folder1_match_col, folder1_fill_col,
                    folder2_source_col, folder2_target_col,
                    match_strategy, similarity_threshold, skip_filled, max_workers
                )
            
            end_time = time.time()
            
            if processed_files is not None:
                st.success(f"å¤„ç†å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
                
                st.subheader("åŒ¹é…æŠ¥å‘Š")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("æ€»æ–‡ä»¶æ•°", match_report['total_files'])
                    st.metric("æ€»è¡Œæ•°", match_report['total_rows'])
                
                with col2:
                    st.metric("åŒ¹é…è¡Œæ•°", match_report['matched_rows'])
                    match_rate = (match_report['matched_rows'] / (match_report['total_rows'] - match_report['skipped_rows']) * 100) if (match_report['total_rows'] - match_report['skipped_rows']) > 0 else 0
                    st.metric("åŒ¹é…ç‡", f"{match_rate:.1f}%")
                
                with col3:
                    st.metric("æœªåŒ¹é…è¡Œæ•°", match_report['unmatched_rows'])
                    st.metric("è·³è¿‡è¡Œæ•°", match_report['skipped_rows'])
                
                with col4:
                    st.metric("å¤„ç†é€Ÿåº¦", f"{match_report['total_rows'] / (end_time - start_time):.1f} è¡Œ/ç§’")
                
                st.subheader("æ–‡ä»¶è¯¦æƒ…")
                for filename, details in match_report['file_details'].items():
                    with st.expander(f"ğŸ“„ {filename}", expanded=False):
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("æ€»è¡Œæ•°", details['total_rows'])
                        col2.metric("åŒ¹é…", details['matched_rows'])
                        col3.metric("æœªåŒ¹é…", details['unmatched_rows'])
                        col4.metric("è·³è¿‡", details['skipped_rows'])
                
                st.subheader("ä¸‹è½½ç»“æœ")
                zip_data = save_processed_files(processed_files)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼ˆZIPï¼‰",
                    data=zip_data,
                    file_name="processed_files.zip",
                    mime="application/zip",
                    use_container_width=True
                )


def find_yt_dlp():
    """æŸ¥æ‰¾yt-dlpå¯æ‰§è¡Œæ–‡ä»¶"""
    if sys.platform.startswith('win'):
        candidates = ["yt-dlp.exe", "yt-dlp"]
    else:
        candidates = ["./yt-dlp", "yt-dlp"]
    
    for candidate in candidates:
        if os.path.exists(candidate):
            return candidate
    return "yt-dlp"


def normalize_url(url):
    """å°†éæ ‡å‡†çš„Niconico URLæ ¼å¼è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
    return url.replace("www.video.nicovideo.jp", "www.nicovideo.jp")


def extract_watch_id(url):
    """ä»Niconicoè§†é¢‘URLä¸­æå–watch ID (sm/nmå·)"""
    match = re.search(r'(sm|nm)\d+', url)
    if match:
        return match.group(0)
    return "unknown_id"


def extract_bilibili_id(url):
    """ä»Bilibiliè§†é¢‘URLä¸­æå–video ID (BVå·æˆ–avå·)"""
    bv_match = re.search(r'BV[a-zA-Z0-9]+', url)
    if bv_match:
        return bv_match.group(0)
    
    av_match = re.search(r'av(\d+)', url)
    if av_match:
        return f"av{av_match.group(1)}"
    
    return "unknown_id"


def run_yt_dlp_to_get_json(url, output_filename_base="danmaku"):
    """è¿è¡Œyt-dlpå‘½ä»¤æ¥æŠ“å–å¼¹å¹•æ•°æ®å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
    yt_dlp_path = find_yt_dlp()
    
    command = [
        yt_dlp_path,
        "--skip-download",
        "--write-sub",
        "--all-subs",
        "--sub-format", "json",
        "--output", f"{output_filename_base}.%(ext)s",
        url
    ]
    
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        
        json_filename = f"{output_filename_base}.comments.json"
        if os.path.exists(json_filename):
            return json_filename
        else:
            return None
            
    except subprocess.CalledProcessError as e:
        st.error(f"yt-dlpæ‰§è¡Œå¤±è´¥: {e.stderr}")
        return None
    except FileNotFoundError:
        st.error(f"æ‰¾ä¸åˆ°yt-dlpå¯æ‰§è¡Œæ–‡ä»¶ã€‚è¯·ç¡®ä¿yt-dlpå·²å®‰è£…æˆ–åœ¨PATHä¸­ã€‚")
        return None


def process_niconico_json_to_dataframe(json_path):
    """è¯»å–yt-dlpç”Ÿæˆçš„JSONæ–‡ä»¶ï¼Œå¤„ç†Niconicoå¼¹å¹•æ•°æ®"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        st.error(f"JSONæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return None

    danmaku_list = []
    for comment in data:
        vpos_ms = comment.get("vposMs", 0)
        time_sec = vpos_ms / 1000
        video_time = time.strftime('%H:%M:%S', time.gmtime(time_sec))
        
        posted_at_str = comment.get("postedAt")
        try:
            posted_at = datetime.fromisoformat(posted_at_str)
            send_time = posted_at.strftime('%Y-%m-%d %H:%M:%S')
        except:
            send_time = posted_at_str
            
        commands = " ".join(comment.get("commands", []))
        
        danmaku_info = {
            "å¼¹å¹•å†…å®¹": comment.get("body"),
            "è§†é¢‘æ—¶é—´": video_time,
            "æ—¶é—´(ç§’)": time_sec,
            "æ ¼å¼/é¢œè‰²": commands,
            "ç”¨æˆ·ID": comment.get("userId"),
            "å‘é€æ—¶é—´": send_time,
            "ç¼–å·": comment.get("no"),
        }
        danmaku_list.append(danmaku_info)
        
    if not danmaku_list:
        return None
        
    df = pd.DataFrame(danmaku_list)
    df = df[['ç¼–å·', 'è§†é¢‘æ—¶é—´', 'æ—¶é—´(ç§’)', 'å¼¹å¹•å†…å®¹', 'æ ¼å¼/é¢œè‰²', 'ç”¨æˆ·ID', 'å‘é€æ—¶é—´']]
    return df


def scrape_niconico_danmaku(url):
    """æŠ“å–Niconicoå¼¹å¹•"""
    normalized_url = normalize_url(url)
    watch_id = extract_watch_id(normalized_url)
    
    with st.spinner(f"æ­£åœ¨æŠ“å–Niconicoè§†é¢‘ {watch_id} çš„å¼¹å¹•..."):
        json_path = run_yt_dlp_to_get_json(normalized_url, output_filename_base=watch_id)
        
        if json_path:
            df = process_niconico_json_to_dataframe(json_path)
            
            try:
                os.remove(json_path)
            except OSError:
                pass
            
            return df, watch_id
        else:
            return None, watch_id


def scrape_bilibili_danmaku(url, cookies_file=None):
    """æŠ“å–Bilibiliå¼¹å¹•"""
    video_id = extract_bilibili_id(url)
    
    with st.spinner(f"æ­£åœ¨æŠ“å–Bilibiliè§†é¢‘ {video_id} çš„å¼¹å¹•..."):
        yt_dlp_path = find_yt_dlp()
        
        command = [
            yt_dlp_path,
            "--skip-download",
            "--write-sub",
            "--all-subs",
            "--sub-format", "json",
            "--output", f"{video_id}.%(ext)s",
        ]
        
        if cookies_file and os.path.exists(cookies_file):
            command.extend(["--cookies", cookies_file])
        
        command.append(url)
        
        try:
            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=60)
            
            json_filename = f"{video_id}.comments.json"
            if os.path.exists(json_filename):
                try:
                    with open(json_filename, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                except (FileNotFoundError, json.JSONDecodeError) as e:
                    st.error(f"JSONæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
                    return None, video_id
                
                danmaku_list = []
                for comment in data:
                    danmaku_info = {
                        "å¼¹å¹•å†…å®¹": comment.get("body", comment.get("text", "")),
                        "å‘é€æ—¶é—´": comment.get("postedAt", comment.get("timestamp", "")),
                        "ç”¨æˆ·ID": comment.get("userId", comment.get("author", "")),
                    }
                    danmaku_list.append(danmaku_info)
                
                if danmaku_list:
                    df = pd.DataFrame(danmaku_list)
                    
                    try:
                        os.remove(json_filename)
                    except OSError:
                        pass
                    
                    return df, video_id
                else:
                    st.warning("æœªæ‰¾åˆ°å¼¹å¹•æ•°æ®")
                    return None, video_id
            else:
                st.error(f"yt-dlpæœªç”Ÿæˆé¢„æœŸçš„æ–‡ä»¶")
                return None, video_id
                
        except subprocess.CalledProcessError as e:
            error_msg = e.stderr if e.stderr else str(e)
            if "412" in error_msg or "Precondition Failed" in error_msg:
                st.error(
                    "âŒ Bilibili APIé€Ÿç‡é™åˆ¶ï¼ˆHTTP 412é”™è¯¯ï¼‰\n\n"
                    "è¿™é€šå¸¸æ˜¯å› ä¸ºï¼š\n"
                    "1. æ²¡æœ‰æä¾›æœ‰æ•ˆçš„Cookieè®¤è¯\n"
                    "2. è¯¥IPåœ°å€çš„è¯·æ±‚å·²è¾¾åˆ°é™åˆ¶\n\n"
                    "**è§£å†³æ–¹æ¡ˆï¼š**\n"
                    "è¯·åœ¨å·¦ä¾§æ ä¸Šä¼ æ‚¨çš„Bilibili Cookieæ–‡ä»¶ï¼Œç„¶åé‡è¯•ã€‚"
                )
            else:
                st.error(f"yt-dlpæ‰§è¡Œå¤±è´¥: {error_msg}")
            return None, video_id
        except subprocess.TimeoutExpired:
            st.error("âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
            return None, video_id
        except FileNotFoundError:
            st.error(f"æ‰¾ä¸åˆ°yt-dlpå¯æ‰§è¡Œæ–‡ä»¶")
            return None, video_id


def danmu_page():
    st.markdown("""
    # ğŸ¬ å¼¹å¹•æŠ“å–å·¥å…·
    
    æ”¯æŒä» **Niconico** å’Œ **Bilibili** æŠ“å–è§†é¢‘å¼¹å¹•ï¼Œå¹¶å¯¼å‡ºä¸º Excel æ–‡ä»¶ã€‚
    """)
    
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®1")
        platform = st.radio(
            "é€‰æ‹©è§†é¢‘å¹³å°",
            options=["Niconico", "Bilibili"],
            help="é€‰æ‹©æ‚¨è¦æŠ“å–å¼¹å¹•çš„è§†é¢‘å¹³å°",
            key="video_pla_selector"
        )
        
        st.divider()
        
        bilibili_cookies_file = None
        if platform == "Bilibili":
            st.subheader("ğŸ” Bilibili Cookieé…ç½®")
            st.markdown(
                """Bilibiliéœ€è¦Cookieè®¤è¯ä»¥é¿å…é€Ÿç‡é™åˆ¶ã€‚\n\n
                **è·å–Cookieçš„æ–¹æ³•ï¼š**
                1. æ‰“å¼€æµè§ˆå™¨è®¿é—® https://www.bilibili.com
                2. ç™»å½•æ‚¨çš„è´¦å·
                3. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…· â†’ Application â†’ Cookies
                4. å¤åˆ¶æ‰€æœ‰Cookieå†…å®¹åˆ°æ–‡æœ¬æ–‡ä»¶
                5. ä¸Šä¼ è¯¥æ–‡ä»¶
                """
            )
            
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ Cookieæ–‡ä»¶",
                type=["txt"],
                help="ä¸Šä¼ ä»æµè§ˆå™¨å¯¼å‡ºçš„Cookieæ–‡ä»¶"
            )
            
            if uploaded_file is not None:
                cookies_content = uploaded_file.read().decode('utf-8')
                bilibili_cookies_file = "temp_cookies.txt"
                with open(bilibili_cookies_file, 'w', encoding='utf-8') as f:
                    f.write(cookies_content)
                st.success("âœ… Cookieæ–‡ä»¶å·²åŠ è½½")
            else:
                st.warning("âš ï¸ æœªä¸Šä¼ Cookieæ–‡ä»¶ï¼Œå¯èƒ½å¯¼è‡´é€Ÿç‡é™åˆ¶é”™è¯¯")
        
        st.divider()
        
        st.markdown("""
        ### ğŸ“Œ ä½¿ç”¨è¯´æ˜
        
        **Niconico:**
        - è¾“å…¥æ ¼å¼: `https://www.nicovideo.jp/watch/sm500873`
        - æˆ–: `https://www.video.nicovideo.jp/watch/sm500873` (ä¼šè‡ªåŠ¨è½¬æ¢)
        
        **Bilibili:**
        - è¾“å…¥æ ¼å¼: `https://www.bilibili.com/video/BV1xx411c7mD`
        - æˆ–: `https://www.bilibili.com/video/av123456789`
        - å»ºè®®ä¸Šä¼ Cookieæ–‡ä»¶ä»¥é¿å…é€Ÿç‡é™åˆ¶
        """)
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader(f"è¾“å…¥{platform}è§†é¢‘é“¾æ¥")
        video_url = st.text_input(
            "è§†é¢‘é“¾æ¥",
            placeholder=f"è¯·è¾“å…¥{platform}è§†é¢‘é“¾æ¥...",
            label_visibility="collapsed"
        )
    
    with col2:
        st.subheader("æ“ä½œ")
        scrape_button = st.button(
            "ğŸ” å¼€å§‹æŠ“å–",
            use_container_width=True,
            type="primary"
        )
    
    st.divider()
    
    if scrape_button:
        if not video_url.strip():
            st.error("âŒ è¯·è¾“å…¥è§†é¢‘é“¾æ¥")
        else:
            if platform == "Niconico":
                df, video_id = scrape_niconico_danmaku(video_url)
            else:
                df, video_id = scrape_bilibili_danmaku(video_url, cookies_file=bilibili_cookies_file)
            
            if df is not None and len(df) > 0:
                st.success(f"âœ… æˆåŠŸæŠ“å– {len(df)} æ¡å¼¹å¹•ï¼")
                
                st.subheader("ğŸ“Š å¼¹å¹•æ•°æ®é¢„è§ˆ")
                st.dataframe(df, use_container_width=True, height=400)
                
                st.subheader("ğŸ’¾ å¯¼å‡ºé€‰é¡¹")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    excel_buffer = pd.ExcelWriter(
                        f"danmaku_{video_id}.xlsx",
                        engine='openpyxl'
                    )
                    df.to_excel(excel_buffer, index=False, sheet_name='å¼¹å¹•æ•°æ®')
                    excel_buffer.close()
                    
                    with open(f"danmaku_{video_id}.xlsx", 'rb') as f:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½ Excel",
                            data=f.read(),
                            file_name=f"danmaku_{video_id}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                    
                    try:
                        os.remove(f"danmaku_{video_id}.xlsx")
                    except OSError:
                        pass
                    
                    if bilibili_cookies_file and os.path.exists(bilibili_cookies_file):
                        try:
                            os.remove(bilibili_cookies_file)
                        except OSError:
                            pass
                
                with col2:
                    csv_buffer = df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ CSV",
                        data=csv_buffer,
                        file_name=f"danmaku_{video_id}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                
                with col3:
                    json_buffer = df.to_json(orient='records', force_ascii=False)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ JSON",
                        data=json_buffer,
                        file_name=f"danmaku_{video_id}.json",
                        mime="application/json",
                        use_container_width=True
                    )
                
                st.subheader("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("æ€»å¼¹å¹•æ•°", len(df))
                
                with col2:
                    if 'ç”¨æˆ·ID' in df.columns:
                        unique_users = df['ç”¨æˆ·ID'].nunique()
                        st.metric("ç‹¬ç«‹ç”¨æˆ·æ•°", unique_users)
                
                with col3:
                    if 'å¼¹å¹•å†…å®¹' in df.columns:
                        avg_length = df['å¼¹å¹•å†…å®¹'].str.len().mean()
                        st.metric("å¹³å‡å¼¹å¹•é•¿åº¦", f"{avg_length:.1f} å­—ç¬¦")
            
            elif df is not None and len(df) == 0:
                st.warning("âš ï¸ æœªæ‰¾åˆ°å¼¹å¹•æ•°æ®ï¼Œè¯·æ£€æŸ¥è§†é¢‘é“¾æ¥æ˜¯å¦æ­£ç¡®")
            else:
                st.error("âŒ æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è§†é¢‘é“¾æ¥æˆ–ç½‘ç»œè¿æ¥")
    
    st.divider()
    st.markdown("""
    ---
    **å¼¹å¹•æŠ“å–å·¥å…·** | åŸºäº Streamlit å’Œ yt-dlp
    
    ğŸ’¡ **æç¤º:**
    - æŸäº›è§†é¢‘çš„å¼¹å¹•å¯èƒ½éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®
    - å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥
    - æ”¯æŒçš„è§†é¢‘å¹³å°ï¼šNiconicoã€Bilibili
    """)


def find_excel_files(folder_path):
    """æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰Excelæ–‡ä»¶"""
    excel_files = []
    folder_path = Path(folder_path)
    
    if not folder_path.exists():
        return False, "æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨"
    
    excel_extensions = ['.xlsx', '.xls', '.xlsm', '.xlsb']
    
    for ext in excel_extensions:
        excel_files.extend(folder_path.rglob(f'*{ext}'))
    
    return True, excel_files


def search_in_dataframe(df, col_name, target_values, keyword, case_sensitive=False, match_whole_word=False):
    """åœ¨DataFrameä¸­æœç´¢æ»¡è¶³æ¡ä»¶çš„è¡Œ"""
    matches = []
    
    if col_name not in df.columns:
        return matches, f"åˆ— '{col_name}' ä¸å­˜åœ¨"
    
    filtered_df = df[df[col_name].astype(str).isin([str(v) for v in target_values])]
    
    if len(filtered_df) == 0:
        return matches, "æœªæ‰¾åˆ°åŒ…å«æŒ‡å®šç›®æ ‡å€¼çš„è¡Œ"
    
    if not keyword or not keyword.strip():
        for idx, row in filtered_df.iterrows():
            matches.append({
                'row_index': idx,
                'row_data': row.to_dict(),
                'matched_column': col_name,
                'matched_value': row[col_name],
                'keyword_found': False,
                'keyword_matches': [],
                'match_count': 0
            })
        return matches, f"æ‰¾åˆ° {len(matches)} è¡ŒåŒ…å«ç›®æ ‡å€¼ï¼Œä½†æœªæœç´¢å…³é”®è¯"
    
    keyword = keyword.strip()
    flags = 0 if case_sensitive else re.IGNORECASE
    
    if match_whole_word:
        pattern = r'\b' + re.escape(keyword) + r'\b'
    else:
        pattern = re.escape(keyword)
    
    keyword_matches = 0
    
    for idx, row in filtered_df.iterrows():
        row_matches = []
        
        for col_idx, (col, cell_value) in enumerate(row.items()):
            if pd.isna(cell_value):
                continue
                
            cell_str = str(cell_value)
            cell_matches = list(re.finditer(pattern, cell_str, flags))
            
            for match in cell_matches:
                row_matches.append({
                    'column': col,
                    'original_value': cell_str,
                    'match_text': match.group(),
                    'start_pos': match.start(),
                    'end_pos': match.end(),
                    'replaced_value': cell_str[:match.start()] + f"**[{match.group()}]**" + cell_str[match.end():]
                })
        
        if row_matches:
            keyword_matches += 1
            matches.append({
                'row_index': idx,
                'row_data': row.to_dict(),
                'matched_column': col_name,
                'matched_value': row[col_name],
                'keyword_found': True,
                'keyword_matches': row_matches,
                'match_count': len(row_matches)
            })
    
    return matches, f"åœ¨ {len(filtered_df)} è¡Œç›®æ ‡è¡Œä¸­æ‰¾åˆ° {keyword_matches} è¡ŒåŒ…å«å…³é”®è¯"


def highlight_keyword(text, keyword, case_sensitive=False):
    """é«˜äº®æ˜¾ç¤ºå…³é”®è¯"""
    if not text or not keyword:
        return text
    
    flags = 0 if case_sensitive else re.IGNORECASE
    pattern = re.escape(keyword)
    
    if case_sensitive:
        highlighted = re.sub(f'({pattern})', r'**\1**', text)
    else:
        highlighted = re.sub(f'({pattern})', r'**\1**', text, flags=re.IGNORECASE)
    
    return highlighted


def replace_in_excel(file_path, replacements, backup=True):
    """åœ¨Excelæ–‡ä»¶ä¸­æ‰§è¡Œæ›¿æ¢æ“ä½œ"""
    try:
        file_path = Path(file_path)
        
        if backup:
            backup_path = file_path.parent / f"{file_path.stem}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}{file_path.suffix}"
            shutil.copy2(file_path, backup_path)
            st.info(f"ğŸ“ å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_path.name}")
        
        df = pd.read_excel(file_path, engine='openpyxl')
        
        replaced_count = 0
        for replacement in replacements:
            row_idx = replacement['row_index']
            col_name = replacement['column']
            old_text = replacement['original_value']
            new_text = replacement['new_value']
            
            if row_idx < len(df) and col_name in df.columns:
                current_value = df.at[row_idx, col_name]
                if pd.isna(current_value):
                    current_value = ""
                
                current_str = str(current_value)
                
                if replacement.get('replace_all', False):
                    flags = 0 if replacement.get('case_sensitive', False) else re.IGNORECASE
                    pattern = re.escape(replacement['search_keyword'])
                    if replacement.get('match_whole_word', False):
                        pattern = r'\b' + pattern + r'\b'
                    
                    new_str = re.sub(pattern, new_text, current_str, flags=flags)
                else:
                    new_str = old_text
                
                df.at[row_idx, col_name] = new_str
                replaced_count += 1
        
        df.to_excel(file_path, index=False, engine='openpyxl')
        return True, replaced_count
    except Exception as e:
        return False, str(e)


def excel_sreplace_page():
    st.sidebar.header("ğŸ”§ æœç´¢å‚æ•°è®¾ç½®")
    
    folder_path = st.sidebar.text_input(
        "ğŸ“ è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„:",
        placeholder="ä¾‹å¦‚: C:/Users/ç”¨æˆ·å/Documents/Excelæ–‡ä»¶",
        help="è¯·è¾“å…¥åŒ…å«Excelæ–‡ä»¶çš„æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„"
    )
    
    col_name = st.sidebar.text_input(
        "ğŸ“Š è¦æœç´¢çš„åˆ—å:",
        value="è§’è‰²å",
        placeholder="ä¾‹å¦‚: è§’è‰²å",
        help="è¯·è¾“å…¥è¦æœç´¢çš„Excelåˆ—åç§°"
    )
    
    target_values_input = st.sidebar.text_input(
        "ğŸ¯ åˆ—ç›®æ ‡å€¼ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰:",
        value="ç­é•¿,ç­é•¿å¤§äºº",
        placeholder="ä¾‹å¦‚: ç­é•¿,ç­é•¿å¤§äºº",
        help="è¯·è¾“å…¥è¦åœ¨æŒ‡å®šåˆ—ä¸­æŸ¥æ‰¾çš„å€¼ï¼Œå¤šä¸ªå€¼ç”¨é€—å·åˆ†éš”"
    )
    
    search_keyword = st.sidebar.text_input(
        "ğŸ”¤ è¦æŸ¥æ‰¾çš„å…³é”®è¯YYY:",
        value="ç§",
        placeholder="è¯·è¾“å…¥è¦åœ¨è¡Œä¸­æŸ¥æ‰¾çš„å…³é”®è¯",
        help="åœ¨æ»¡è¶³æ¡ä»¶çš„è¡Œä¸­æŸ¥æ‰¾æ­¤å…³é”®è¯"
    )
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("âš™ï¸ é«˜çº§é€‰é¡¹")
    
    case_sensitive = st.sidebar.checkbox(
        "åŒºåˆ†å¤§å°å†™",
        value=False,
        help="å‹¾é€‰åæœç´¢æ—¶åŒºåˆ†è‹±æ–‡å¤§å°å†™"
    )
    
    match_whole_word = st.sidebar.checkbox(
        "å…¨è¯åŒ¹é…",
        value=False,
        help="å‹¾é€‰ååªåŒ¹é…å®Œæ•´çš„è¯è¯­"
    )
    
    target_values = [v.strip() for v in target_values_input.split(',') if v.strip()]
    
    if not folder_path:
        st.warning("âš ï¸ è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„")
        return
    
    if not col_name:
        st.warning("âš ï¸ è¯·è¾“å…¥è¦æœç´¢çš„åˆ—å")
        return
    
    if not target_values:
        st.warning("âš ï¸ è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªç›®æ ‡å€¼")
        return
    
    success, result = find_excel_files(folder_path)
    
    if not success:
        st.error(f"âŒ {result}")
        return
    
    excel_files = result
    
    if not excel_files:
        st.warning("âš ï¸ åœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæ–‡ä»¶")
        return
    
    st.success(f"âœ… æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
    
    with st.expander("ğŸ“ æ‰¾åˆ°çš„Excelæ–‡ä»¶"):
        for i, file_path in enumerate(excel_files[:10]):
            st.write(f"{i+1}. {file_path.name}")
        
        if len(excel_files) > 10:
            st.info(f"... è¿˜æœ‰ {len(excel_files) - 10} ä¸ªæ–‡ä»¶")
    
    if st.button("ğŸš€ å¼€å§‹æœç´¢", type="primary", use_container_width=True):
        if not search_keyword.strip():
            st.warning("âš ï¸ å…³é”®è¯YYYä¸ºç©ºï¼Œå°†åªæ˜¾ç¤ºåŒ…å«ç›®æ ‡å€¼çš„è¡Œ")
        
        all_matches = []
        files_with_matches = 0
        total_matches = 0
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, file_path in enumerate(excel_files):
            progress = (i + 1) / len(excel_files)
            progress_bar.progress(progress)
            status_text.text(f"ğŸ” æ­£åœ¨å¤„ç†æ–‡ä»¶ {i+1}/{len(excel_files)}: {file_path.name}")
            
            try:
                df = pd.read_excel(file_path, engine='openpyxl')
                
                matches, message = search_in_dataframe(
                    df, col_name, target_values, search_keyword, 
                    case_sensitive, match_whole_word
                )
                
                if matches:
                    files_with_matches += 1
                    total_matches += len(matches)
                    
                    for match in matches:
                        match['file_path'] = str(file_path)
                        match['file_name'] = file_path.name
                        all_matches.append(match)
                
                import time
                time.sleep(0.1)
                
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
        
        progress_bar.progress(1.0)
        status_text.text(f"âœ… æœç´¢å®Œæˆï¼")
        
        st.session_state.search_results = all_matches
        st.session_state.search_keyword = search_keyword
        st.session_state.case_sensitive = case_sensitive
        st.session_state.match_whole_word = match_whole_word
        
        st.header("ğŸ“Š æœç´¢ç»“æœ")
        
        if total_matches == 0:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„è¡Œ")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ‰«ææ–‡ä»¶æ•°", len(excel_files))
            with col2:
                st.metric("åŒ…å«åŒ¹é…çš„æ–‡ä»¶", files_with_matches)
            with col3:
                st.metric("æ€»åŒ¹é…è¡Œæ•°", total_matches)
        else:
            st.success(f"âœ… åœ¨ {files_with_matches} ä¸ªæ–‡ä»¶ä¸­æ‰¾åˆ° {total_matches} è¡ŒåŒ¹é…ç»“æœ")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ‰«ææ–‡ä»¶æ•°", len(excel_files))
            with col2:
                st.metric("åŒ…å«åŒ¹é…çš„æ–‡ä»¶", files_with_matches)
            with col3:
                st.metric("æ€»åŒ¹é…è¡Œæ•°", total_matches)
            
            files_group = {}
            for match in all_matches:
                file_name = match['file_name']
                if file_name not in files_group:
                    files_group[file_name] = []
                files_group[file_name].append(match)
            
            for file_name, file_matches in files_group.items():
                with st.expander(f"ğŸ“„ {file_name} ({len(file_matches)} è¡ŒåŒ¹é…)", expanded=True):
                    st.write(f"**æ–‡ä»¶è·¯å¾„:** {file_matches[0]['file_path']}")
                    
                    display_data = []
                    for i, match in enumerate(file_matches):
                        row_data = match['row_data']
                        
                        row_display = {}
                        for col_name_display, cell_value in row_data.items():
                            if pd.isna(cell_value):
                                display_value = ""
                            else:
                                cell_str = str(cell_value)
                                
                                if search_keyword and search_keyword.strip():
                                    highlighted = highlight_keyword(
                                        cell_str, search_keyword, case_sensitive
                                    )
                                    row_display[col_name_display] = highlighted
                                else:
                                    row_display[col_name_display] = cell_str
                        
                        display_data.append({
                            'åºå·': i + 1,
                            'åŒ¹é…åˆ—å€¼': match['matched_value'],
                            'å…³é”®è¯åŒ¹é…æ•°': match.get('match_count', 0) if match.get('keyword_found', False) else 'æ— ',
                            **row_display
                        })
                    
                    if display_data:
                        all_columns = set()
                        for item in display_data:
                            all_columns.update(item.keys())
                        
                        base_columns = ['åºå·', 'åŒ¹é…åˆ—å€¼', 'å…³é”®è¯åŒ¹é…æ•°']
                        other_columns = [col for col in all_columns if col not in base_columns]
                        display_columns = base_columns + sorted(other_columns)
                        
                        display_df = pd.DataFrame(display_data)
                        
                        for col in display_columns:
                            if col not in display_df.columns:
                                display_df[col] = ""
                        
                        display_df = display_df[display_columns]
                        
                        st.dataframe(display_df, use_container_width=True)
                    
                    if search_keyword and search_keyword.strip():
                        st.subheader("ğŸ” åŒ¹é…è¯¦æƒ…")
                        for i, match in enumerate(file_matches):
                            if match.get('keyword_found', False) and match.get('keyword_matches'):
                                with st.expander(f"åŒ¹é…è¯¦æƒ… - è¡Œ {i+1}"):
                                    st.write(f"**æ–‡ä»¶:** {file_name}")
                                    st.write(f"**è¡Œç´¢å¼•:** {match['row_index'] + 2}")
                                    st.write(f"**åœ¨åˆ— '{col_name}' ä¸­æ‰¾åˆ°å€¼:** {match['matched_value']}")
                                    st.write(f"**å…³é”®è¯åŒ¹é…ä½ç½®:**")
                                    
                                    for kw_match in match['keyword_matches']:
                                        col_name_kw = kw_match['column']
                                        cell_value = kw_match['original_value']
                                        match_text = kw_match['match_text']
                                        start_pos = kw_match['start_pos']
                                        end_pos = kw_match['end_pos']
                                        
                                        context_start = max(0, start_pos - 20)
                                        context_end = min(len(cell_value), end_pos + 20)
                                        context = cell_value[context_start:context_end]
                                        
                                        if context_start > 0:
                                            context = "..." + context
                                        if context_end < len(cell_value):
                                            context = context + "..."
                                        
                                        highlighted_context = highlight_keyword(
                                            context, search_keyword, case_sensitive
                                        )
                                        
                                        st.write(f"**åˆ— '{col_name_kw}':** {highlighted_context}")
            
            if all_matches:
                st.header("ğŸ’¾ ä¸‹è½½æœç´¢ç»“æœ")
                
                download_data = []
                for match in all_matches:
                    row_data = {
                        'æ–‡ä»¶è·¯å¾„': match['file_path'],
                        'æ–‡ä»¶åç§°': match['file_name'],
                        'è¡Œç´¢å¼•': match['row_index'] + 2,
                        'åŒ¹é…åˆ—': match['matched_column'],
                        'åŒ¹é…åˆ—å€¼': match['matched_value'],
                        'æ˜¯å¦æ‰¾åˆ°å…³é”®è¯': 'æ˜¯' if match.get('keyword_found', False) else 'å¦',
                        'å…³é”®è¯åŒ¹é…æ•°': match.get('match_count', 0)
                    }
                    
                    for col_name_dl, cell_value in match['row_data'].items():
                        row_data[col_name_dl] = cell_value if not pd.isna(cell_value) else ""
                    
                    download_data.append(row_data)
                
                download_df = pd.DataFrame(download_data)
                
                csv_data = download_df.to_csv(index=False).encode('utf-8-sig')
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æœç´¢ç»“æœ(CSV)",
                    data=csv_data,
                    file_name=f"excel_search_results.csv",
                    mime="text/csv",
                    use_container_width=True
                )
    
    if st.session_state.get('search_results'):
        st.markdown("---")
        st.header("ğŸ”„ æ›¿æ¢åŠŸèƒ½")
        
        search_results = st.session_state.search_results
        search_keyword = st.session_state.search_keyword
        case_sensitive = st.session_state.case_sensitive
        match_whole_word = st.session_state.match_whole_word
        
        col1, col2 = st.columns(2)
        
        with col1:
            replace_keyword = st.text_input(
                "ğŸ”„ æ›¿æ¢ä¸º:",
                value="åƒ•",
                placeholder="è¯·è¾“å…¥æ›¿æ¢åçš„è¯è¯­",
                help="å°†æŸ¥æ‰¾åˆ°çš„å…³é”®è¯æ›¿æ¢ä¸ºæ­¤è¯è¯­"
            )
        
        with col2:
            replace_mode = st.radio(
                "æ›¿æ¢æ¨¡å¼:",
                ["æ›¿æ¢æ‰€æœ‰åŒ¹é…", "ä»…æ›¿æ¢é€‰ä¸­é¡¹"],
                help="é€‰æ‹©æ›¿æ¢å…¨éƒ¨åŒ¹é…é¡¹è¿˜æ˜¯ä»…æ›¿æ¢é€‰ä¸­çš„åŒ¹é…é¡¹",
                key = "tihuanA"
            )
            
            create_backup = st.checkbox(
                "åˆ›å»ºå¤‡ä»½æ–‡ä»¶",
                value=True,
                help="æ›¿æ¢å‰è‡ªåŠ¨åˆ›å»ºå¤‡ä»½æ–‡ä»¶"
            )
        
        st.subheader("ğŸ‘ï¸ æ›¿æ¢é¢„è§ˆ")
        
        all_replacements = []
        files_to_replace = set()
        
        for match in search_results:
            if match.get('keyword_found', False) and match.get('keyword_matches'):
                file_path = match['file_path']
                files_to_replace.add(file_path)
                
                for kw_match in match['keyword_matches']:
                    replacement_info = {
                        'file_path': file_path,
                        'file_name': match['file_name'],
                        'row_index': match['row_index'],
                        'column': kw_match['column'],
                        'original_value': kw_match['original_value'],
                        'search_keyword': search_keyword,
                        'replace_keyword': replace_keyword,
                        'start_pos': kw_match['start_pos'],
                        'end_pos': kw_match['end_pos'],
                        'case_sensitive': case_sensitive,
                        'match_whole_word': match_whole_word,
                        'replace_all': (replace_mode == "æ›¿æ¢æ‰€æœ‰åŒ¹é…")
                    }
                    
                    if replacement_info['replace_all']:
                        flags = 0 if case_sensitive else re.IGNORECASE
                        pattern = re.escape(search_keyword)
                        if match_whole_word:
                            pattern = r'\b' + pattern + r'\b'
                        
                        new_value = re.sub(pattern, replace_keyword, kw_match['original_value'], flags=flags)
                    else:
                        new_value = kw_match['original_value'][:kw_match['start_pos']] + replace_keyword + kw_match['original_value'][kw_match['end_pos']:]
                    
                    replacement_info['new_value'] = new_value
                    all_replacements.append(replacement_info)
        
        if all_replacements:
            st.info(f"ğŸ“Š å…±æ‰¾åˆ° {len(all_replacements)} å¤„å¯æ›¿æ¢å†…å®¹ï¼Œæ¶‰åŠ {len(files_to_replace)} ä¸ªæ–‡ä»¶")
            
            for file_path in files_to_replace:
                file_replacements = [r for r in all_replacements if r['file_path'] == file_path]
                
                with st.expander(f"ğŸ“„ {Path(file_path).name} - {len(file_replacements)} å¤„æ›¿æ¢"):
                    preview_data = []
                    
                    for i, replacement in enumerate(file_replacements[:10]):
                        original_text = replacement['original_value']
                        new_text = replacement['new_value']
                        
                        preview_data.append({
                            'åºå·': i + 1,
                            'è¡Œç´¢å¼•': replacement['row_index'] + 2,
                            'åˆ—å': replacement['column'],
                            'åŸæ–‡æœ¬': original_text,
                            'æ›¿æ¢å': new_text
                        })
                    
                    if preview_data:
                        st.dataframe(pd.DataFrame(preview_data), use_container_width=True)
                    
                    if len(file_replacements) > 10:
                        st.info(f"... è¿˜æœ‰ {len(file_replacements) - 10} å¤„æ›¿æ¢")
            
            if st.button("âœ… æ‰§è¡Œæ›¿æ¢", type="primary", use_container_width=True):
                with st.spinner("æ­£åœ¨æ‰§è¡Œæ›¿æ¢..."):
                    success_count = 0
                    failed_count = 0
                    
                    for file_path in files_to_replace:
                        file_replacements = [r for r in all_replacements if r['file_path'] == file_path]
                        success, count = replace_in_excel(file_path, file_replacements, create_backup)
                        
                        if success:
                            success_count += 1
                            st.success(f"âœ… æˆåŠŸæ›¿æ¢ {Path(file_path).name} ä¸­çš„ {count} å¤„å†…å®¹")
                        else:
                            failed_count += 1
                            st.error(f"âŒ æ›¿æ¢ {Path(file_path).name} å¤±è´¥: {count}")
                    
                    st.markdown("---")
                    st.subheader("æ›¿æ¢ç»“æœ")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("æˆåŠŸæ›¿æ¢æ–‡ä»¶æ•°", success_count)
                    with col2:
                        st.metric("å¤±è´¥æ–‡ä»¶æ•°", failed_count)
                    
                    if success_count > 0:
                        st.success("âœ… æ›¿æ¢å®Œæˆï¼")
                    else:
                        st.error("âŒ æ›¿æ¢å¤±è´¥ï¼")


def excel_ABC_page():
    st.title("ğŸ“Š Excelæ‰¹é‡å¤„ç†å·¥å…·")
    
    if 'excel_files' not in st.session_state:
        st.session_state.excel_files = []
    if 'dataframes' not in st.session_state:
        st.session_state.dataframes = {}
    
    def load_excel_file(file_path):
        try:
            df = pd.read_excel(file_path)
            return df
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None
    
    def check_condition(value, keywords, match_mode):
        if not keywords:
            return True
        
        value_str = str(value).lower()
        keywords_list = [kw.strip().lower() for kw in keywords if kw.strip()]
        
        if not keywords_list:
            return True
        
        if match_mode == "åŒæ—¶åŒ…å«æ‰€æœ‰å…³é”®è¯":
            return all(kw in value_str for kw in keywords_list)
        else:
            return any(kw in value_str for kw in keywords_list)
    
    def process_dataframe(df, col1, col2, keywords, match_mode, operation, params):
        df_copy = df.copy()
        modified_count = 0
        
        for idx, row in df_copy.iterrows():
            if check_condition(row[col1], keywords, match_mode):
                if operation == "åˆ é™¤å€¼":
                    target_value = params.get('target_value', '')
                    if target_value:
                        cell_value = str(row[col2])
                        if target_value in cell_value:
                            df_copy.at[idx, col2] = cell_value.replace(target_value, '')
                            modified_count += 1
                        
                elif operation == "æ›¿æ¢å€¼":
                    old_value = params.get('old_value', '')
                    new_value = params.get('new_value', '')
                    if old_value:
                        cell_value = str(row[col2])
                        if old_value in cell_value:
                            df_copy.at[idx, col2] = cell_value.replace(old_value, new_value)
                            modified_count += 1
                        
                elif operation == "ä¿®æ”¹ä¸­é—´å€¼":
                    value_a = params.get('value_a', '')
                    value_c = params.get('value_c', '')
                    new_value = params.get('new_value', '')
                    
                    cell_value = str(row[col2])
                    if value_a and value_c and value_a in cell_value and value_c in cell_value:
                        pos_a = cell_value.find(value_a)
                        pos_c = cell_value.find(value_c, pos_a + len(value_a))
                        
                        if pos_c > pos_a:
                            before = cell_value[:pos_a + len(value_a)]
                            after = cell_value[pos_c:]
                            df_copy.at[idx, col2] = before + new_value + after
                            modified_count += 1
        
        return df_copy, modified_count
    
    st.header("1ï¸âƒ£ ä¸Šä¼ æ–‡ä»¶å¤¹")
    uploaded_files = st.file_uploader(
        "é€‰æ‹©Excelæ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰", 
        type=['xlsx', 'xls'], 
        accept_multiple_files=True,
        key="excel_uploader"
    )
    
    if uploaded_files:
        st.session_state.excel_files = []
        st.session_state.dataframes = {}
        
        for uploaded_file in uploaded_files:
            temp_path = f"temp_{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            df = load_excel_file(temp_path)
            if df is not None:
                st.session_state.excel_files.append(uploaded_file.name)
                st.session_state.dataframes[uploaded_file.name] = df
            
            os.remove(temp_path)
        
        st.success(f"å·²åŠ è½½ {len(st.session_state.excel_files)} ä¸ªExcelæ–‡ä»¶")
        
        with st.expander("æŸ¥çœ‹å·²åŠ è½½çš„æ–‡ä»¶"):
            for file_name in st.session_state.excel_files:
                st.write(f"- {file_name}")
    
    if st.session_state.excel_files:
        st.header("2ï¸âƒ£ é…ç½®å¤„ç†è§„åˆ™")
        
        sample_file = st.session_state.excel_files[0]
        sample_df = st.session_state.dataframes[sample_file]
        columns = list(sample_df.columns)
        
        col_left, col_right = st.columns(2)
        
        with col_left:
            st.subheader("é€‰æ‹©åˆ—")
            col1 = st.selectbox("ç¬¬ä¸€åˆ—ï¼ˆæ¡ä»¶åˆ—ï¼‰", columns, key="col1")
            col2 = st.selectbox("ç¬¬äºŒåˆ—ï¼ˆæ“ä½œåˆ—ï¼‰", columns, key="col2")
        
        with col_right:
            st.subheader("æ¡ä»¶è®¾ç½®")
            keywords_input = st.text_area(
                "å…³é”®è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œç•™ç©ºåˆ™å¤„ç†æ‰€æœ‰è¡Œï¼‰",
                height=100,
                placeholder="è¾“å…¥å…³é”®è¯\nå¯è¾“å…¥å¤šä¸ª\næ¯è¡Œä¸€ä¸ª",
                key="keywords_input"
            )
            keywords = [kw.strip() for kw in keywords_input.split('\n') if kw.strip()]
            
            match_mode = st.radio(
                "åŒ¹é…æ¨¡å¼",
                ["åŒæ—¶åŒ…å«æ‰€æœ‰å…³é”®è¯", "åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯"],
                disabled=len(keywords) == 0,
                key="match_mode"
            )
        
        st.divider()
        
        st.subheader("é€‰æ‹©æ“ä½œ")
        operation = st.radio(
            "æ“ä½œç±»å‹",
            ["åˆ é™¤å€¼", "æ›¿æ¢å€¼", "ä¿®æ”¹ä¸­é—´å€¼"],
            key="operation"
        )
        
        params = {}
        
        if operation == "åˆ é™¤å€¼":
            st.info("ğŸ’¡ åˆ é™¤ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­åŒ…å«çš„æŒ‡å®šå†…å®¹")
            params['target_value'] = st.text_input(
                "è¦åˆ é™¤çš„å†…å®¹", 
                key="delete_value", 
                placeholder="ä¾‹å¦‚ï¼šåˆ é™¤'å¸…'ï¼Œåˆ™'æˆ‘å¥½å¸…'å˜ä¸º'æˆ‘å¥½'"
            )
            
        elif operation == "æ›¿æ¢å€¼":
            st.info("ğŸ’¡ å°†ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­çš„æŸä¸ªå†…å®¹æ›¿æ¢ä¸ºæ–°å†…å®¹")
            col_a, col_b = st.columns(2)
            with col_a:
                params['old_value'] = st.text_input(
                    "è¦æ›¿æ¢çš„å†…å®¹", 
                    key="old_value",
                    placeholder="ä¾‹å¦‚ï¼šå¸…"
                )
            with col_b:
                params['new_value'] = st.text_input(
                    "æ›¿æ¢ä¸º", 
                    key="new_value",
                    placeholder="ä¾‹å¦‚ï¼šä¸‘"
                )
                
        elif operation == "ä¿®æ”¹ä¸­é—´å€¼":
            st.info("ğŸ’¡ ä¿®æ”¹å¤¹åœ¨ä¸¤ä¸ªå€¼ä¹‹é—´çš„å†…å®¹")
            col_a, col_b, col_c = st.columns(3)
            with col_a:
                params['value_a'] = st.text_input("èµ·å§‹å€¼ A", key="value_a")
            with col_b:
                params['value_c'] = st.text_input("ç»“æŸå€¼ C", key="value_c")
            with col_c:
                params['new_value'] = st.text_input("æ–°çš„ä¸­é—´å€¼", key="middle_new_value")
        
        st.divider()
        
        st.header("3ï¸âƒ£ é¢„è§ˆå’Œæ‰§è¡Œ")
        
        col_preview, col_execute = st.columns(2)
        
        with col_preview:
            if st.button("ğŸ” é¢„è§ˆæ•ˆæœï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰", type="secondary", use_container_width=True):
                preview_df = st.session_state.dataframes[sample_file].copy()
                processed_df, count = process_dataframe(
                    preview_df, col1, col2, keywords, match_mode, operation, params
                )
                
                st.success(f"é¢„è§ˆå®Œæˆï¼å…±ä¿®æ”¹ {count} è¡Œæ•°æ®")
                
                col_before, col_after = st.columns(2)
                with col_before:
                    st.write("**å¤„ç†å‰**")
                    st.dataframe(preview_df[[col1, col2]].head(20), use_container_width=True)
                with col_after:
                    st.write("**å¤„ç†å**")
                    st.dataframe(processed_df[[col1, col2]].head(20), use_container_width=True)
        
        with col_execute:
            if st.button("âœ… æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶", type="primary", use_container_width=True):
                with st.spinner("æ­£åœ¨å¤„ç†..."):
                    results = {}
                    total_modified = 0
                    
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    for idx, file_name in enumerate(st.session_state.excel_files):
                        status_text.text(f"æ­£åœ¨å¤„ç†: {file_name}")
                        df = st.session_state.dataframes[file_name]
                        processed_df, count = process_dataframe(
                            df, col1, col2, keywords, match_mode, operation, params
                        )
                        results[file_name] = processed_df
                        total_modified += count
                        progress_bar.progress((idx + 1) / len(st.session_state.excel_files))
                    
                    status_text.empty()
                    progress_bar.empty()
                    
                    st.success(f"âœ… å¤„ç†å®Œæˆï¼å…±ä¿®æ”¹ {total_modified} è¡Œæ•°æ®")
                    
                    zip_buffer = BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for file_name, df in results.items():
                            excel_buffer = BytesIO()
                            df.to_excel(excel_buffer, index=False)
                            zip_file.writestr(f"processed_{file_name}", excel_buffer.getvalue())
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼ˆZIPï¼‰",
                        data=zip_buffer.getvalue(),
                        file_name="processed_excel_files.zip",
                        mime="application/zip",
                        use_container_width=True
                    )
    
    else:
        st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ Excelæ–‡ä»¶")
    
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### åŠŸèƒ½è¯´æ˜
        
        1. **ä¸Šä¼ æ–‡ä»¶**ï¼šé€‰æ‹©å¤šä¸ªExcelæ–‡ä»¶ï¼ˆæ”¯æŒ.xlsxå’Œ.xlsæ ¼å¼ï¼‰
        
        2. **é€‰æ‹©åˆ—**ï¼š
           - ç¬¬ä¸€åˆ—ï¼šæ¡ä»¶åˆ—ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ç¬¦åˆå¤„ç†æ¡ä»¶
           - ç¬¬äºŒåˆ—ï¼šæ“ä½œåˆ—ï¼Œå¯¹ç¬¦åˆæ¡ä»¶çš„è¡Œè¿›è¡Œæ“ä½œ
        
        3. **è®¾ç½®æ¡ä»¶**ï¼š
           - è¾“å…¥å…³é”®è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰
           - ç•™ç©ºè¡¨ç¤ºå¤„ç†æ‰€æœ‰è¡Œ
           - é€‰æ‹©åŒ¹é…æ¨¡å¼ï¼šåŒæ—¶åŒ…å«æ‰€æœ‰å…³é”®è¯ æˆ– åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯
        
        4. **é€‰æ‹©æ“ä½œ**ï¼š
           - **åˆ é™¤å€¼**ï¼šåˆ é™¤ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­åŒ…å«çš„æŒ‡å®šå†…å®¹ï¼ˆä¾‹å¦‚ï¼šåˆ é™¤"å¸…"ï¼Œ"æˆ‘å¥½å¸…"å˜ä¸º"æˆ‘å¥½"ï¼‰
           - **æ›¿æ¢å€¼**ï¼šå°†ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­çš„æŸä¸ªå†…å®¹æ›¿æ¢ä¸ºæ–°å†…å®¹ï¼ˆä¾‹å¦‚ï¼š"å¸…"æ›¿æ¢ä¸º"ä¸‘"ï¼Œ"æˆ‘å¥½å¸…"å˜ä¸º"æˆ‘å¥½ä¸‘"ï¼‰
           - **ä¿®æ”¹ä¸­é—´å€¼**ï¼šä¿®æ”¹å¤¹åœ¨Aå’ŒCä¹‹é—´çš„Bå€¼ï¼ˆä¾‹å¦‚ï¼šA="<"ï¼ŒC=">"ï¼Œå°†"<æ—§å€¼>"æ”¹ä¸º"<æ–°å€¼>"ï¼‰
        
        5. **é¢„è§ˆå’Œæ‰§è¡Œ**ï¼š
           - å…ˆé¢„è§ˆç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å¤„ç†æ•ˆæœ
           - ç¡®è®¤æ— è¯¯åæ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶
           - ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶å‹ç¼©åŒ…
        
        ### ä½¿ç”¨ç¤ºä¾‹
        
        **ç¤ºä¾‹1ï¼šåˆ é™¤æŒ‡å®šå†…å®¹**
        - æ¡ä»¶ï¼šç¬¬ä¸€åˆ—åŒ…å«"äº§å“"
        - æ“ä½œï¼šåˆ é™¤ç¬¬äºŒåˆ—ä¸­çš„"æ—§ç‰ˆ"
        - ç»“æœï¼š"æ—§ç‰ˆäº§å“è¯´æ˜" â†’ "äº§å“è¯´æ˜"
        
        **ç¤ºä¾‹2ï¼šæ›¿æ¢å†…å®¹**
        - æ¡ä»¶ï¼šç¬¬ä¸€åˆ—åŒ…å«"è¯„ä»·"
        - æ“ä½œï¼šå°†"å¸…"æ›¿æ¢ä¸º"ä¸‘"
        - ç»“æœï¼š"è¿™ä¸ªäººå¥½å¸…" â†’ "è¿™ä¸ªäººå¥½ä¸‘"
        
        **ç¤ºä¾‹3ï¼šä¿®æ”¹ä¸­é—´å€¼**
        - æ¡ä»¶ï¼šç¬¬ä¸€åˆ—åŒ…å«"æ ‡ç­¾"
        - æ“ä½œï¼šA="ã€"ï¼ŒC="ã€‘"ï¼Œæ–°å€¼="å·²å¤„ç†"
        - ç»“æœï¼š"ã€å¾…å¤„ç†ã€‘ä»»åŠ¡" â†’ "ã€å·²å¤„ç†ã€‘ä»»åŠ¡"
        """)
