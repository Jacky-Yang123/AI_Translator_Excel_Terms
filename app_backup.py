import os
import pandas as pd
import requests
import json
import time
from datetime import datetime
import sys
from io import BytesIO
import warnings
import concurrent.futures
from threading import Lock
from difflib import SequenceMatcher
import model_GRAND_match.model_grand_match
from concurrent.futures import ThreadPoolExecutor, as_completed
warnings.filterwarnings('ignore')
import re
import jieba
import streamlit as st
import io
import zipfile
from pathlib import Path
import glob
import webbrowser
import shutil
import difflib
import threading
import openpyxl
from queue import Queue
import concurrent.futures
import zipfile
from io import BytesIO
from typing import List, Dict, Tuple, Optional, Union
import streamlit as st
import yt_dlp
import os
import json
import pandas as pd
import xml.etree.ElementTree as ET
import subprocess
import platform
import time
import glob
import tempfile
import io
from datetime import datetime
import jieba
try:
    from wordcloud import WordCloud
    HAS_WORDCLOUD = True
except ImportError:
    HAS_WORDCLOUD = False
import matplotlib.pyplot as plt



# --- æ ¸å¿ƒå·¥å…·å‡½æ•°ç±» (æ”¾åœ¨å‡½æ•°å¤–ä»¥ä¾¿å¤ç”¨) ---
class Utils:
    @staticmethod
    def load_config(config_file):
        if os.path.exists(config_file):
            with open(config_file, 'r') as f: return json.load(f)
        return {
            "save_path": os.path.join(os.path.expanduser("~"), "Downloads", "Yt-DLP-Data"),
            "proxy": "",
            "naming_tmpl": "%(title)s"
        }

    @staticmethod
    def save_config(config_file, config):
        with open(config_file, 'w') as f: json.dump(config, f)

    @staticmethod
    def open_folder(path):
        if platform.system() == "Windows": os.startfile(path)
        elif platform.system() == "Darwin": subprocess.Popen(["open", path])
        else: subprocess.Popen(["xdg-open", path])

    @staticmethod
    def create_netscape_cookie_file(raw_cookie_str):
        if not raw_cookie_str or "=" not in raw_cookie_str: return None
        try:
            fd, path = tempfile.mkstemp(suffix='.txt', text=True)
            with os.fdopen(fd, 'w') as f:
                f.write("# Netscape HTTP Cookie File\n\n")
                for item in raw_cookie_str.split(';'):
                    if '=' in item:
                        key, value = item.strip().split('=', 1)
                        f.write(f".bilibili.com\tTRUE\t/\tFALSE\t253402300799\t{key}\t{value}\n")
            return path
        except: return None

    @staticmethod
    def get_chinese_font():
        system = platform.system()
        if system == "Windows":
            fonts = ["simhei.ttf", "msyh.ttc", "simsun.ttc"]
            for f in fonts:
                path = os.path.join("C:\\Windows\\Fonts", f)
                if os.path.exists(path): return path
        elif system == "Darwin": return "/System/Library/Fonts/PingFang.ttc"
        return None

    @staticmethod
    def generate_wordcloud_img(text_list):
        if not HAS_WORDCLOUD: return None
        if not text_list: return None
        
        full_text = " ".join([str(t) for t in text_list if str(t)])
        cut_text = " ".join(jieba.cut(full_text))
        
        font_path = Utils.get_chinese_font()
        # å¦‚æœæ²¡æœ‰ä¸­æ–‡å­—ä½“ï¼Œä¸ºäº†ä¸æŠ¥é”™ï¼Œä¸ä¼ font_pathå‚æ•°(è™½ç„¶ä¼šä¹±ç )
        params = {
            'background_color': 'white', 'width': 800, 'height': 400,
            'max_words': 200, 'colormap': 'viridis',
            'stopwords': {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'ä¹Ÿ', 'å°±', 'ä¸', 'éƒ½', 'å—', 'å•Š', 'å§', 'æˆ‘', 'è¿™'}
        }
        if font_path: params['font_path'] = font_path

        wc = WordCloud(**params).generate(cut_text)
        return wc

    @staticmethod
    def process_xml_to_excel(xml_path, excel_path):
        try:
            tree = ET.parse(xml_path)
            root = tree.getroot()
            data = []
            for d in root.findall('d'):
                content = d.text
                p_attr = d.get('p')
                if p_attr and content:
                    attrs = p_attr.split(',')
                    if len(attrs) >= 7:
                        data.append({
                            "æ—¶é—´": f"{int(float(attrs[0])//60):02d}:{int(float(attrs[0])%60):02d}",
                            "ç§’æ•°": round(float(attrs[0]), 2),
                            "å†…å®¹": content,
                            "ç”¨æˆ·Hash": attrs[6],
                            "æ—¥æœŸ": datetime.fromtimestamp(int(attrs[4])).strftime('%Y-%m-%d')
                        })
            if data:
                pd.DataFrame(data).sort_values(by="ç§’æ•°").to_excel(excel_path, index=False)
                return True, len(data)
            return False, 0
        except: return False, 0

    @staticmethod
    def process_json_to_excel(json_path, excel_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f: info = json.load(f)
            comments = info.get('comments', [])
            data = []
            for c in comments:
                data.append({
                    "ç”¨æˆ·": c.get('author'),
                    "å†…å®¹": c.get('text'),
                    "ç‚¹èµ": c.get('like_count'),
                    "æ—¶é—´": datetime.fromtimestamp(c.get('timestamp', 0)).strftime('%Y-%m-%d %H:%M') if c.get('timestamp') else "-"
                })
            if data:
                pd.DataFrame(data).to_excel(excel_path, index=False)
                return True, len(data)
            return False, 0
        except: return False, 0


# --- ä¸»å‡½æ•°ç»„ä»¶ ---
def ytdlp_downloader_app():
    """
    Streamlit ä¸»å‡½æ•°ç»„ä»¶ã€‚
    è°ƒç”¨æ­¤å‡½æ•°å³å¯åœ¨ä»»ä½•é¡µé¢æ¸²æŸ“ä¸‹è½½å™¨ã€‚
    """
    
    # 1. å±€éƒ¨çŠ¶æ€åˆå§‹åŒ– (State Init)
    if 'ytdlp_queue' not in st.session_state: st.session_state.ytdlp_queue = []
    if 'ytdlp_history' not in st.session_state: st.session_state.ytdlp_history = []
    if 'current_meta' not in st.session_state: st.session_state.current_meta = None
    if 'available_formats' not in st.session_state: st.session_state.available_formats = []
    
    CONFIG_FILE = "ytdlp_config.json"
    config = Utils.load_config(CONFIG_FILE)

    st.title("ğŸ“º YT-DLP å…¨èƒ½åª’ä½“ç»ˆç«¯")
    if not HAS_WORDCLOUD:
        st.warning("âš ï¸ æ£€æµ‹åˆ°æœªå®‰è£… wordcloud åº“ï¼Œè¯äº‘åŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†ä¸‹è½½åŠŸèƒ½æ­£å¸¸ã€‚")

    # --- ä¾§è¾¹æ  (Sidebar) ---
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        new_path = st.text_input("ğŸ“‚ ä¿å­˜è·¯å¾„", value=config['save_path'])
        if new_path != config['save_path']:
            config['save_path'] = new_path
            Utils.save_config(CONFIG_FILE, config)

        st.divider()
        st.subheader("ğŸª Cookie (VIP/è¯„è®º)")
        raw_cookie = st.text_area("ç²˜è´´ Cookie (SESSDATA=...)", height=100, help="F12æŠ“å–Bç«™è¯·æ±‚å¤´ä¸­çš„Cookie")
        
        temp_cookie_path = None
        if raw_cookie and "SESSDATA" in raw_cookie:
            temp_cookie_path = Utils.create_netscape_cookie_file(raw_cookie)
            if temp_cookie_path: st.success("âœ… Cookie å·²æ¿€æ´»")
        
        st.divider()
        config['proxy'] = st.text_input("ä»£ç† (Proxy)", value=config['proxy'])
        if st.button("ğŸ“‚ æ‰“å¼€æ–‡ä»¶å¤¹"): 
            if os.path.exists(config['save_path']): Utils.open_folder(config['save_path'])

    # --- é¡µé¢ä¸»ä½“ Tabs ---
    tab_dl, tab_review = st.tabs(["â¬‡ï¸ ä¸‹è½½ä¸è§£æ", "ğŸ‘ï¸ èµ„äº§ç®¡ç†ä¸è¯äº‘"])

    # === Tab 1: ä¸‹è½½ä¸­å¿ƒ ===
    with tab_dl:
        col1, col2 = st.columns([4,1])
        with col1: url = st.text_input("è§†é¢‘é“¾æ¥", key="url_input")
        with col2: btn_analyze = st.button("ğŸ” è§£æ", use_container_width=True, type="primary")

        # åŸºç¡€å‚æ•°æ„é€ å™¨
        def get_opts():
            opts = {'quiet': True, 'proxy': config['proxy'] or None, 'no_warnings': True, 'extractor_args': {'bilibili': {'comment_sort': 'time'}}}
            if temp_cookie_path: opts['cookiefile'] = temp_cookie_path
            return opts

        # è§£æé€»è¾‘
        if btn_analyze and url:
            with st.spinner("æ­£åœ¨è§£ææµ..."):
                try:
                    with yt_dlp.YoutubeDL(get_opts()) as ydl:
                        meta = ydl.extract_info(url, download=False)
                        st.session_state.current_meta = meta
                        formats = meta.get('formats', [])
                        heights = sorted(list(set([f.get('height') for f in formats if f.get('height')])), reverse=True)
                        st.session_state.available_formats = [f"{h}p" for h in heights]
                except Exception as e: st.error(f"è§£æé”™è¯¯: {e}")

        # ä»»åŠ¡é…ç½®å¡ç‰‡
        if st.session_state.current_meta:
            meta = st.session_state.current_meta
            st.divider()
            c1, c2 = st.columns([1, 2])
            with c1: 
                if meta.get('thumbnail'): st.image(meta['thumbnail'], use_container_width=True)
            with c2:
                st.subheader(meta.get('title'))
                quality = st.selectbox("ç”»è´¨é€‰æ‹©", ["âœ¨ æœ€ä½³ (MP4)"] + st.session_state.available_formats + ["ğŸµ çº¯éŸ³é¢‘"])
                
                cd1, cd2 = st.columns(2)
                with cd1: get_danmaku = st.checkbox("å¯¼å‡ºå¼¹å¹• Excel", value=True)
                with cd2: get_comments = st.checkbox("å¯¼å‡ºè¯„è®º Excel", value=True)
                
                limit_cmt = 100
                if get_comments: limit_cmt = st.slider("è¯„è®ºæŠ“å–é‡", 10, 5000, 500, step=50)

                if st.button("â• åŠ å…¥é˜Ÿåˆ—", type="primary"):
                    st.session_state.ytdlp_queue.append({
                        "url": meta['webpage_url'], "title": meta['title'], "quality": quality,
                        "danmaku": get_danmaku, "comments": get_comments, "limit_cmt": limit_cmt
                    })
                    st.success("å·²åŠ å…¥ä¸‹è½½é˜Ÿåˆ—")

        # é˜Ÿåˆ—æ‰§è¡Œ
        if st.session_state.ytdlp_queue:
            st.divider()
            if st.button(f"ğŸš€ å¼€å§‹ä¸‹è½½ ({len(st.session_state.ytdlp_queue)} ä¸ªä»»åŠ¡)", type="primary", use_container_width=True):
                prog = st.progress(0)
                for idx, task in enumerate(st.session_state.ytdlp_queue):
                    opts = get_opts()
                    opts.update({'outtmpl': os.path.join(config['save_path'], f"{task['title']}.%(ext)s"), 'ignoreerrors': True, 'merge_output_format': 'mp4', 'writeinfojson': True})
                    
                    # ç”»è´¨å‚æ•°
                    if "çº¯éŸ³é¢‘" in task['quality']: opts['format'] = 'bestaudio/best'
                    elif "æœ€ä½³" in task['quality']: opts['format'] = 'bestvideo+bestaudio/best'
                    else: opts['format'] = f"bestvideo[height={task['quality'].replace('p','')}]" + "+bestaudio/best"

                    if task['danmaku']: opts.update({'writesubtitles': True, 'allsubtitles': True})
                    if task['comments']: opts.update({'getcomments': True, 'max_comments': task['limit_cmt']})

                    try:
                        with yt_dlp.YoutubeDL(opts) as ydl:
                            ydl.download([task['url']])
                            base = os.path.join(config['save_path'], task['title'])
                            
                            # Excel è½¬æ¢
                            if task['danmaku']:
                                xmls = glob.glob(f"{base}*.xml")
                                if xmls: 
                                    Utils.process_xml_to_excel(xmls[0], f"{base}_å¼¹å¹•.xlsx")
                                    try: os.remove(xmls[0])
                                    except: pass
                            
                            if task['comments']:
                                json_f = f"{base}.info.json"
                                if os.path.exists(json_f):
                                    Utils.process_json_to_excel(json_f, f"{base}_è¯„è®º.xlsx")
                                    try: os.remove(json_f)
                                    except: pass

                            st.session_state.ytdlp_history.append({"title": task['title'], "video_path": f"{base}.mp4", "base_name": base})
                    except Exception as e: st.error(f"ä»»åŠ¡å¤±è´¥: {e}")
                    prog.progress((idx+1)/len(st.session_state.ytdlp_queue))
                
                st.session_state.ytdlp_queue = []
                st.success("å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")

    # === Tab 2: èµ„äº§ä¸è¯äº‘ ===
    with tab_review:
        if not st.session_state.ytdlp_history: st.info("æš‚æ— å†å²è®°å½•")
        
        for item in reversed(st.session_state.ytdlp_history):
            with st.expander(f"ğŸ¥ {item['title']}", expanded=True):
                c_vid, c_data = st.columns([1, 1.5])
                with c_vid:
                    if os.path.exists(item['video_path']): st.video(item['video_path'])
                    else: st.warning("æ–‡ä»¶æœªæ‰¾åˆ°")
                
                with c_data:
                    dm_path = f"{item['base_name']}_å¼¹å¹•.xlsx"
                    cm_path = f"{item['base_name']}_è¯„è®º.xlsx"
                    
                    t1, t2 = st.tabs(["ğŸ“Š æ•°æ®", "â˜ï¸ è¯äº‘"])
                    with t1:
                        if os.path.exists(dm_path): st.dataframe(pd.read_excel(dm_path), height=150)
                        if os.path.exists(cm_path): st.dataframe(pd.read_excel(cm_path), height=150)
                    
                    with t2:
                        if not HAS_WORDCLOUD:
                            st.error("è¯äº‘åº“ç¼ºå¤±ï¼Œè¯·å®‰è£… wordcloud")
                        else:
                            wc1, wc2 = st.columns(2)
                            with wc1:
                                if os.path.exists(dm_path) and st.button("å¼¹å¹•è¯äº‘", key=f"d_{item['title']}"):
                                    wc = Utils.generate_wordcloud_img(pd.read_excel(dm_path)['å†…å®¹'].tolist())
                                    if wc: 
                                        st.image(wc.to_array(), use_container_width=True)
                                        buf = io.BytesIO()
                                        wc.to_image().save(buf, format='PNG')
                                        st.download_button("ä¸‹è½½", buf.getvalue(), "dm_wc.png", "image/png", key=f"dd_{item['title']}")
                            with wc2:
                                if os.path.exists(cm_path) and st.button("è¯„è®ºè¯äº‘", key=f"c_{item['title']}"):
                                    wc = Utils.generate_wordcloud_img(pd.read_excel(cm_path)['å†…å®¹'].tolist())
                                    if wc: 
                                        st.image(wc.to_array(), use_container_width=True)
                                        buf = io.BytesIO()
                                        wc.to_image().save(buf, format='PNG')
                                        st.download_button("ä¸‹è½½", buf.getvalue(), "cm_wc.png", "image/png", key=f"dc_{item['title']}")
class MultiAPIExcelTranslator:
    def __init__(self, api_key, api_provider, api_url, model, context_size=10, max_retries=10):
        self.api_key = api_key
        self.api_provider = api_provider
        self.api_url = api_url
        self.model = model
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.context_history = {}  # æŒ‰è¯­è¨€åˆ†åˆ«å­˜å‚¨ä¸Šä¸‹æ–‡
        self.term_dict = {}
        self.role_column = None
        self.context_size = context_size
        self.max_retries = max_retries
        
        # æ”¹ä¸ºæŒ‰è¯­è¨€å­˜å‚¨æœ¯è¯­åº“
        self.term_base_dict = {}  # {è¯­è¨€: [{source: xxx, target: xxx}]}
        
        self.role_personality_dict = {}
        self.current_text_terms = {}
        self.current_role_personality = None
        self.target_languages = ["è‹±æ–‡"]
        self.language_column_names = {"è‹±æ–‡": "è‹±æ–‡ç¿»è¯‘ç»“æœ"}

        # æ–°å¢ï¼šè§’è‰²æ˜ å°„è¡¨
        self.role_mapping = {}
        self.enable_fuzzy_match = False
        self.fuzzy_threshold = 0.6

        self.init_chinese_tokenizer()

    def init_chinese_tokenizer(self):
        try:
            self.chinese_tokenizer = jieba
            st.success("âœ… ä¸­æ–‡åˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            st.warning(f"âš ï¸ ä¸­æ–‡åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.chinese_tokenizer = None

    def tokenize_chinese_text(self, text):
        if not text or pd.isna(text):
            return []

        text = str(text).strip()
        if not text:
            return []

        try:
            if self.chinese_tokenizer:
                words = self.chinese_tokenizer.cut(text)
                return [word for word in words if word.strip() and re.search(r'[\w\u4e00-\u9fa5]', word)]
            else:
                return [char for char in text if char.strip() and re.search(r'[\w\u4e00-\u9fa5]', char)]
        except Exception as e:
            st.warning(f"âš ï¸ ä¸­æ–‡åˆ†è¯å¤±è´¥: {e}")
            return [char for char in text if char.strip()]

    def clean_role_name(self, role_name):
        """æ¸…ç†è§’è‰²åç§°ï¼šå»é™¤é¢å¤–æ ‡è®°ã€ç©ºæ ¼ç­‰"""
        if not role_name or pd.isna(role_name):
            return ""

        role_name = str(role_name).strip()
        role_name = re.sub(r'\|.*$', '', role_name)
        role_name = re.sub(r'[\s\u3000]+', '', role_name)
        role_name = role_name.strip()

        return role_name

    def fuzzy_match_role(self, role_name, threshold=None):
        """æ¨¡ç³ŠåŒ¹é…è§’è‰²åç§°"""
        if not role_name or not self.role_personality_dict:
            return None, 0

        if threshold is None:
            threshold = self.fuzzy_threshold

        cleaned_role = self.clean_role_name(role_name)
        if not cleaned_role:
            return None, 0

        if role_name in self.role_mapping:
            return self.role_mapping[role_name], 1.0

        best_match = None
        best_score = 0

        for official_role in self.role_personality_dict.keys():
            cleaned_official = self.clean_role_name(official_role)

            if cleaned_role == cleaned_official:
                return official_role, 1.0

            score = difflib.SequenceMatcher(None, cleaned_role, cleaned_official).ratio()

            if cleaned_role in cleaned_official or cleaned_official in cleaned_role:
                score = max(score, 0.8)

            if score > best_score:
                best_score = score
                best_match = official_role

        if best_score >= threshold:
            return best_match, best_score

        return None, best_score

    def analyze_role_matches(self, df, role_col):
        """åˆ†ææ•°æ®ä¸­çš„æ‰€æœ‰è§’è‰²åç§°"""
        if not role_col or role_col not in df.columns:
            return {}

        unique_roles = df[role_col].dropna().unique()
        fuzzy_matches = {}

        for role in unique_roles:
            role_str = str(role).strip()
            if not role_str:
                continue

            if role_str in self.role_mapping:
                continue

            if role_str in self.role_personality_dict:
                continue

            matched_role, score = self.fuzzy_match_role(role_str)

            if matched_role:
                if role_str not in fuzzy_matches:
                    fuzzy_matches[role_str] = []
                fuzzy_matches[role_str].append((matched_role, score))

                if score < 1.0:
                    for official_role in self.role_personality_dict.keys():
                        if official_role == matched_role:
                            continue
                        alt_score = difflib.SequenceMatcher(
                            None,
                            self.clean_role_name(role_str),
                            self.clean_role_name(official_role)
                        ).ratio()

                        if alt_score >= self.fuzzy_threshold * 0.8:
                            fuzzy_matches[role_str].append((official_role, alt_score))

                fuzzy_matches[role_str].sort(key=lambda x: x[1], reverse=True)

        return fuzzy_matches

    def find_role_personality(self, role_name):
        """æŸ¥æ‰¾è§’è‰²æ€§æ ¼æè¿°"""
        if not role_name or not self.role_personality_dict:
            return None

        role_name = str(role_name).strip()
        if not role_name:
            return None

        if role_name in self.role_mapping:
            mapped_role = self.role_mapping[role_name]
            return self.role_personality_dict.get(mapped_role)

        if role_name in self.role_personality_dict:
            return self.role_personality_dict[role_name]

        if self.enable_fuzzy_match:
            matched_role, score = self.fuzzy_match_role(role_name)
            if matched_role:
                return self.role_personality_dict[matched_role]

        return None

    def add_to_context(self, original, translation, role=None, language="è‹±æ–‡"):
        """ä¸ºæŒ‡å®šè¯­è¨€æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆç¡®ä¿è¯­è¨€ç‹¬ç«‹ï¼‰"""
        if language not in self.context_history:
            self.context_history[language] = []
        
        self.context_history[language].append((original, translation, role))
        if len(self.context_history[language]) > self.context_size:
            self.context_history[language].pop(0)

    def build_context_prompt(self, language="è‹±æ–‡"):
        """ä¸ºæŒ‡å®šè¯­è¨€æ„å»ºä¸Šä¸‹æ–‡æç¤ºï¼ˆç¡®ä¿è¯­è¨€ç‹¬ç«‹ï¼‰"""
        if language not in self.context_history or not self.context_history[language]:
            return ""

        context_str = f"\n\n### é‡è¦ä¸Šä¸‹æ–‡å‚è€ƒï¼ˆ{language}ç¿»è¯‘ï¼‰ï¼š\n"
        for i, (orig, trans, role) in enumerate(self.context_history[language], 1):
            role_info = f" [{role}]" if role else ""
            context_str += f"å‰æ–‡{i}{role_info}:\nåŸæ–‡: {orig}\n{language}è¯‘æ–‡: {trans}\n\n"
        return context_str

    def find_matched_terms(self, text, language):
        """ä¸ºæŒ‡å®šè¯­è¨€æŸ¥æ‰¾åŒ¹é…çš„æœ¯è¯­"""
        if language not in self.term_base_dict or not self.term_base_dict[language]:
            return {}

        words = self.tokenize_chinese_text(text)
        matched_terms = {}

        # è¯çº§åˆ«åŒ¹é…
        for word in words:
            for term_entry in self.term_base_dict[language]:
                if term_entry['source'] == word:
                    if word not in matched_terms:
                        matched_terms[word] = []
                    if term_entry['target'] not in matched_terms[word]:
                        matched_terms[word].append(term_entry['target'])

        # çŸ­è¯­çº§åˆ«åŒ¹é…
        for term_entry in self.term_base_dict[language]:
            term = term_entry['source']
            if term in text:
                if term not in matched_terms:
                    matched_terms[term] = []
                if term_entry['target'] not in matched_terms[term]:
                    matched_terms[term].append(term_entry['target'])

        return matched_terms

    def build_term_base_prompt(self, text, language):
        """ä¸ºæŒ‡å®šè¯­è¨€æ„å»ºæœ¯è¯­åº“æç¤º"""
        matched_terms = self.find_matched_terms(text, language)

        if not matched_terms:
            return ""

        term_base_str = f"\n\n### æœ¯è¯­åº“åŒ¹é…ï¼š\n"
        
        for orig, trans_list in matched_terms.items():
            if len(trans_list) == 1:
                term_base_str += f"- ã€Œ{orig}ã€ â†’ {language}è¯‘åï¼šã€Œ{trans_list[0]}ã€\n"
            else:
                term_base_str += f"- ã€Œ{orig}ã€ â†’ {language}è¯‘åå€™é€‰ï¼š{' / '.join([f'ã€Œ{t}ã€' for t in trans_list])} ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„ï¼‰\n"

        return term_base_str

    def build_role_personality_prompt(self, role_name):
        if not role_name:
            return ""

        personality = self.find_role_personality(role_name)
        self.current_role_personality = personality

        if not personality:
            return ""

        mapped_role = self.role_mapping.get(str(role_name).strip())
        if mapped_role and mapped_role != str(role_name).strip():
            role_personality_str = f"\n\n### è§’è‰²æ€§æ ¼æè¿°ï¼š\nè§’è‰²ã€Œ{role_name}ã€(æ˜ å°„ä¸ºã€Œ{mapped_role}ã€)çš„æ€§æ ¼ç‰¹ç‚¹ï¼š{personality}\n"
        else:
            role_personality_str = f"\n\n### è§’è‰²æ€§æ ¼æè¿°ï¼š\nè§’è‰²ã€Œ{role_name}ã€çš„æ€§æ ¼ç‰¹ç‚¹ï¼š{personality}\n"

        return role_personality_str

    def set_target_languages(self, languages, column_names):
        """è®¾ç½®ç›®æ ‡è¯­è¨€åˆ—è¡¨å’Œå¯¹åº”çš„åˆ—å"""
        self.target_languages = languages
        self.language_column_names = column_names
        # ä¸ºæ¯ç§è¯­è¨€åˆå§‹åŒ–ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡å†å²
        for lang in languages:
            if lang not in self.context_history:
                self.context_history[lang] = []
    def set_target_language(self, language):
        self.target_language = language
    def get_language_specific_requirements(self, language):
        language_requirements = {
            "è‹±æ–‡": """
è‹±æ–‡ç¿»è¯‘è¦æ±‚ï¼š
- ä¿æŒè‡ªç„¶æµç•…ï¼Œç¬¦åˆè‹±è¯­æ¯è¯­è€…çš„è¡¨è¾¾ä¹ æƒ¯
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
""",
            "æ—¥æ–‡": """
æ—¥æ–‡ç¿»è¯‘è¦æ±‚ï¼š
- æ³¨æ„æ•¬ä½“å’Œå¸¸ä½“çš„ä½¿ç”¨ï¼Œæ ¹æ®è§’è‰²å…³ç³»å’Œåœºæ™¯é€‰æ‹©åˆé€‚çš„è¯­ä½“
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œç¬¦åˆæ—¥è¯­è¡¨è¾¾ä¹ æƒ¯
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”å’Œè¯­å°¾
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
""",
            "éŸ©æ–‡": """
éŸ©æ–‡ç¿»è¯‘è¦æ±‚ï¼š
- æ³¨æ„å°Šæ•¬è¯­å’Œéå°Šæ•¬è¯­çš„ä½¿ç”¨ï¼Œæ ¹æ®è§’è‰²å…³ç³»å’Œåœºæ™¯é€‰æ‹©åˆé€‚çš„è¯­ä½“
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œç¬¦åˆéŸ©è¯­è¡¨è¾¾ä¹ æƒ¯
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
"""
        }

        if language in language_requirements:
            return language_requirements[language]
        else:
            return f"""
{language}ç¿»è¯‘è¦æ±‚ï¼š
- æ³¨æ„æ­£å¼å’Œéæ­£å¼è¯­ä½“çš„ä½¿ç”¨ï¼Œæ ¹æ®è§’è‰²å…³ç³»å’Œåœºæ™¯é€‰æ‹©åˆé€‚çš„è¯­ä½“
- æ¸¸æˆUIæ–‡æœ¬è¦ç®€æ´æ˜äº†ï¼Œç¬¦åˆ{language}è¡¨è¾¾ä¹ æƒ¯
- è§’è‰²å¯¹è¯è¦ç¬¦åˆäººç‰©æ€§æ ¼ï¼Œä½¿ç”¨æ°å½“çš„è¯­æ°”
- ä¸“æœ‰åè¯å’Œæœ¯è¯­è¦ä¿æŒä¸€è‡´æ€§
- æ–‡åŒ–ç‰¹å®šè¡¨è¾¾è¦è¿›è¡Œé€‚å½“çš„æœ¬åœ°åŒ–å¤„ç†
"""

    def is_translation_error(self, response_text, original_text):
        if not response_text or response_text.strip() == "":
            return True
        if len(response_text) < len(original_text) * 0.1:
            return True
        return False

    def translate_text_with_retry(self, text, target_language, custom_requirements="", role=None):
        if not text or pd.isna(text) or str(text).strip() == "":
            return text

        last_exception = None

        for attempt in range(self.max_retries):
            try:
                translated_text = self._translate_single_attempt(text, target_language, custom_requirements, role)

                if not self.is_translation_error(translated_text, text):
                    return translated_text
                else:
                    st.warning(f"âš ï¸ [{target_language}] ç¬¬ {attempt + 1} æ¬¡ç¿»è¯‘ç»“æœå¼‚å¸¸ï¼Œå‡†å¤‡é‡è¯•...")

            except requests.exceptions.RequestException as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] ç½‘ç»œé”™è¯¯ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            except requests.exceptions.Timeout as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] è¯·æ±‚è¶…æ—¶ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            except requests.exceptions.ConnectionError as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] è¿æ¥é”™è¯¯ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            except Exception as e:
                last_exception = e
                st.warning(f"âš ï¸ [{target_language}] APIé”™è¯¯ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {e}")

            if attempt < self.max_retries - 1:
                wait_time = min(2 ** attempt, 60)
                time.sleep(wait_time)

        st.error(f"âŒ [{target_language}] ç¿»è¯‘å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.max_retries}")
        if last_exception:
            st.error(f"æœ€åé”™è¯¯: {last_exception}")

        return text

    def _translate_single_attempt(self, text, target_language, custom_requirements="", role=None):
        # ä¸ºå½“å‰è¯­è¨€æ„å»ºç‹¬ç«‹çš„ä¸Šä¸‹æ–‡å’Œæœ¯è¯­æç¤º
        context_prompt = self.build_context_prompt(target_language)
        term_base_prompt = self.build_term_base_prompt(text, target_language)
        role_personality_prompt = self.build_role_personality_prompt(role) if role else ""
        language_requirements = self.get_language_specific_requirements(target_language)

        role_prompt = ""
        if role and not pd.isna(role) and str(role).strip() != "":
            role_prompt = f"\nå½“å‰æ–‡æœ¬çš„è¯´è¯äºº: {role}\n"

        prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ã€‚

## è§’è‰²ä¿¡æ¯ï¼š
{role_prompt}{role_personality_prompt}

## {target_language}ç¿»è¯‘è§„èŒƒï¼ˆä¼˜å…ˆçº§æœ€ä½ï¼‰ï¼š
{language_requirements}

## ç”¨æˆ·è‡ªå®šä¹‰è¦æ±‚ï¼ˆä¼˜å…ˆçº§ç¬¬ä¸€é«˜ï¼‰ï¼š
{custom_requirements}

{context_prompt}

{term_base_prompt}

## å¾…ç¿»è¯‘æ–‡æœ¬ï¼š
{text}

## é‡è¦è¯´æ˜ï¼ˆä¼˜å…ˆçº§ç¬¬äºŒé«˜ï¼‰ï¼š
1. è¯·åªè¿”å›{target_language}ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å¤‡æ³¨
2. æœ¯è¯­åº“ä¸­çš„ç‰¹å®šè¯æ±‡ç¿»è¯‘ï¼Œå¦‚æœæ˜¯äººåæˆ–è€…å›ºå®šç‰¹æ®Šåç§°éœ€è¦ä¸¥æ ¼é‡‡ç”¨ç›¸åŒçš„ç¿»è¯‘,ä½†æ³¨æ„å¦‚æœæ˜¯ä¸€äº›æ™®é€šçš„è¯æ±‡åˆ™çœ‹å¥å­ç¿»è¯‘ä¸å¿…ä¸€å®šæŒ‰ç…§æœ¯è¯­åº“æ¥
3. å¦‚æœä¸€ä¸ªæœ¯è¯­æœ‰å¤šä¸ªå€™é€‰è¯‘åï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„
4. è¯·æ ¹æ®è§’è‰²æ€§æ ¼æè¿°è°ƒæ•´ç¿»è¯‘é£æ ¼å’Œè¯­æ°”
5. å‚è€ƒä¸Šä¸‹æ–‡ä¸­çš„{target_language}è¯‘æ–‡é£æ ¼ï¼Œä¿æŒç¿»è¯‘ä¸€è‡´æ€§ï¼Œä½†æ˜¯æœ‰çš„æ—¶å€™ä¸Šä¸‹æ–‡è§’è‰²å¯èƒ½ä¸æ˜¯ä¸€ä¸ªäººæˆ–è€…åªæ˜¯UIç¿»è¯‘ï¼Œä½ è¿˜æ˜¯éœ€è¦å‚è€ƒåŸæ–‡åˆ¤æ–­æ˜¯å¦å‚è€ƒä¸Šä¸‹æ–‡

{target_language}ç¿»è¯‘ç»“æœï¼š
"""

        data = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": f"ä½ æ˜¯ä¸€åä¸“ä¸šçš„{target_language}ç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿æ¸¸æˆæœ¬åœ°åŒ–ã€UIç•Œé¢ç¿»è¯‘å’Œè§’è‰²æ–‡æ¡ˆç¿»è¯‘ã€‚ä½ æ­£åœ¨è¿›è¡Œä¸­æ–‡åˆ°{target_language}çš„ç¿»è¯‘å·¥ä½œã€‚è¯·ç¡®ä¿æœ¯è¯­ä¸€è‡´æ€§å’Œé£æ ¼ç»Ÿä¸€ï¼Œå¹¶æ ¹æ®è§’è‰²ç‰¹ç‚¹è°ƒæ•´ç¿»è¯‘é£æ ¼ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.2,
            "max_tokens": 4000
        }
        print(str(prompt))
        response = requests.post(self.api_url, headers=self.headers, json=data, timeout=60)
        response.raise_for_status()
        result = response.json()
        translated_text = result["choices"][0]["message"]["content"].strip()
        
        translated_text = self.clean_translation(translated_text)
        # å°†ç¿»è¯‘ç»“æœæ·»åŠ åˆ°è¯¥è¯­è¨€çš„ç‹¬ç«‹ä¸Šä¸‹æ–‡ä¸­
        self.add_to_context(text, translated_text, role, target_language)

        return translated_text

    def translate_text(self, text, target_language, custom_requirements="", role=None):
        return self.translate_text_with_retry(text, target_language, custom_requirements, role)

    def clean_translation(self, text):
        if text.startswith('"') and text.endswith('"'):
            text = text[1:-1]
        elif text.startswith("'") and text.endswith("'"):
            text = text[1:-1]

        prefixes = [
            "moniorï¼š", "monior:", "è§’è‰²ï¼š", "è§’è‰²:",
            "ç¿»è¯‘ï¼š", "ç¿»è¯‘:", "è¯‘æ–‡ï¼š", "è¯‘æ–‡:",
            "ç»“æœï¼š", "ç»“æœ:", "ç¿»è¯‘ç»“æœï¼š", "ç¿»è¯‘ç»“æœ:",
            "è‹±æ–‡ç¿»è¯‘ç»“æœï¼š", "æ—¥æ–‡ç¿»è¯‘ç»“æœï¼š", "éŸ©æ–‡ç¿»è¯‘ç»“æœï¼š",
            "è‹±æ–‡ï¼š", "æ—¥æ–‡ï¼š", "éŸ©æ–‡ï¼š", "æ³•æ–‡ï¼š", "å¾·æ–‡ï¼š",
        ]
        for prefix in prefixes:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()

        return text

    def reset_context(self):
        self.context_history = {}
        self.term_dict = {}
        self.term_base_dict = {}
        self.role_column = None
        self.current_text_terms = {}
        self.current_role_personality = None
        self.role_mapping = {}
    def load_term_base(self, df, source_col, target_col):
        """åŠ è½½æœ¯è¯­åº“ - æ”¯æŒé‡å¤æœ¯è¯­"""
        try:
            # æ„å»ºæœ¯è¯­åº“åˆ—è¡¨ï¼Œæ”¯æŒé‡å¤æœ¯è¯­
            self.term_base_list = []
            missing_count = 0
            
            for _, row in df.iterrows():
                source = row[source_col]
                target = row[target_col]
                
                if pd.isna(source) or pd.isna(target):
                    missing_count += 1
                    continue
                    
                source = str(source).strip()
                target = str(target).strip()
                
                if source and target:
                    # ä¸å†æ£€æŸ¥é‡å¤ï¼Œç›´æ¥æ·»åŠ åˆ°åˆ—è¡¨
                    self.term_base_list.append({
                        'source': source,
                        'target': target
                    })
            
            st.success(f"âœ… æˆåŠŸåŠ è½½æœ¯è¯­: {len(self.term_base_list)} æ¡")
            if missing_count > 0:
                st.warning(f"âš ï¸ è·³è¿‡ {missing_count} æ¡ä¸å®Œæ•´çš„è®°å½•")
            
            return True
        except Exception as e:
            st.error(f"âŒâŒ åŠ è½½æœ¯è¯­åº“å¤±è´¥: {e}")
            return False
    def load_term_base_multilang(self, df, source_col, target_cols_dict):
        """
        åŠ è½½å¤šè¯­è¨€æœ¯è¯­åº“
        df: æœ¯è¯­åº“DataFrame
        source_col: åŸæ–‡åˆ—å
        target_cols_dict: {è¯­è¨€: åˆ—å} ä¾‹å¦‚ {"è‹±æ–‡": "English", "æ—¥æ–‡": "Japanese"}
        """
        try:
            self.term_base_dict = {}
            
            for language, target_col in target_cols_dict.items():
                if target_col not in df.columns:
                    st.warning(f"âš ï¸ æœ¯è¯­åº“ä¸­æœªæ‰¾åˆ° {language} å¯¹åº”çš„åˆ—: {target_col}")
                    continue
                
                self.term_base_dict[language] = []
                missing_count = 0

                for _, row in df.iterrows():
                    source = row[source_col]
                    target = row[target_col]

                    if pd.isna(source) or pd.isna(target):
                        missing_count += 1
                        continue

                    source = str(source).strip()
                    target = str(target).strip()

                    if source and target:
                        # å°†åŸæ–‡æ·»åŠ åˆ°åˆ†è¯è¯å…¸
                        try:
                            self.chinese_tokenizer.add_word(source)
                        except:
                            pass

                        self.term_base_dict[language].append({
                            'source': source,
                            'target': target
                        })

                st.success(f"âœ… {language} æœ¯è¯­åŠ è½½æˆåŠŸ: {len(self.term_base_dict[language])} æ¡")
                if missing_count > 0:
                    st.info(f"   è·³è¿‡ {missing_count} æ¡ä¸å®Œæ•´çš„ {language} æœ¯è¯­")

            # æ˜¾ç¤ºæœ¯è¯­åº“ç»Ÿè®¡
            total_terms = sum(len(terms) for terms in self.term_base_dict.values())
            st.success(f"ğŸ“Š æ€»è®¡åŠ è½½æœ¯è¯­: {total_terms} æ¡ï¼Œè¦†ç›– {len(self.term_base_dict)} ç§è¯­è¨€")
            
            # æ˜¾ç¤ºæœ¯è¯­ç¤ºä¾‹
            with st.expander("ğŸ“‹ æŸ¥çœ‹æœ¯è¯­åº“ç¤ºä¾‹"):
                for language, terms in self.term_base_dict.items():
                    if terms:
                        st.write(f"**{language}æœ¯è¯­ç¤ºä¾‹ï¼š**")
                        for i, term in enumerate(terms[:5]):
                            st.write(f"  {i+1}. {term['source']} â†’ {term['target']}")
                        if len(terms) > 5:
                            st.write(f"  ... è¿˜æœ‰ {len(terms)-5} æ¡")

            return True
            
        except Exception as e:
            st.error(f"âŒ åŠ è½½å¤šè¯­è¨€æœ¯è¯­åº“å¤±è´¥: {e}")
            import traceback
            st.error(traceback.format_exc())
            return False

    def load_role_personality(self, df, role_col, personality_col):
        try:
            self.role_personality_dict = {}
            missing_count = 0

            for _, row in df.iterrows():
                role = row[role_col]
                personality = row[personality_col]

                if pd.isna(role) or pd.isna(personality):
                    missing_count += 1
                    continue

                role = str(role).strip()
                personality = str(personality).strip()

                if role and personality:
                    self.role_personality_dict[role] = personality

            st.success(f"âœ… æˆåŠŸåŠ è½½è§’è‰²æ€§æ ¼: {len(self.role_personality_dict)} æ¡")
            if missing_count > 0:
                st.warning(f"âš ï¸ è·³è¿‡ {missing_count} æ¡ä¸å®Œæ•´çš„è®°å½•")

            self.analyze_role_personality()

            return True
        except Exception as e:
            st.error(f"âŒ åŠ è½½è§’è‰²æ€§æ ¼åº“å¤±è´¥: {e}")
            return False

    def analyze_role_personality(self):
        if not self.role_personality_dict:
            return

        st.write(f"ğŸ“Š è§’è‰²æ€§æ ¼åº“ç»Ÿè®¡: {len(self.role_personality_dict)} ä¸ªè§’è‰²")

        st.write("ğŸ“‹ éƒ¨åˆ†è§’è‰²æ€§æ ¼é¢„è§ˆ:")
        count = 0
        for role, personality in list(self.role_personality_dict.items())[:5]:
            st.write(f"  - {role}: {personality[:50]}..." if len(personality) > 50 else f"  - {role}: {personality}")
            count += 1
            if count >= 5:
                break


def get_api_providers():
    providers = {
        "DeepSeek": {
            "url": "https://api.deepseek.com/v1/chat/completions",
            "models": ["deepseek-chat", "deepseek-coder"]
        },
        "OpenAI": {
            "url": "https://api.openai.com/v1/chat/completions",
            "models": ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
        },
        "è‡ªå®šä¹‰API": {
            "url": "https://tb.api.mkeai.com/v1/chat/completions",
            "models": ["custom-model"]
        }
    }
    return providers


def render_role_matching_interface(translator, df, role_col):
    """æ¸²æŸ“è§’è‰²åŒ¹é…ç¡®è®¤ç•Œé¢"""
    st.header("ğŸ­ è§’è‰²æ¨¡ç³ŠåŒ¹é…ç¡®è®¤")

    fuzzy_matches = translator.analyze_role_matches(df, role_col)

    if not fuzzy_matches:
        st.success("âœ… æ‰€æœ‰è§’è‰²éƒ½å·²ç²¾ç¡®åŒ¹é…ï¼Œæ— éœ€ç¡®è®¤")
        return True

    st.warning(f"âš ï¸ å‘ç° {len(fuzzy_matches)} ä¸ªéœ€è¦ç¡®è®¤çš„è§’è‰²åŒ¹é…")
    st.info("ğŸ’¡ æç¤ºï¼šç³»ç»Ÿä¼šè‡ªåŠ¨ä¸ºç›¸åŒçš„è§’è‰²åæ‰¹é‡åº”ç”¨æ‚¨çš„é€‰æ‹©")

    if 'role_confirmations' not in st.session_state:
        st.session_state.role_confirmations = {}

    with st.form("role_matching_form"):
        for idx, (original_role, candidates) in enumerate(fuzzy_matches.items()):
            st.markdown(f"---")
            st.markdown(f"### è§’è‰² {idx + 1}: `{original_role}`")

            cleaned = translator.clean_role_name(original_role)
            st.caption(f"æ¸…ç†å: `{cleaned}`")

            role_count = len(df[df[role_col] == original_role])
            st.caption(f"ğŸ“Š åœ¨æ–‡æ¡£ä¸­å‡ºç° **{role_count}** æ¬¡")

            options = ["âŒ ä¸åŒ¹é…ä»»ä½•è§’è‰²"] + [
                f"âœ… {candidate} (ç›¸ä¼¼åº¦: {score:.2%})"
                for candidate, score in candidates
            ]

            default_idx = 1 if candidates else 0

            selected = st.radio(
                f"è¯·é€‰æ‹©åŒ¹é…çš„å®˜æ–¹è§’è‰²:",
                options=options,
                index=default_idx,
                key=f"role_match_{idx}"
            )

            if selected.startswith("âœ…"):
                matched_role = selected.split("(")[0].replace("âœ…", "").strip()
                st.session_state.role_confirmations[original_role] = matched_role
            else:
                st.session_state.role_confirmations[original_role] = None

            if st.session_state.role_confirmations.get(original_role):
                matched = st.session_state.role_confirmations[original_role]
                personality = translator.role_personality_dict.get(matched)
                if personality:
                    with st.expander("ğŸ‘¤ æŸ¥çœ‹è§’è‰²æ€§æ ¼æè¿°"):
                        st.write(personality)

        submitted = st.form_submit_button("âœ… ç¡®è®¤æ‰€æœ‰åŒ¹é…", use_container_width=True)

        if submitted:
            for original_role, matched_role in st.session_state.role_confirmations.items():
                if matched_role:
                    translator.role_mapping[original_role] = matched_role

            st.success(f"âœ… å·²ç¡®è®¤ {len([v for v in st.session_state.role_confirmations.values() if v])} ä¸ªè§’è‰²æ˜ å°„")

            with st.expander("ğŸ“‹ æŸ¥çœ‹æ˜ å°„æ‘˜è¦"):
                for orig, matched in st.session_state.role_confirmations.items():
                    if matched:
                        st.write(f"â€¢ `{orig}` â†’ `{matched}`")
                    else:
                        st.write(f"â€¢ `{orig}` â†’ âŒ æœªåŒ¹é…")

            return True

    return False


def batch_translation_page():
    st.title("æ‰¹é‡ç¿»è¯‘å·¥å…· - å¤šè¯­è¨€ä¼˜åŒ–ç‰ˆ")
    text_col = None
    role_col = None
    source_col = None
    role_name_col = None
    personality_col = None
    
    st.markdown("### æ”¯æŒåŒæ—¶ç¿»è¯‘å¤šç§è¯­è¨€ï¼Œç‹¬ç«‹çš„æœ¯è¯­åº“å’Œä¸Šä¸‹æ–‡ç®¡ç†")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'translator' not in st.session_state:
        st.session_state.translator = None
    if 'current_file' not in st.session_state:
        st.session_state.current_file = None
    if 'term_base_df' not in st.session_state:
        st.session_state.term_base_df = None
    if 'role_personality_df' not in st.session_state:
        st.session_state.role_personality_df = None
    if 'role_matching_confirmed' not in st.session_state:
        st.session_state.role_matching_confirmed = False
    if 'translation_progress' not in st.session_state:
        st.session_state.translation_progress = None
    if 'language_configs' not in st.session_state:
        st.session_state.language_configs = {}
    if 'term_language_mapping' not in st.session_state:
        st.session_state.term_language_mapping = {}

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ APIé…ç½®")

        api_providers = get_api_providers()
        api_provider = st.selectbox(
            "ğŸŒ APIæä¾›å•†",
            options=list(api_providers.keys()),
            index=0,
            key="batch_api_provider"
        )

        api_key = st.text_input(
            "ğŸ”‘ APIå¯†é’¥",
            type="password",
            key="batch_api_key"
        )

        if api_provider == "è‡ªå®šä¹‰API":
            api_url = st.text_input(
                "ğŸ”— API URL",
                value="https://tb.api.mkeai.com/v1/chat/completions",
                key="batch_api_url"
            )
        else:
            api_url = api_providers[api_provider]["url"]

        model = st.text_input(
            "ğŸ¤– æ¨¡å‹åç§°",
            value="deepseek-chat",
            key="batch_model"
        )

        st.markdown("---")
        st.header("ğŸŒ å¤šè¯­è¨€é…ç½®")
        
        # å¯é€‰è¯­è¨€åˆ—è¡¨
        available_languages = ["è‹±æ–‡", "æ—¥æ–‡", "éŸ©æ–‡", "æ³•æ–‡", "å¾·æ–‡", "è¥¿ç­ç‰™æ–‡", "ä¿„æ–‡", "é˜¿æ‹‰ä¼¯æ–‡", "è‘¡è„ç‰™æ–‡", "æ„å¤§åˆ©æ–‡"]
        
        # é€‰æ‹©è¦ç¿»è¯‘çš„è¯­è¨€
        selected_languages = st.multiselect(
            "ğŸ¯ é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼ˆå¯å¤šé€‰ï¼‰",
            options=available_languages,
            default=["è‹±æ–‡"],
            help="å¯ä»¥åŒæ—¶é€‰æ‹©å¤šç§è¯­è¨€è¿›è¡Œç¿»è¯‘",
            key="selected_languages"
        )
        
        if not selected_languages:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ç§ç›®æ ‡è¯­è¨€")
        
        # ä¸ºæ¯ç§è¯­è¨€é…ç½®åˆ—å
        st.subheader("ğŸ“ è‡ªå®šä¹‰ç»“æœåˆ—å")
        language_column_names = {}
        
        for lang in selected_languages:
            default_name = f"{lang}ç¿»è¯‘ç»“æœ"
            col_name = st.text_input(
                f"{lang} ç»“æœåˆ—å",
                value=default_name,
                key=f"col_name_{lang}",
                help=f"è®¾ç½®{lang}ç¿»è¯‘ç»“æœåœ¨Excelä¸­çš„åˆ—å"
            )
            language_column_names[lang] = col_name
        
        st.session_state.language_configs = {
            'languages': selected_languages,
            'column_names': language_column_names
        }

        st.markdown("---")
        st.header("ğŸ’¾ è‡ªåŠ¨ä¿å­˜è®¾ç½®")
        
        auto_save_interval = st.number_input(
            "è‡ªåŠ¨ä¿å­˜é—´éš”ï¼ˆæ¯Nè¡Œï¼‰",
            min_value=10,
            max_value=500,
            value=50,
            step=10,
            help="æ¯ç¿»è¯‘Nè¡Œåè‡ªåŠ¨ä¿å­˜ä¸€æ¬¡"
        )
        
        save_directory = st.text_input(
            "ä¿å­˜ç›®å½•",
            value="./translation_saves",
            help="è‡ªåŠ¨ä¿å­˜æ–‡ä»¶çš„ç›®å½•è·¯å¾„"
        )

        st.markdown("---")
        st.header("ğŸ­ è§’è‰²åŒ¹é…è®¾ç½®")

        enable_fuzzy = st.checkbox(
            "å¯ç”¨æ¨¡ç³Šè§’è‰²åŒ¹é…",
            value=True,
            help="è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ä¸­çš„è§’è‰²åå˜ä½“ï¼ˆå¦‚ç©ºæ ¼ã€é”™åˆ«å­—ç­‰ï¼‰",
            key="enable_fuzzy_match"
        )

        if enable_fuzzy:
            fuzzy_threshold = st.slider(
                "åŒ¹é…ç›¸ä¼¼åº¦é˜ˆå€¼",
                min_value=0.5,
                max_value=1.0,
                value=0.6,
                step=0.05,
                help="ç›¸ä¼¼åº¦è¶Šé«˜è¶Šä¸¥æ ¼ï¼Œ0.6ä¸ºæ¨èå€¼",
                key="fuzzy_threshold"
            )
        else:
            fuzzy_threshold = 1.0

        st.markdown("---")
        context_size = st.slider(
            "ğŸ“š ä¸Šä¸‹æ–‡è®°å½•æ•°é‡",
            min_value=1,
            max_value=20,
            value=10,
            help="æ¯ç§è¯­è¨€ç‹¬ç«‹ç»´æŠ¤çš„ä¸Šä¸‹æ–‡æ•°é‡",
            key="batch_context_size"
        )

        max_retries = st.number_input(
            "ğŸ”„ æœ€å¤§é‡è¯•æ¬¡æ•°",
            min_value=1,
            max_value=10000,
            value=10,
            key="batch_max_retries"
        )

    # ä¸»ç•Œé¢
    col1, col2 = st.columns([1, 1])

    with col1:
        st.header("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„è¿›åº¦æ–‡ä»¶
        saved_files = []
        if os.path.exists(save_directory):
            saved_files = [f for f in os.listdir(save_directory) if f.endswith('_progress.xlsx')]
        
        resume_mode = st.checkbox(
            "ğŸ”„ ä»ä¸Šæ¬¡è¿›åº¦ç»§ç»­ç¿»è¯‘",
            value=False,
            help="ä»ä¹‹å‰ä¿å­˜çš„è¿›åº¦æ–‡ä»¶ç»§ç»­ç¿»è¯‘"
        )
        
        if resume_mode and saved_files:
            st.info("ğŸ“‹ æ‰¾åˆ°ä»¥ä¸‹è¿›åº¦æ–‡ä»¶ï¼š")
            selected_progress_file = st.selectbox(
                "é€‰æ‹©è¦ç»§ç»­çš„è¿›åº¦æ–‡ä»¶",
                options=saved_files,
                format_func=lambda x: f"{x} ({datetime.fromtimestamp(os.path.getmtime(os.path.join(save_directory, x))).strftime('%Y-%m-%d %H:%M:%S')})"
            )
            
            if st.button("ğŸ“‚ åŠ è½½è¿›åº¦æ–‡ä»¶"):
                try:
                    progress_path = os.path.join(save_directory, selected_progress_file)
                    df = pd.read_excel(progress_path)
                    df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                    st.session_state.current_file = df
                    
                    # ç»Ÿè®¡æ¯ç§è¯­è¨€çš„ç¿»è¯‘è¿›åº¦
                    progress_info = []
                    for lang, col_name in st.session_state.language_configs['column_names'].items():
                        if col_name in df.columns:
                            translated_count = df[col_name].notna().sum()
                            total_count = len(df)
                            progress_info.append(f"{lang}: {translated_count}/{total_count}")
                    
                    st.success(f"âœ… æˆåŠŸåŠ è½½è¿›åº¦æ–‡ä»¶ï¼")
                    if progress_info:
                        st.info(f"ğŸ“Š ç¿»è¯‘è¿›åº¦: {', '.join(progress_info)}")
                    
                    with st.expander("ğŸ“Š æ–‡ä»¶é¢„è§ˆ"):
                        st.dataframe(df.head(10))
                except Exception as e:
                    st.error(f"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
        else:
            uploaded_file = st.file_uploader(
                "ğŸ“„ ä¸Šä¼ ç¿»è¯‘æ–‡ä»¶ (Excel)",
                type=['xlsx', 'xls', 'csv'],
                key="batch_file_uploader"
            )

            if uploaded_file is not None:
                try:
                    if uploaded_file.name.endswith('.csv'):
                        df = pd.read_csv(uploaded_file)
                    else:
                        df = pd.read_excel(uploaded_file)
                    df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                    st.session_state.current_file = df
                    st.success(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®")

                    with st.expander("ğŸ“Š æ–‡ä»¶é¢„è§ˆ"):
                        st.dataframe(df.head(10))

                except Exception as e:
                    st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        
        if st.session_state.current_file is not None:
            df = st.session_state.current_file
            cols = df.columns.tolist()
            text_col = st.selectbox(
                "ğŸ“ é€‰æ‹©æ–‡æœ¬åˆ—",
                options=cols,
                index=0,
                key="batch_text_col"
            )

            role_col = st.selectbox(
                "ğŸ‘¥ é€‰æ‹©è§’è‰²åˆ— (å¯é€‰)",
                options=["æ— "] + cols,
                index=0,
                key="batch_role_col"
            )
            if role_col == "æ— ":
                role_col = None

    with col2:
        st.header("ğŸ“š æœ¯è¯­åº“å’Œæ€§æ ¼åº“")

        uploaded_term_base = st.file_uploader(
            "ğŸ“š ä¸Šä¼ å¤šè¯­è¨€æœ¯è¯­åº“ (Excel)",
            type=['xlsx', 'xls'],
            key="batch_term_base_uploader",
            help="æœ¯è¯­åº“åº”åŒ…å«åŸæ–‡åˆ—å’Œå¤šä¸ªç›®æ ‡è¯­è¨€åˆ—"
        )

        if uploaded_term_base is not None:
            try:
                term_df = pd.read_excel(uploaded_term_base)
                term_df.columns = term_df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.term_base_df = term_df
                st.success(f"âœ… æˆåŠŸè¯»å–æœ¯è¯­åº“ï¼Œå…± {len(term_df)} æ¡æœ¯è¯­")

                with st.expander("ğŸ“‹ é…ç½®æœ¯è¯­åº“åˆ—æ˜ å°„", expanded=True):
                    term_cols = term_df.columns.tolist()
                    
                    # é€‰æ‹©åŸæ–‡åˆ—
                    source_col = st.selectbox(
                        "ğŸ“¤ é€‰æ‹©åŸæ–‡åˆ—ï¼ˆä¸­æ–‡ï¼‰",
                        options=term_cols,
                        index=0,
                        key="batch_source_col"
                    )
                    
                    st.markdown("---")
                    st.subheader("ğŸŒ ä¸ºæ¯ç§è¯­è¨€é€‰æ‹©å¯¹åº”çš„æœ¯è¯­åˆ—")
                    st.info("ğŸ’¡ æç¤ºï¼šä¸ºæ¯ç§ç›®æ ‡è¯­è¨€é€‰æ‹©æœ¯è¯­åº“ä¸­å¯¹åº”çš„ç¿»è¯‘åˆ—")
                    
                    # ä¸ºæ¯ç§é€‰å®šçš„è¯­è¨€é…ç½®æœ¯è¯­åˆ—
                    term_language_mapping = {}
                    for lang in selected_languages:
                        st.markdown(f"**{lang} æœ¯è¯­åˆ—ï¼š**")
                        target_col = st.selectbox(
                            f"é€‰æ‹© {lang} å¯¹åº”çš„æœ¯è¯­åˆ—",
                            options=["ä¸ä½¿ç”¨æœ¯è¯­åº“"] + term_cols,
                            index=0,
                            key=f"term_col_{lang}",
                            help=f"é€‰æ‹©æœ¯è¯­åº“ä¸­ {lang} ç¿»è¯‘å¯¹åº”çš„åˆ—"
                        )
                        
                        if target_col != "ä¸ä½¿ç”¨æœ¯è¯­åº“":
                            term_language_mapping[lang] = target_col
                            
                            # æ˜¾ç¤ºè¯¥è¯­è¨€çš„æœ¯è¯­ç¤ºä¾‹
                            sample_terms = term_df[[source_col, target_col]].head(3).dropna()
                            if not sample_terms.empty:
                                st.caption(f"ç¤ºä¾‹ï¼š")
                                for _, row in sample_terms.iterrows():
                                    st.caption(f"  â€¢ {row[source_col]} â†’ {row[target_col]}")
                    
                    st.session_state.term_language_mapping = term_language_mapping
                    
                    if term_language_mapping:
                        st.success(f"âœ… å·²é…ç½® {len(term_language_mapping)} ç§è¯­è¨€çš„æœ¯è¯­æ˜ å°„")
                    else:
                        st.warning("âš ï¸ æœªé…ç½®ä»»ä½•è¯­è¨€çš„æœ¯è¯­æ˜ å°„")

            except Exception as e:
                st.error(f"âŒ æœ¯è¯­åº“è¯»å–å¤±è´¥: {e}")

        uploaded_role_personality = st.file_uploader(
            "ğŸ“‹ ä¸Šä¼ è§’è‰²æ€§æ ¼åº“æ–‡ä»¶ (Excel)",
            type=['xlsx', 'xls'],
            key="batch_role_personality_uploader"
        )

        if uploaded_role_personality is not None:
            try:
                role_personality_df = pd.read_excel(uploaded_role_personality)
                role_personality_df.columns = role_personality_df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.role_personality_df = role_personality_df
                st.success(f"âœ… æˆåŠŸè¯»å–è§’è‰²æ€§æ ¼åº“ï¼Œå…± {len(role_personality_df)} æ¡è®°å½•")

                role_personality_cols = role_personality_df.columns.tolist()
                role_name_col = st.selectbox(
                    "ğŸ‘¥ é€‰æ‹©è§’è‰²åç§°åˆ—",
                    options=role_personality_cols,
                    index=0,
                    key="batch_role_name_col"
                )

                personality_col = st.selectbox(
                    "ğŸ’¬ é€‰æ‹©æ€§æ ¼æè¿°åˆ—",
                    options=role_personality_cols,
                    index=min(1, len(role_personality_cols) - 1) if len(role_personality_cols) > 1 else 0,
                    key="batch_personality_col"
                )

            except Exception as e:
                st.error(f"âŒ è§’è‰²æ€§æ ¼åº“è¯»å–å¤±è´¥: {e}")

    # ç¿»è¯‘è¦æ±‚è®¾ç½®
    st.header("ğŸ¯ ç¿»è¯‘è¦æ±‚è®¾ç½®")

    custom_requirements = st.text_area(
        "ğŸ’¬ è‡ªå®šä¹‰ç¿»è¯‘è¦æ±‚ï¼ˆé€‚ç”¨äºæ‰€æœ‰è¯­è¨€ï¼‰",
        value="è§’è‰²å¯¹è¯è‡ªç„¶æµç•…ï¼›ä¸“ä¸šæœ¯è¯­ç»Ÿä¸€ï¼›ä¿æŒåŸæ–‡é£æ ¼ï¼›æœ¬åœ°åŒ–é€‚é…ï¼›ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼›æ ¹æ®è§’è‰²è°ƒæ•´è¯­æ°”;è¯·æ³¨æ„ä½¿ç”¨è¯­ä½“ï¼Œä¸”æ‰€æœ‰è§’è‰²é™¤äº†å¾®å‹æœºå’Œç­é•¿ï¼Œå…¶ä»–éƒ½ä¸ºå¥³ç”Ÿç”¨è¯­ï¼Œä¸è¦ç”¨ç”·æ€§ç”¨è¯­ï¼Œç°åœ¨è§’è‰²ä»¬éƒ½ååˆ†ç†Ÿæ‚‰å½¼æ­¤äº†ï¼Œä¸éœ€è¦ä½¿ç”¨å¤ªæ­£å¼å°Šé‡çš„è¯­ä½“äº†ä¾‹å¦‚æ—¥è¯­çš„è¯ä¸éœ€è¦ã§ã™ã¾ã™å‹äº†ã€‚",
        height=100,
        key="batch_custom_requirements"
    )

    # åˆå§‹åŒ–ç¿»è¯‘å™¨å’ŒåŠ è½½èµ„æº
    if st.button("ğŸ”§ åˆå§‹åŒ–ç¿»è¯‘å™¨", type="secondary", use_container_width=True):
        if not api_key:
            st.error("âŒ è¯·å…ˆè¾“å…¥APIå¯†é’¥")
        elif not selected_languages:
            st.error("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ç§ç›®æ ‡è¯­è¨€")
        else:
            try:
                translator = MultiAPIExcelTranslator(
                    api_key, api_provider, api_url, model,
                    context_size, max_retries
                )
                translator.enable_fuzzy_match = enable_fuzzy
                translator.fuzzy_threshold = fuzzy_threshold
                translator.set_target_languages(selected_languages, language_column_names)

                st.session_state.translator = translator
                st.session_state.role_matching_confirmed = False

                st.info(f"ğŸŒ ä½¿ç”¨ {api_provider} API")
                st.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
                st.info(f"ğŸ¯ ç›®æ ‡è¯­è¨€: {', '.join(selected_languages)}")
                
                # æ˜¾ç¤ºåˆ—åé…ç½®
                with st.expander("ğŸ“‹ æŸ¥çœ‹åˆ—åé…ç½®"):
                    for lang, col_name in language_column_names.items():
                        st.write(f"â€¢ {lang} â†’ `{col_name}`")

                # åŠ è½½å¤šè¯­è¨€æœ¯è¯­åº“
                if st.session_state.term_base_df is not None and st.session_state.term_language_mapping:
                    if translator.load_term_base_multilang(
                        st.session_state.term_base_df, 
                        source_col, 
                        st.session_state.term_language_mapping
                    ):
                        st.success("âœ… å¤šè¯­è¨€æœ¯è¯­åº“åŠ è½½æˆåŠŸ")
                elif st.session_state.term_base_df is not None:
                    st.warning("âš ï¸ æœ¯è¯­åº“å·²ä¸Šä¼ ä½†æœªé…ç½®è¯­è¨€æ˜ å°„")

                # åŠ è½½è§’è‰²æ€§æ ¼åº“
                if st.session_state.role_personality_df is not None:
                    if translator.load_role_personality(
                            st.session_state.role_personality_df,
                            role_name_col,
                            personality_col
                    ):
                        st.success("âœ… è§’è‰²æ€§æ ¼åº“åŠ è½½æˆåŠŸ")

                st.success("âœ… ç¿»è¯‘å™¨åˆå§‹åŒ–å®Œæˆï¼")

            except Exception as e:
                st.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                st.error(traceback.format_exc())

    # è§’è‰²åŒ¹é…ç¡®è®¤ç•Œé¢
    if (st.session_state.translator is not None and
            st.session_state.current_file is not None and
            role_col is not None and
            enable_fuzzy and
            not st.session_state.role_matching_confirmed):

        st.markdown("---")
        confirmed = render_role_matching_interface(
            st.session_state.translator,
            st.session_state.current_file,
            role_col
        )
        if confirmed:
            st.session_state.role_matching_confirmed = True
            st.rerun()

    # å¼€å§‹ç¿»è¯‘æŒ‰é’®
    st.markdown("---")
    translation_ready = (
            st.session_state.translator is not None and
            st.session_state.current_file is not None and
            selected_languages and
            (not enable_fuzzy or not role_col or st.session_state.role_matching_confirmed)
    )

    if not translation_ready and enable_fuzzy and role_col:
        st.info("ğŸ’¡ è¯·å…ˆå®Œæˆè§’è‰²åŒ¹é…ç¡®è®¤åå†å¼€å§‹ç¿»è¯‘")

    if st.button(
            "ğŸ¯ å¼€å§‹å¤šè¯­è¨€ç¿»è¯‘",
            type="primary",
            use_container_width=True,
            disabled=not translation_ready,
            key="batch_start_translation"
    ):
        try:
            translator = st.session_state.translator
            df = st.session_state.current_file.copy()
            languages = st.session_state.language_configs['languages']
            column_names = st.session_state.language_configs['column_names']

            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            os.makedirs(save_directory, exist_ok=True)

            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            with st.expander("ğŸ“‹ æŸ¥çœ‹ç¿»è¯‘é…ç½®", expanded=True):
                st.write("**è¯­è¨€é…ç½®ï¼š**")
                for lang, col_name in column_names.items():
                    term_status = "âœ… å·²é…ç½®æœ¯è¯­åº“" if lang in translator.term_base_dict and translator.term_base_dict[lang] else "âš ï¸ æœªé…ç½®æœ¯è¯­åº“"
                    st.write(f"â€¢ {lang} â†’ `{col_name}` ({term_status})")
                
                if translator.role_mapping:
                    st.write("**è§’è‰²æ˜ å°„ï¼š**")
                    for orig, mapped in translator.role_mapping.items():
                        st.write(f"â€¢ `{orig}` â†’ `{mapped}`")

            # ä¸ºæ¯ç§è¯­è¨€æ·»åŠ ç»“æœåˆ—
            for lang in languages:
                col_name = column_names[lang]
                if col_name not in df.columns:
                    df[col_name] = ''
            
            # è®¡ç®—èµ·å§‹ä½ç½®
            start_index = 0
            if resume_mode:
                min_translated_index = len(df)
                for lang in languages:
                    col_name = column_names[lang]
                    if col_name in df.columns:
                        last_index = -1
                        for idx in range(len(df)):
                            if not pd.isna(df.at[idx, col_name]) and str(df.at[idx, col_name]).strip() != '':
                                if not str(df.at[idx, col_name]).startswith('[ç¿»è¯‘å¤±è´¥'):
                                    last_index = idx
                        min_translated_index = min(min_translated_index, last_index + 1)
                
                start_index = min_translated_index
                if start_index > 0:
                    st.info(f"ğŸ”„ ç»§ç»­ç¿»è¯‘ï¼šè·³è¿‡å‰ {start_index} è¡Œï¼ˆå·²ç¿»è¯‘ï¼‰")

            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ä¸ºæ¯ç§è¯­è¨€åˆ›å»ºç»Ÿè®¡
            stats = {lang: {'success': 0, 'error': 0} for lang in languages}
            
            total_rows = len(df)
            
            # ç”Ÿæˆä¿å­˜æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            progress_filename = f"translation_progress_multilang_{timestamp}.xlsx"
            progress_path = os.path.join(save_directory, progress_filename)

            try:
                for index in range(start_index, total_rows):
                    row = df.iloc[index]
                    progress = (index + 1) / total_rows
                    progress_bar.progress(progress)
                    
                    # æ„å»ºçŠ¶æ€ä¿¡æ¯
                    stats_str = " | ".join([f"{lang}: âœ“{stats[lang]['success']} âœ—{stats[lang]['error']}" for lang in languages])
                    status_text.text(f"ğŸ“ æ­£åœ¨ç¿»è¯‘ç¬¬ {index + 1}/{total_rows} è¡Œ... | {stats_str}")

                    text = str(row[text_col])
                    role = row[role_col] if role_col and role_col in row else None

                    # è·³è¿‡åŸæ–‡ä¸ºç©ºçš„è¡Œ
                    if pd.isna(text) == "" or str(text).strip() == "" or text == "nan":
                        print("ä¸ºç©º")
                        continue
                    
                    # å¯¹æ¯ç§è¯­è¨€è¿›è¡Œç¿»è¯‘ï¼ˆç‹¬ç«‹ç¿»è¯‘ï¼Œäº’ä¸å½±å“ï¼‰
                    for lang in languages:
                        col_name = column_names[lang]
                        
                        # å¦‚æœè¯¥è¡Œè¯¥è¯­è¨€å·²ç»ç¿»è¯‘è¿‡ï¼Œè·³è¿‡
                        existing_translation = df.at[index, col_name]
                        if not pd.isna(existing_translation) and str(existing_translation).strip() != '' and not str(existing_translation).startswith('[ç¿»è¯‘å¤±è´¥'):
                            stats[lang]['success'] += 1
                            continue

                        try:
                            # æ¯ç§è¯­è¨€ç‹¬ç«‹ç¿»è¯‘ï¼Œä½¿ç”¨å„è‡ªçš„ä¸Šä¸‹æ–‡å’Œæœ¯è¯­åº“
                            translated_text = translator.translate_text(
                                text, lang, custom_requirements, role
                            )
                            
                            df.at[index, col_name] = translated_text
                            stats[lang]['success'] += 1
                            
                        except Exception as e:
                            error_msg = str(e)
                            st.warning(f"âš ï¸ [{lang}] ç¬¬ {index + 1} è¡Œç¿»è¯‘å¤±è´¥: {error_msg}")
                            df.at[index, col_name] = f"[ç¿»è¯‘å¤±è´¥: {error_msg}]"
                            stats[lang]['error'] += 1
                        
                        # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIé€Ÿç‡é™åˆ¶
                        time.sleep(0.15)

                    # è‡ªåŠ¨ä¿å­˜
                    if (index + 1) % auto_save_interval == 0:
                        try:
                            with pd.ExcelWriter(progress_path, engine='openpyxl') as writer:
                                df.to_excel(writer, index=False, sheet_name='ç¿»è¯‘è¿›åº¦')
                            st.info(f"ğŸ’¾ å·²è‡ªåŠ¨ä¿å­˜è¿›åº¦: {index + 1}/{total_rows} è¡Œ")
                        except Exception as save_error:
                            st.warning(f"âš ï¸ è‡ªåŠ¨ä¿å­˜å¤±è´¥: {save_error}")

                # æœ€ç»ˆä¿å­˜
                final_filename = f"translation_final_multilang_{timestamp}.xlsx"
                final_path = os.path.join(save_directory, final_filename)
                
                with pd.ExcelWriter(final_path, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='ç¿»è¯‘ç»“æœ')
                
                progress_bar.progress(1.0)
                
                # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
                st.success("âœ… å¤šè¯­è¨€ç¿»è¯‘å®Œæˆï¼")
                
                stats_cols = st.columns(len(languages))
                for idx, lang in enumerate(languages):
                    with stats_cols[idx]:
                        st.metric(
                            f"{lang}",
                            f"âœ“ {stats[lang]['success']}",
                            f"âœ— {stats[lang]['error']}" if stats[lang]['error'] > 0 else None,
                            delta_color="inverse"
                        )

                st.subheader("ğŸ“Š ç¿»è¯‘ç»“æœé¢„è§ˆ")
                
                # æ„å»ºæ˜¾ç¤ºåˆ—é¡ºåºï¼šåŸæ–‡åˆ— + è§’è‰²åˆ— + æ‰€æœ‰ç¿»è¯‘ç»“æœåˆ—
                display_cols = [text_col]
                if role_col:
                    display_cols.append(role_col)
                display_cols.extend([column_names[lang] for lang in languages])
                
                st.dataframe(df[display_cols].head(20))

                # æä¾›ä¸‹è½½æŒ‰é’®
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='ç¿»è¯‘ç»“æœ')

                st.download_button(
                    label="ğŸ’¾ ä¸‹è½½å¤šè¯­è¨€ç¿»è¯‘ç»“æœ",
                    data=output.getvalue(),
                    file_name=f"translated_multilang_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
                
                st.success(f"ğŸ“ æ–‡ä»¶å·²è‡ªåŠ¨ä¿å­˜è‡³: {final_path}")

            except KeyboardInterrupt:
                st.warning("âš ï¸ ç¿»è¯‘è¢«ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
                try:
                    interrupt_filename = f"translation_interrupted_{timestamp}.xlsx"
                    interrupt_path = os.path.join(save_directory, interrupt_filename)
                    with pd.ExcelWriter(interrupt_path, engine='openpyxl') as writer:
                        df.to_excel(writer, index=False, sheet_name='ç¿»è¯‘è¿›åº¦')
                    st.info(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜è‡³: {interrupt_path}")
                except Exception as save_error:
                    st.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {save_error}")

        except Exception as e:
            st.error(f"âŒ ç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            st.error(traceback.format_exc())
            
            # å°è¯•ä¿å­˜å½“å‰è¿›åº¦
            try:
                error_filename = f"translation_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                error_path = os.path.join(save_directory, error_filename)
                with pd.ExcelWriter(error_path, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='ç¿»è¯‘è¿›åº¦')
                st.info(f"ğŸ’¾ é”™è¯¯å‰çš„è¿›åº¦å·²ä¿å­˜è‡³: {error_path}")
            except Exception as save_error:
                st.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {save_error}")
def get_api_providers():
    providers = {
        "DeepSeek": {
            "url": "https://api.deepseek.com/v1/chat/completions",
            "models": ["deepseek-chat", "deepseek-coder"]
        },
        "OpenAI": {
            "url": "https://api.openai.com/v1/chat/completions",
            "models": ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
        },
        "è‡ªå®šä¹‰API": {
            "url": "https://tb.api.mkeai.com/v1/chat/completions",
            "models": ["custom-model"]
        }
    }
    return providers

def get_preset_options():
    presets = {
        "æ¸¸æˆUIç®€çº¦é£æ ¼": "æ¸¸æˆUIç®€çº¦é£æ ¼",
        "è§’è‰²å¯¹è¯è‡ªç„¶æµç•…": "è§’è‰²å¯¹è¯è‡ªç„¶æµç•…", 
        "ä¸“ä¸šæœ¯è¯­ç»Ÿä¸€": "ä¸“ä¸šæœ¯è¯­ç»Ÿä¸€",
        "ä¿æŒåŸæ–‡é£æ ¼": "ä¿æŒåŸæ–‡é£æ ¼",
        "æœ¬åœ°åŒ–é€‚é…": "æœ¬åœ°åŒ–é€‚é…",
        "ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§": "ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§",
        "æ ¹æ®è§’è‰²è°ƒæ•´è¯­æ°”": "æ ¹æ®è§’è‰²è°ƒæ•´è¯­æ°”"
    }
    return presets

def get_preset_languages():
    return ["è‹±æ–‡", "æ—¥æ–‡", "éŸ©æ–‡", "æ³•æ–‡", "å¾·æ–‡", "è¥¿ç­ç‰™æ–‡", "è‡ªå®šä¹‰"]

def get_default_custom_requirements():
    return "è§’è‰²å¯¹è¯è‡ªç„¶æµç•…ï¼›ä¸“ä¸šæœ¯è¯­ç»Ÿä¸€ï¼›ä¿æŒåŸæ–‡é£æ ¼ï¼›æœ¬åœ°åŒ–é€‚é…ï¼›ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼›æ ¹æ®è§’è‰²è°ƒæ•´è¯­æ°”ï¼›è¯·æ³¨æ„ä½¿ç”¨è¯­ä½“ï¼Œä¸”æ‰€æœ‰è§’è‰²é™¤äº†å¾®å‹æœºå’Œç‚½é•¿ï¼Œå…¶ä»–éƒ½ä¸ºå¥³ç”Ÿç”¨è¯­ï¼Œä¸è¦ç”¨ç”·æ€§ç”¨è¯­ï¼Œç°åœ¨è§’è‰²ä»¬éƒ½ååˆ†ç†Ÿæ‚‰å½¼æ­¤äº†ï¼Œä¸éœ€è¦ä½¿ç”¨å¤ªæ­£å¼å°Šé‡çš„è¯­ä½“äº†ä¾‹å¦‚æ—¥è¯­çš„è¯ä¸éœ€è¦ã§ã™ã¾ã™å‹äº†ã€‚"

def parse_ai_translation_result(text):
    try:
        text = text.strip()
        lines = text.split('\n')
        translations = {}
        
        # æŸ¥æ‰¾è¡¨æ ¼å¼€å§‹ä½ç½®
        table_start = -1
        header_found = False
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line or not '|' in line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨å¤´è¡Œ
            if ('åŸæ–‡' in line or 'ä¸­æ–‡' in line) and ('ç¿»è¯‘' in line or 'Translation' in line or 'è‹±æ–‡' in line or 'æ—¥æ–‡' in line):
                header_found = True
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯åˆ†éš”çº¿
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if '|' in next_line and ('---' in next_line or '===' in next_line or '--' in next_line):
                        table_start = i + 2  # æ•°æ®ä»åˆ†éš”çº¿åå¼€å§‹
                    else:
                        table_start = i + 1  # æ²¡æœ‰åˆ†éš”çº¿ï¼Œæ•°æ®ä»è¡¨å¤´åå¼€å§‹
                else:
                    table_start = i + 1
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ ‡å‡†è¡¨å¤´ï¼Œå°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å« | çš„è¡Œä½œä¸ºæ•°æ®èµ·ç‚¹
        if not header_found:
            for i, line in enumerate(lines):
                if '|' in line and not ('---' in line or '===' in line):
                    # å°è¯•è§£æè¿™ä¸€è¡Œï¼Œçœ‹æ˜¯å¦æœ‰ä¸¤åˆ—æ•°æ®
                    parts = [part.strip() for part in line.split('|') if part.strip()]
                    if len(parts) >= 2:
                        table_start = i
                        break
        
        if table_start == -1:
            st.warning("æœªæ‰¾åˆ°è¡¨æ ¼ç»“æ„ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•...")
            return parse_fallback_format(text)
        
        # è§£æè¡¨æ ¼æ•°æ®
        success_count = 0
        for i in range(table_start, len(lines)):
            line = lines[i].strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œåˆ†éš”çº¿
            if not line or not '|' in line:
                continue
            if '---' in line or '===' in line:
                continue
                
            # åˆ†å‰²è¡Œï¼Œç§»é™¤ç©ºç™½éƒ¨åˆ†
            parts = [part.strip() for part in line.split('|')]
            # ç§»é™¤é¦–å°¾çš„ç©ºå­—ç¬¦ä¸²ï¼ˆæ¥è‡ªè¡Œé¦–å°¾çš„|ï¼‰
            parts = [p for p in parts if p]
            
            if len(parts) >= 2:
                original_text = parts[0]
                translation_text = parts[1]
                
                # æ¸…ç†Markdownæ ¼å¼ç¬¦å·
                original_text = re.sub(r'\*\*|\*|`|#', '', original_text).strip()
                translation_text = re.sub(r'\*\*|\*|`|#', '', translation_text).strip()
                
                # åªæ·»åŠ éç©ºçš„æœ‰æ•ˆç¿»è¯‘
                if original_text and translation_text:
                    if original_text not in translations:
                        translations[original_text] = translation_text
                        success_count += 1
        
        if success_count > 0:
            st.success(f"âœ… æˆåŠŸè§£æ {success_count} æ¡ç¿»è¯‘")
        else:
            st.warning("è¡¨æ ¼è§£ææˆåŠŸä½†æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
            return parse_fallback_format(text)
        
        return translations
        
    except Exception as e:
        st.error(f"è§£æAIç¿»è¯‘ç»“æœæ—¶å‡ºé”™: {e}")
        st.warning("å°è¯•ä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•...")
        return parse_fallback_format(text)

def parse_fallback_format(text):
    try:
        translations = {}
        lines = text.strip().split('\n')
        success_count = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # è·³è¿‡åˆ†éš”çº¿å’Œè¡¨å¤´
            if '---' in line or '===' in line:
                continue
            if 'åŸæ–‡' in line or 'Translation' in line:
                continue
                
            # å°è¯•å¤šç§åˆ†éš”ç¬¦
            if '|' in line:
                # å¤„ç†è¡¨æ ¼æ ¼å¼
                parts = [p.strip() for p in line.split('|')]
                # ç§»é™¤ç©ºå­—ç¬¦ä¸²
                parts = [p for p in parts if p]
                
                if len(parts) >= 2:
                    original = parts[0]
                    translation = parts[1]
                    
                    # æ¸…ç†æ–‡æœ¬
                    original = re.sub(r'\*\*|\*|`|#', '', original).strip()
                    translation = re.sub(r'\*\*|\*|`|#', '', translation).strip()
                    
                    if original and translation:
                        if original not in translations:
                            translations[original] = translation
                            success_count += 1
            elif '\t' in line:
                # å¤„ç†åˆ¶è¡¨ç¬¦åˆ†éš”
                parts = line.split('\t')
                if len(parts) >= 2:
                    original = parts[0].strip()
                    translation = parts[1].strip()
                    
                    original = re.sub(r'\*\*|\*|`|#', '', original).strip()
                    translation = re.sub(r'\*\*|\*|`|#', '', translation).strip()
                    
                    if original and translation:
                        if original not in translations:
                            translations[original] = translation
                            success_count += 1
        
        if success_count > 0:
            st.info(f"ğŸ“ å¤‡ç”¨æ–¹æ³•æˆåŠŸè§£æ {success_count} æ¡ç¿»è¯‘")
        else:
            st.error("âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿæœªèƒ½è§£æå‡ºæœ‰æ•ˆæ•°æ®")
        
        return translations
    except Exception as e:
        st.error(f"å¤‡ç”¨è§£ææ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
        return {}

def merge_translations_with_excel(original_df, text_col, translations, target_language):
    try:
        result_df = original_df.copy()
        result_df[f'{target_language}ç¿»è¯‘ç»“æœ'] = ''
        
        matched_count = 0
        unmatched_texts = []
        
        for index, row in result_df.iterrows():
            original_text = str(row[text_col]) if not pd.isna(row[text_col]) else ''
            if original_text and original_text in translations:
                result_df.at[index, f'{target_language}ç¿»è¯‘ç»“æœ'] = translations[original_text]
                matched_count += 1
            elif original_text:
                unmatched_texts.append(original_text)
        
        return result_df, matched_count, unmatched_texts
    except Exception as e:
        st.error(f"åˆå¹¶ç¿»è¯‘ç»“æœæ—¶å‡ºé”™: {e}")
        return original_df, 0, []



# é¡µé¢2: æç¤ºè¯ç”Ÿæˆå™¨ï¼ˆåŸç¬¬äºŒä¸ªç¨‹åºï¼‰
def prompt_generator_page():
    st.title("ğŸ“ å•è¯­ç§ç¿»è¯‘æç¤ºè¯ç”Ÿæˆå™¨")
    st.markdown("### æ ¹æ®å¾…ç¿»è¯‘æ–‡æœ¬ã€æœ¯è¯­åº“å’Œè§’è‰²æ€§æ ¼ä¿¡æ¯ï¼Œç”Ÿæˆé’ˆå¯¹å•ä¸€ç›®æ ‡è¯­è¨€çš„ç¿»è¯‘æç¤ºè¯ã€‚")
    st.markdown("**æ³¨æ„ï¼š** æœ¬é¡µé¢ä»…ç”¨äºç”Ÿæˆæç¤ºè¯æ–‡æœ¬ï¼Œä¸è¿›è¡Œå®é™…çš„APIç¿»è¯‘è°ƒç”¨ã€‚")
    
    if 'prompt_translator' not in st.session_state:
        st.session_state.prompt_translator = MultiAPIExcelTranslator(
            api_key="", 
            api_provider="DeepSeek", 
            api_url=get_api_providers()["DeepSeek"]["url"], 
            model="deepseek-chat"
        )
    
    translator = st.session_state.prompt_translator
    
    if 'term_base_loaded' not in st.session_state:
        st.session_state.term_base_loaded = False
    if 'role_personality_loaded' not in st.session_state:
        st.session_state.role_personality_loaded = False
    
    st.header("ğŸ¯ åŸºæœ¬è®¾ç½®")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        language_option = st.selectbox(
            "ğŸŒ é€‰æ‹©ç›®æ ‡è¯­è¨€",
            options=get_preset_languages(),
            index=0,
            key="prompt_language_option"
        )
    
    with col2:
        if language_option == "è‡ªå®šä¹‰":
            custom_language = st.text_input(
                "âœï¸ è¾“å…¥è‡ªå®šä¹‰è¯­è¨€",
                value=st.session_state.get('prompt_custom_language', ''),
                placeholder="ä¾‹å¦‚ï¼šä¿„æ–‡ã€è‘¡è„ç‰™æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ç­‰",
                key="prompt_custom_language_input"
            )
            st.session_state.prompt_custom_language = custom_language
            target_language = custom_language
        else:
            target_language = language_option
            st.session_state.prompt_custom_language = ""
    
    if target_language:
        translator.set_target_language(target_language)
        st.info(f"ğŸ¯ å½“å‰ç›®æ ‡è¯­è¨€: {target_language}")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.header("ğŸ“ å¾…ç¿»è¯‘æ–‡æœ¬")
        
        uploaded_file = st.file_uploader(
            "ğŸ“„ ä¸Šä¼ å¾…ç¿»è¯‘æ–‡æœ¬æ–‡ä»¶ (Excel)",
            type=['xlsx', 'xls'],
            key="prompt_file_uploader"
        )
        
        df_text = None
        text_col = None
        role_col = None
        personality_col = None
        
        if uploaded_file is not None:
            try:
                df_text = pd.read_excel(uploaded_file)
                # æ¸…ç†åˆ—åä¸­çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦
                df_text.columns = df_text.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.prompt_df_text = df_text
                st.success(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼Œå…± {len(df_text)} è¡Œæ•°æ®")
                
                with st.expander("ğŸ“Š æ–‡ä»¶é¢„è§ˆ"):
                    st.dataframe(df_text.head(10))
                
                cols = df_text.columns.tolist()
                text_col = st.selectbox(
                    "ğŸ“ é€‰æ‹©æ–‡æœ¬åˆ—",
                    options=cols,
                    index=0,
                    key="prompt_text_col_select"
                )
                
                role_col = st.selectbox(
                    "ğŸ‘¥ é€‰æ‹©è¯´è¯äºº/è§’è‰²åˆ— (å¯é€‰)",
                    options=["æ— "] + cols,
                    index=0,
                    key="prompt_role_col_select"
                )
                role_col = role_col if role_col != "æ— " else None
                
                personality_col = st.selectbox(
                    "ğŸ’¬ é€‰æ‹©æ€§æ ¼æè¿°åˆ— (å¯é€‰)",
                    options=["æ— "] + cols,
                    index=0,
                    key="prompt_personality_col_select"
                )
                personality_col = personality_col if personality_col != "æ— " else None
                
                st.session_state.prompt_text_col = text_col
                st.session_state.prompt_role_col = role_col
                st.session_state.prompt_personality_col = personality_col
                
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        else:
            if 'prompt_df_text' in st.session_state:
                df_text = st.session_state.prompt_df_text
                text_col = st.session_state.get('prompt_text_col')
                role_col = st.session_state.get('prompt_role_col')
                personality_col = st.session_state.get('prompt_personality_col')
                
                # æ˜¾ç¤ºå½“å‰å·²åŠ è½½çš„æ–‡ä»¶ä¿¡æ¯
                if df_text is not None:
                    st.info(f"âœ… å·²åŠ è½½æ–‡ä»¶ï¼š{len(df_text)} è¡Œæ•°æ®")
                    if text_col:
                        st.write(f"ğŸ“ æ–‡æœ¬åˆ—: {text_col}")
                    if role_col:
                        st.write(f"ğŸ‘¥ è§’è‰²åˆ—: {role_col}")
                    if personality_col:
                        st.write(f"ğŸ’¬ æ€§æ ¼åˆ—: {personality_col}")
        
        st.subheader("âœ‚ï¸ åˆ†æ‰¹æ¬¡è®¾ç½®")
        batch_size = st.number_input(
            "æ¯æ‰¹æ¬¡è¡Œæ•°",
            min_value=1,
            max_value=200,
            value=50,
            step=10,
            key="prompt_batch_size"
        )
    
    with col2:
        st.header("ğŸ“š æœ¯è¯­åº“å’Œæ€§æ ¼åº“")
        
        st.subheader("ğŸ“š æœ¯è¯­åº“åŠŸèƒ½")
        uploaded_term_base = st.file_uploader(
            "ğŸ“š ä¸Šä¼ æœ¯è¯­åº“æ–‡ä»¶ (Excel)",
            type=['xlsx', 'xls'],
            key="prompt_term_base_uploader"
        )
        
        if uploaded_term_base is not None:
            try:
                df = pd.read_excel(uploaded_term_base)
                # æ¸…ç†åˆ—åä¸­çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦
                df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.prompt_term_base_df = df
                st.success(f"âœ… æˆåŠŸè¯»å–æœ¯è¯­åº“ï¼Œå…± {len(df)} æ¡è®°å½•")
                
                with st.expander("ğŸ“Š æœ¯è¯­åº“é¢„è§ˆ"):
                    st.dataframe(df.head(10))
                
                cols = df.columns.tolist()
                
                source_col = st.selectbox(
                    "ğŸ“ é€‰æ‹©ä¸­æ–‡æºæ–‡åˆ—",
                    options=cols,
                    index=0,
                    key="prompt_term_source_col"
                )
                
                target_col = st.selectbox(
                    "ğŸ“¤ é€‰æ‹©ç¿»è¯‘åˆ—",
                    options=cols,
                    index=min(1, len(cols)-1) if len(cols) > 1 else 0,
                    key="prompt_term_target_col"
                )
                
                if st.button("ğŸ“¥ åŠ è½½æœ¯è¯­åº“", key="prompt_load_term_base"):
                    if translator.load_term_base(df, source_col, target_col):
                        st.session_state.term_base_loaded = True
                        st.success("âœ… æœ¯è¯­åº“åŠ è½½æˆåŠŸ")
                        st.rerun()
                
            except Exception as e:
                st.error(f"âŒ å¤„ç†æœ¯è¯­åº“æ–‡ä»¶å¤±è´¥: {e}")
        
        if st.session_state.get('term_base_loaded', False):
            st.info(f"âœ… æœ¯è¯­åº“å·²åŠ è½½: {len(translator.term_base_list)} æ¡æœ¯è¯­")
        
        st.divider()
        
        st.subheader("ğŸ‘¤ è§’è‰²æ€§æ ¼åº“åŠŸèƒ½")
        uploaded_role = st.file_uploader(
            "ğŸ“‹ ä¸Šä¼ è§’è‰²æ€§æ ¼åº“æ–‡ä»¶ (Excel)",
            type=['xlsx', 'xls'],
            key="prompt_role_personality_uploader"
        )
        
        if uploaded_role is not None:
            try:
                df = pd.read_excel(uploaded_role)
                # æ¸…ç†åˆ—åä¸­çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦
                df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.prompt_role_personality_df = df
                st.success(f"âœ… æˆåŠŸè¯»å–è§’è‰²æ€§æ ¼åº“ï¼Œå…± {len(df)} æ¡è®°å½•")
                
                with st.expander("ğŸ“Š è§’è‰²æ€§æ ¼åº“é¢„è§ˆ"):
                    st.dataframe(df.head(10))
                
                cols = df.columns.tolist()
                role_col = st.selectbox(
                    "ğŸ‘¥ é€‰æ‹©è§’è‰²åç§°åˆ—",
                    options=cols,
                    index=0,
                    key="prompt_role_name_col"
                )
                
                personality_col = st.selectbox(
                    "ğŸ’¬ é€‰æ‹©æ€§æ ¼æè¿°åˆ—",
                    options=cols,
                    index=min(1, len(cols)-1) if len(cols) > 1 else 0,
                    key="prompt_personality_desc_col"
                )
                
                if st.button("ğŸ“¥ åŠ è½½è§’è‰²æ€§æ ¼åº“", key="prompt_load_role_personality"):
                    if translator.load_role_personality(df, role_col, personality_col):
                        st.session_state.role_personality_loaded = True
                        st.success("âœ… è§’è‰²æ€§æ ¼åº“åŠ è½½æˆåŠŸ")
                        st.rerun()
                
            except Exception as e:
                st.error(f"âŒ å¤„ç†è§’è‰²æ€§æ ¼åº“æ–‡ä»¶å¤±è´¥: {e}")
        
        if st.session_state.get('role_personality_loaded', False):
            st.info(f"âœ… è§’è‰²æ€§æ ¼åº“å·²åŠ è½½: {len(translator.role_personality_dict)} æ¡è§’è‰²")
    
    st.divider()
    
    st.header("ğŸ¯ ç¿»è¯‘è¦æ±‚è®¾ç½®")
    
    st.subheader("ğŸ·ï¸ é¢„è®¾é€‰é¡¹")
    presets = get_preset_options()
    preset_options = st.multiselect(
        "é€‰æ‹©é¢„è®¾ç¿»è¯‘è¦æ±‚ï¼ˆå¯å¤šé€‰ï¼‰",
        options=list(presets.keys()),
        default=st.session_state.get('prompt_preset_options', []),
        key="prompt_preset_multiselect"
    )
    st.session_state.prompt_preset_options = preset_options
    
    custom_requirements = st.text_area(
        "ğŸ’¬ è‡ªå®šä¹‰ç¿»è¯‘è¦æ±‚",
        value=st.session_state.get('prompt_custom_requirements', get_default_custom_requirements()),
        placeholder=f"ä¾‹å¦‚ï¼šæ¸¸æˆUIç®€çº¦é£æ ¼ã€è§’è‰²å¯¹è¯è‡ªç„¶æµç•…ã€ä¸“ä¸šæœ¯è¯­ç»Ÿä¸€ã€{target_language}æœ¬åœ°åŒ–é€‚é…ç­‰",
        height=100,
        key="prompt_custom_requirements_text"
    )
    st.session_state.prompt_custom_requirements = custom_requirements
    
    all_requirements = []
    if preset_options:
        all_requirements.extend(preset_options)
    if custom_requirements:
        all_requirements.append(custom_requirements)
    
    final_requirements = "ï¼›".join(all_requirements) if all_requirements else ""
    
    if st.button("ğŸš€ ç”Ÿæˆæç¤ºè¯", key="generate_prompt_btn", use_container_width=True):
        if df_text is None or text_col is None:
            st.error("âŒ è¯·å…ˆä¸Šä¼ å¾…ç¿»è¯‘æ–‡æœ¬æ–‡ä»¶å¹¶é€‰æ‹©æ–‡æœ¬åˆ—ã€‚")
            return
        
        # ä»session_stateè·å–æœ€æ–°çš„åˆ—é€‰æ‹©
        text_col = st.session_state.get('prompt_text_col')
        role_col = st.session_state.get('prompt_role_col')
        personality_col = st.session_state.get('prompt_personality_col')
        
        if not text_col:
            st.error("âŒ è¯·é€‰æ‹©æ–‡æœ¬åˆ—ã€‚")
            return
        
        if not target_language or target_language.strip() == "":
            st.error("âŒ è¯·å…ˆé€‰æ‹©æˆ–è¾“å…¥ç›®æ ‡è¯­è¨€ã€‚")
            return
        
        term_base_loaded = st.session_state.get('term_base_loaded', False)
        role_personality_loaded = st.session_state.get('role_personality_loaded', False)
        
        if term_base_loaded:
            st.info(f"âœ… æœ¯è¯­åº“å·²åŠ è½½: {len(translator.term_base_list)} æ¡æœ¯è¯­")
        else:
            st.warning("âš ï¸ æœªåŠ è½½æœ¯è¯­åº“ï¼Œæç¤ºè¯ä¸­å°†ä¸åŒ…å«æœ¯è¯­åŒ¹é…ä¿¡æ¯")
        
        if role_personality_loaded:
            st.info(f"âœ… è§’è‰²æ€§æ ¼åº“å·²åŠ è½½: {len(translator.role_personality_dict)} æ¡è§’è‰²")
        else:
            st.warning("âš ï¸ æœªåŠ è½½è§’è‰²æ€§æ ¼åº“ï¼Œæç¤ºè¯ä¸­å°†ä¸åŒ…å«è§’è‰²æ€§æ ¼ä¿¡æ¯")
        
        fixed_requirements = f"""
## ç¿»è¯‘è¦æ±‚ï¼š(å›ºå®š)
ä½ æ˜¯ä¸€åä¸“ä¸šçš„äºŒæ¬¡å…ƒæ¸¸æˆæœ¬åœ°åŒ–ç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†ä¸­æ–‡äºŒæ¬¡å…ƒæ¸¸æˆæ–‡æ¡ˆç¿»è¯‘ä¸º{target_language}ã€‚è¯·å°†ç”¨æˆ·è¾“å…¥çš„ä¸­æ–‡æ¸¸æˆæ–‡æœ¬ï¼Œä»¥è¡¨æ ¼å½¢å¼è¾“å‡ºå¯¹åº”çš„{target_language}ç¿»è¯‘ã€‚è¡¨æ ¼åº”åŒ…å«ä¸¤åˆ—ï¼šåŸæ–‡ï¼ˆä¸­æ–‡ï¼‰ã€{target_language}ç¿»è¯‘ã€‚"""
        
        language_specific_requirements = translator.get_language_specific_requirements(target_language)
        
        other_requirements = f"""
## å…¶ä»–è¦æ±‚ï¼š(ç”¨æˆ·è¾“å…¥)
{final_requirements if final_requirements else "æ— "}
"""
        
        important_notes = f"""
## é‡è¦è¯´æ˜ï¼ï¼š(å›ºå®š)
â€¢ è¯·åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ç»“æœï¼Œä»¥è¡¨æ ¼å½¢å¼è¾“å‡ºä¸­æ–‡åŸæ–‡ï¼Œ{target_language}ç¿»è¯‘
â€¢ ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å¤‡æ³¨
â€¢ æœ¯è¯­åº“ä¸­çš„ç‰¹å®šè¯æ±‡ç¿»è¯‘éœ€è¦ä¸¥æ ¼é‡‡ç”¨ç›¸åŒçš„ç¿»è¯‘
â€¢ è¯·æ ¹æ®è§’è‰²æ€§æ ¼ç‰¹ç‚¹è°ƒæ•´ç¿»è¯‘é£æ ¼å’Œè¯­æ°”ã€‚
â€¢ æœ¬æ¬¡ç¿»è¯‘ç›®æ ‡è¯­è¨€ä¸ºï¼š{target_language}
"""
        
        num_batches = (len(df_text) + batch_size - 1) // batch_size
        all_prompts = []
        
        for i in range(num_batches):
            start_index = i * batch_size
            end_index = min((i + 1) * batch_size, len(df_text))
            batch_df = df_text.iloc[start_index:end_index].copy()
            
            translator.reset_context()
            
            # è·å–æ–‡æœ¬åˆ—æ•°æ®
            text_list = batch_df[text_col].tolist()
            
            # å®‰å…¨è·å–è§’è‰²åˆ—æ•°æ®
            if role_col and role_col != "æ— " and role_col in batch_df.columns:
                role_list = batch_df[role_col].tolist()
            else:
                role_list = [None] * len(batch_df)
            
            # å®‰å…¨è·å–æ€§æ ¼åˆ—æ•°æ®
            if personality_col and personality_col != "æ— " and personality_col in batch_df.columns:
                personality_list = batch_df[personality_col].tolist()
            else:
                personality_list = [None] * len(batch_df)
            
            all_text_in_batch = " ".join([str(t) for t in text_list if not pd.isna(t)])
            
            term_base_prompt = ""
            if term_base_loaded:
                term_base_prompt = translator.build_term_base_prompt(all_text_in_batch)
            else:
                term_base_prompt = "\n\n### æœ¯è¯­åº“åŒ¹é…ï¼š\næ— æœ¯è¯­åº“åŠ è½½ï¼Œè·³è¿‡æœ¯è¯­åŒ¹é…ã€‚"
            
            text_and_personality_prompt = "### å¾…ç¿»è¯‘æ–‡æœ¬åŠè¯´è¯äººæ€§æ ¼æ ¼å¼ï¼š(ç”¨æˆ·è¾“å…¥)\n"
            text_and_personality_prompt += "è¯´è¯äºº\tåŸæ–‡\tè¯´è¯äººæ€§æ ¼\n"
            
            for j in range(len(batch_df)):
                role = role_list[j]
                text = text_list[j]
                personality = personality_list[j]
                
                if personality_col and not pd.isna(personality):
                    personality_desc = str(personality).strip()
                elif role_col and role and role_personality_loaded:
                    personality_desc = translator.find_role_personality(role)
                    personality_desc = personality_desc if personality_desc else "æ— "
                else:
                    personality_desc = "æ— "
                
                role_name = str(role).strip() if role and not pd.isna(role) else "æ— "
                text_content = str(text).strip() if not pd.isna(text) else ""
                
                text_and_personality_prompt += f"{role_name}\t{text_content}\t{personality_desc}\n"
            
            full_prompt = f"""
# æ‰¹æ¬¡ {i+1}/{num_batches} - ç›®æ ‡è¯­è¨€: {target_language}

{fixed_requirements}

{language_specific_requirements}

{other_requirements}

{term_base_prompt}

{text_and_personality_prompt}

{important_notes}
"""
            all_prompts.append(full_prompt.strip())
        
        st.subheader("âœ… ç”Ÿæˆç»“æœ")
        
        st.session_state.all_prompts = all_prompts
        st.session_state.num_batches = num_batches
        st.session_state.current_batch_index = 0
        st.session_state.target_language = target_language
        
        st.success(f"âœ… æç¤ºè¯ç”ŸæˆæˆåŠŸï¼Œå…± {num_batches} ä¸ªæ‰¹æ¬¡ï¼Œç›®æ ‡è¯­è¨€: {target_language}ã€‚")
    
    if st.session_state.get('all_prompts'):
        all_prompts = st.session_state.all_prompts
        num_batches = st.session_state.num_batches
        current_batch_index = st.session_state.current_batch_index
        target_language = st.session_state.get('target_language', 'è‹±æ–‡')
        
        st.subheader(f"æ‰¹æ¬¡ {current_batch_index + 1}/{num_batches} æç¤ºè¯ - ç›®æ ‡è¯­è¨€: {target_language}")
        
        current_prompt = all_prompts[current_batch_index]
        
        st.code(current_prompt, language=None)
        st.info("ğŸ‘† è¯·ä½¿ç”¨ä¸Šæ–¹ä»£ç å—å³ä¸‹è§’çš„å¤åˆ¶æŒ‰é’®è¿›è¡Œä¸€é”®å¤åˆ¶ã€‚")
        
        col_prev, col_info, col_next = st.columns([1, 2, 1])
        
        with col_prev:
            if st.button("ä¸Šä¸€æ‰¹æ¬¡", disabled=(current_batch_index == 0), key="prompt_prev_batch"):
                st.session_state.current_batch_index -= 1
                st.rerun()
                
        with col_info:
            st.markdown(f"<p style='text-align: center;'>å½“å‰æ‰¹æ¬¡: {current_batch_index + 1} / {num_batches} | ç›®æ ‡è¯­è¨€: {target_language}</p>", unsafe_allow_html=True)
            
        with col_next:
            if st.button("ä¸‹ä¸€æ‰¹æ¬¡", disabled=(current_batch_index == num_batches - 1), key="prompt_next_batch"):
                st.session_state.current_batch_index += 1
                st.rerun()
                
        st.markdown("---")
        
        final_output = f"# ç¿»è¯‘æç¤ºè¯ - ç›®æ ‡è¯­è¨€: {target_language}\n\n" + ("-"*80) + "\n\n".join(all_prompts)
        
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½æ‰€æœ‰æç¤ºè¯ (.txt)",
            data=final_output,
            file_name=f"translation_prompts_{target_language}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
            use_container_width=True
        )

class ExcelSearchReplace:
    def __init__(self):
        self.excel_files = []
        self.search_results = {}
        self.case_sensitive = False
        self.match_whole_word = False
        
    def find_excel_files(self, folder_path):
        """æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰Excelæ–‡ä»¶"""
        self.excel_files = []
        folder_path = Path(folder_path)
        
        if not folder_path.exists():
            return False, "æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨"
        
        # æ”¯æŒçš„Excelæ–‡ä»¶æ‰©å±•å
        excel_extensions = ['.xlsx', '.xls', '.xlsm', '.xlsb']
        
        for ext in excel_extensions:
            self.excel_files.extend(folder_path.rglob(f'*{ext}'))
        
        return True, f"æ‰¾åˆ° {len(self.excel_files)} ä¸ªExcelæ–‡ä»¶"
    
    def search_in_excel(self, search_term, case_sensitive=False, match_whole_word=False):
        """åœ¨Excelæ–‡ä»¶ä¸­æœç´¢è¯è¯­"""
        self.search_results = {}
        self.case_sensitive = case_sensitive
        self.match_whole_word = match_whole_word
        total_matches = 0
        
        for file_path in self.excel_files:
            try:
                # è¯»å–Excelæ–‡ä»¶çš„æ‰€æœ‰å·¥ä½œè¡¨
                excel_data = pd.read_excel(file_path, sheet_name=None, dtype=str)
                file_matches = []
                
                for sheet_name, df in excel_data.items():
                    sheet_matches = self._search_in_dataframe(df, search_term, sheet_name, str(file_path))
                    file_matches.extend(sheet_matches)
                
                if file_matches:
                    self.search_results[str(file_path)] = {
                        'matches': file_matches,
                        'match_count': len(file_matches)
                    }
                    total_matches += len(file_matches)
                    
            except Exception as e:
                st.error(f"è¯»å–æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
        
        return total_matches
    
    def _search_in_dataframe(self, df, search_term, sheet_name, file_path):
        """åœ¨DataFrameä¸­æœç´¢è¯è¯­"""
        matches = []
        
        # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        if self.match_whole_word:
            pattern = r'\b' + re.escape(search_term) + r'\b'
        else:
            pattern = re.escape(search_term)
        
        flags = 0 if self.case_sensitive else re.IGNORECASE
        
        for row_idx, row in df.iterrows():
            for col_idx, cell_value in enumerate(row):
                if pd.isna(cell_value):
                    continue
                
                cell_str = str(cell_value)
                matches_found = list(re.finditer(pattern, cell_str, flags))
                
                for match in matches_found:
                    matches.append({
                        'file_path': file_path,
                        'sheet_name': sheet_name,
                        'row': row_idx + 2,  # +2 å› ä¸ºExcelä»1å¼€å§‹ï¼Œä¸”æœ‰æ ‡é¢˜è¡Œ
                        'column': df.columns[col_idx] if col_idx < len(df.columns) else f'Col{col_idx+1}',
                        'original_text': cell_str,
                        'matched_text': match.group(),
                        'start_pos': match.start(),
                        'end_pos': match.end()
                    })
        
        return matches
    
    def replace_in_excel(self, search_term, replace_term, backup=True):
        """æ›¿æ¢Excelæ–‡ä»¶ä¸­çš„è¯è¯­"""
        replaced_files = 0
        total_replacements = 0
        
        for file_path_str, file_data in self.search_results.items():
            file_path = Path(file_path_str)
            
            try:
                # å¤‡ä»½åŸæ–‡ä»¶
                if backup:
                    backup_path = file_path.parent / f"{file_path.stem}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}{file_path.suffix}"
                    shutil.copy2(file_path, backup_path)
                    st.info(f"å·²åˆ›å»ºå¤‡ä»½: {backup_path.name}")
                
                # è¯»å–Excelæ–‡ä»¶
                excel_data = pd.read_excel(file_path, sheet_name=None, dtype=str)
                replacements_in_file = 0
                
                # æ„å»ºæ›¿æ¢æ¨¡å¼
                if self.match_whole_word:
                    pattern = r'\b' + re.escape(search_term) + r'\b'
                else:
                    pattern = re.escape(search_term)
                
                flags = 0 if self.case_sensitive else re.IGNORECASE
                
                # å¯¹æ¯ä¸ªå·¥ä½œè¡¨è¿›è¡Œæ›¿æ¢
                for sheet_name, df in excel_data.items():
                    df_replaced = df.applymap(
                        lambda x: self._replace_text(x, pattern, replace_term, flags) 
                        if pd.notna(x) else x
                    )
                    excel_data[sheet_name] = df_replaced
                    
                    # è®¡ç®—æ›¿æ¢æ•°é‡
                    for row_idx, row in df.iterrows():
                        for col_idx, cell_value in enumerate(row):
                            if pd.isna(cell_value):
                                continue
                            cell_str = str(cell_value)
                            replacements = len(re.findall(pattern, cell_str, flags))
                            replacements_in_file += replacements
                
                # ä¿å­˜æ›¿æ¢åçš„æ–‡ä»¶
                with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                    for sheet_name, df in excel_data.items():
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                replaced_files += 1
                total_replacements += replacements_in_file
                
                st.success(f"âœ… {file_path.name}: å®Œæˆ {replacements_in_file} å¤„æ›¿æ¢")
                
            except Exception as e:
                st.error(f"æ›¿æ¢æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
        
        return replaced_files, total_replacements
    
    def _replace_text(self, text, pattern, replace_term, flags):
        """æ›¿æ¢æ–‡æœ¬ä¸­çš„åŒ¹é…é¡¹"""
        if pd.isna(text):
            return text
        
        text_str = str(text)
        replaced_text = re.sub(pattern, replace_term, text_str, flags=flags)
        return replaced_text

import streamlit as st
import pandas as pd
from pathlib import Path
import os
import subprocess
import platform
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
def grand_match():
    model_GRAND_match.model_grand_match.grand_match()
def excel_replace_page():
    st.set_page_config(
        page_title="Excelæ–‡ä»¶æ‰¹é‡æœç´¢æ›¿æ¢å·¥å…·",
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ” Excelæ–‡ä»¶æ‰¹é‡æœç´¢æ›¿æ¢å·¥å…·")
    st.markdown("### æ‰¹é‡æœç´¢å’Œæ›¿æ¢æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰Excelæ–‡ä»¶çš„å†…å®¹")
    
    # åˆå§‹åŒ–æœç´¢æ›¿æ¢å·¥å…·
    if 'search_tool' not in st.session_state:
        st.session_state.search_tool = ExcelSearchReplace()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡
    if 'folder_path' not in st.session_state:
        st.session_state.folder_path = ""
    if 'search_term' not in st.session_state:
        st.session_state.search_term = ""
    if 'replace_term' not in st.session_state:
        st.session_state.replace_term = ""
    if 'case_sensitive' not in st.session_state:
        st.session_state.case_sensitive = False
    if 'match_whole_word' not in st.session_state:
        st.session_state.match_whole_word = False
    if 'replace_confirmed' not in st.session_state:
        st.session_state.replace_confirmed = False
    if 'show_confirm_checkbox' not in st.session_state:
        st.session_state.show_confirm_checkbox = False
    if 'edited_data' not in st.session_state:
        st.session_state.edited_data = {}
    
    search_tool = st.session_state.search_tool
    
    # ä¾§è¾¹æ  - æ–‡ä»¶å¤¹é€‰æ‹©
    st.sidebar.header("ğŸ“ æ–‡ä»¶å¤¹è®¾ç½®")
    folder_path = st.sidebar.text_input(
        "è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„:",
        value=st.session_state.folder_path,
        placeholder="ä¾‹å¦‚: C:/Users/ç”¨æˆ·å/Documents/Excelæ–‡ä»¶",
        help="è¯·è¾“å…¥åŒ…å«Excelæ–‡ä»¶çš„æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„"
    )
    
    if folder_path and folder_path != st.session_state.folder_path:
        st.session_state.folder_path = folder_path
        success, message = search_tool.find_excel_files(folder_path)
        if success:
            st.sidebar.success(message)
        else:
            st.sidebar.error(message)
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨
    if search_tool.excel_files:
        st.sidebar.subheader("ğŸ“Š æ‰¾åˆ°çš„Excelæ–‡ä»¶")
        for i, file_path in enumerate(search_tool.excel_files[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            st.sidebar.write(f"{i+1}. {file_path.name}")
        
        if len(search_tool.excel_files) > 10:
            st.sidebar.info(f"... è¿˜æœ‰ {len(search_tool.excel_files) - 10} ä¸ªæ–‡ä»¶")
    
    # ä¸»ç•Œé¢ - æœç´¢è®¾ç½®
    st.header("ğŸ” æœç´¢è®¾ç½®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        search_term = st.text_input(
            "æœç´¢è¯è¯­:",
            value=st.session_state.search_term,
            placeholder="è¯·è¾“å…¥è¦æœç´¢çš„è¯è¯­",
            help="æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼è¯­æ³•"
        )
        st.session_state.search_term = search_term
    
    with col2:
        # æœç´¢é€‰é¡¹
        st.subheader("âš™ï¸ æœç´¢é€‰é¡¹")
        case_sensitive = st.checkbox(
            "å¤§å°å†™æ•æ„Ÿ",
            value=st.session_state.case_sensitive,
            help="å‹¾é€‰ååŒºåˆ†å¤§å°å†™"
        )
        st.session_state.case_sensitive = case_sensitive
        
        match_whole_word = st.checkbox(
            "å…¨è¯åŒ¹é…",
            value=st.session_state.match_whole_word,
            help="å‹¾é€‰ååªåŒ¹é…å®Œæ•´è¯è¯­"
        )
        st.session_state.match_whole_word = match_whole_word
    
    # æœç´¢æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æœç´¢", key="search_btn", use_container_width=True):
        if not folder_path:
            st.error("âŒ è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„")
            return
        
        if not search_term:
            st.error("âŒ è¯·è¾“å…¥æœç´¢è¯è¯­")
            return
        
        # æ‰§è¡Œå¤šçº¿ç¨‹æœç´¢
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        with st.spinner("æ­£åœ¨æœç´¢Excelæ–‡ä»¶..."):
            total_matches = multithreaded_search(
                search_tool,
                search_term,
                case_sensitive,
                match_whole_word,
                progress_bar,
                status_text
            )
        
        progress_bar.empty()
        status_text.empty()
        
        if total_matches > 0:
            st.success(f"âœ… æœç´¢å®Œæˆï¼å…±æ‰¾åˆ° {total_matches} ä¸ªåŒ¹é…é¡¹")
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…é¡¹")
    
    # æ˜¾ç¤ºæœç´¢ç»“æœ
    if search_tool.search_results:
        st.header("ğŸ“Š æœç´¢ç»“æœé¢„è§ˆ")
        
        total_files = len(search_tool.search_results)
        total_matches = sum(data['match_count'] for data in search_tool.search_results.values())
        
        st.info(f"**ç»Ÿè®¡ä¿¡æ¯:** åœ¨ {total_files} ä¸ªæ–‡ä»¶ä¸­æ‰¾åˆ° {total_matches} ä¸ªåŒ¹é…é¡¹")
        
        # æ–‡ä»¶åˆ—è¡¨
        selected_file = st.selectbox(
            "é€‰æ‹©æ–‡ä»¶æŸ¥çœ‹è¯¦æƒ…:",
            options=list(search_tool.search_results.keys()),
            format_func=lambda x: f"{Path(x).name} ({search_tool.search_results[x]['match_count']} å¤„)"
        )
        
        if selected_file:
            file_data = search_tool.search_results[selected_file]
            matches = file_data['matches']
            
            # æ–‡ä»¶ä¿¡æ¯å’Œæ“ä½œæŒ‰é’®
            st.subheader(f"ğŸ“„ æ–‡ä»¶: {Path(selected_file).name}")
            
            # æ˜¾ç¤ºå®Œæ•´æ–‡ä»¶è·¯å¾„
            st.code(selected_file, language=None)
            
            # æ“ä½œæŒ‰é’®è¡Œ
            col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 3])
            
            with col_btn1:
                if st.button("ğŸ“‚ æ‰“å¼€æ–‡ä»¶å¤¹", key=f"open_folder_{selected_file}"):
                    open_folder(selected_file)
            
            with col_btn2:
                if st.button("ğŸ“Š æ‰“å¼€Excel", key=f"open_excel_{selected_file}"):
                    open_file(selected_file)
            
            st.write(f"**åŒ¹é…æ•°é‡:** {len(matches)} å¤„")
            
            # æ˜¾ç¤ºåŒ¹é…è¯¦æƒ… - ä½¿ç”¨å¯ç¼–è¾‘çš„è¡¨æ ¼
            display_rows = []
            row_identifiers = []  # å­˜å‚¨è¡Œæ ‡è¯†ç¬¦ç”¨äºåç»­ä¿å­˜
            
            for i, match in enumerate(matches[:50]):  # åªæ˜¾ç¤ºå‰50ä¸ª
                # è·å–è¯¥è¡Œçš„å®Œæ•´æ•°æ®
                row_data = get_row_data_as_list(selected_file, match['sheet_name'], match['row'])
                
                # æ·»åŠ ä½ç½®ä¿¡æ¯ä½œä¸ºç¬¬ä¸€åˆ—
                row_dict = {
                    "ä½ç½®": f"{match['sheet_name']} | è¡Œ{match['row']} åˆ—{match['column']}"
                }
                
                # æ·»åŠ è¯¥è¡Œçš„æ‰€æœ‰åˆ—æ•°æ®
                if row_data:
                    for col_idx, cell_value in enumerate(row_data, start=1):
                        row_dict[f"åˆ—{col_idx}"] = cell_value
                
                display_rows.append(row_dict)
                row_identifiers.append({
                    'sheet': match['sheet_name'],
                    'row': match['row'],
                    'original_data': row_data
                })
            
            if display_rows:
                st.markdown("### âœï¸ å¯ç¼–è¾‘è¡¨æ ¼ï¼ˆç›´æ¥ä¿®æ”¹å•å…ƒæ ¼å†…å®¹ï¼‰")
                
                # åˆ›å»ºDataFrame
                df_display = pd.DataFrame(display_rows)
                
                # ä½¿ç”¨data_editoråˆ›å»ºå¯ç¼–è¾‘è¡¨æ ¼
                edited_df = st.data_editor(
                    df_display,
                    use_container_width=True,
                    height=400,
                    key=f"editable_table_{selected_file}",
                    column_config={
                        "ä½ç½®": st.column_config.TextColumn(
                            "ä½ç½®",
                            disabled=True,  # ä½ç½®åˆ—ä¸å¯ç¼–è¾‘
                            width="medium"
                        )
                    }
                )
                
                # æ£€æµ‹æ˜¯å¦æœ‰ä¿®æ”¹
                if not df_display.equals(edited_df):
                    st.warning("âš ï¸ æ£€æµ‹åˆ°å†…å®¹å·²ä¿®æ”¹ï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¿å­˜æ›´æ”¹")
                    
                    # æ˜¾ç¤ºä¿®æ”¹å¯¹æ¯”
                    with st.expander("ğŸ“‹ æŸ¥çœ‹ä¿®æ”¹è¯¦æƒ…"):
                        changes_found = False
                        for idx in range(len(df_display)):
                            for col in df_display.columns:
                                if col != "ä½ç½®":  # è·³è¿‡ä½ç½®åˆ—
                                    old_val = df_display.iloc[idx][col]
                                    new_val = edited_df.iloc[idx][col]
                                    if old_val != new_val:
                                        changes_found = True
                                        st.markdown(f"**{df_display.iloc[idx]['ä½ç½®']} - {col}:**")
                                        st.markdown(f"- åŸå€¼: `{old_val}`")
                                        st.markdown(f"- æ–°å€¼: `{new_val}`")
                                        st.markdown("---")
                        
                        if not changes_found:
                            st.info("æœªæ£€æµ‹åˆ°æœ‰æ•ˆä¿®æ”¹")
                    
                    # ä¿å­˜ä¿®æ”¹æŒ‰é’®
                    col_save1, col_save2 = st.columns([1, 3])
                    
                    with col_save1:
                        if st.button("ğŸ’¾ ä¿å­˜ä¿®æ”¹åˆ°Excel", key=f"save_edits_{selected_file}", type="primary"):
                            try:
                                # åŠ è½½Excelæ–‡ä»¶
                                wb = openpyxl.load_workbook(selected_file)
                                
                                # éå†æ‰€æœ‰ä¿®æ”¹
                                changes_count = 0
                                for idx in range(len(edited_df)):
                                    sheet_name = row_identifiers[idx]['sheet']
                                    row_num = row_identifiers[idx]['row']
                                    ws = wb[sheet_name]
                                    
                                    # æ£€æŸ¥æ¯ä¸€åˆ—çš„ä¿®æ”¹
                                    for col in edited_df.columns:
                                        if col != "ä½ç½®":
                                            col_idx = int(col.replace("åˆ—", ""))
                                            new_val = edited_df.iloc[idx][col]
                                            old_val = df_display.iloc[idx][col]
                                            
                                            if new_val != old_val:
                                                # å†™å…¥æ–°å€¼åˆ°Excel
                                                ws.cell(row=row_num, column=col_idx, value=new_val)
                                                changes_count += 1
                                
                                # ä¿å­˜æ–‡ä»¶
                                wb.save(selected_file)
                                wb.close()
                                
                                st.success(f"âœ… æˆåŠŸä¿å­˜ {changes_count} å¤„ä¿®æ”¹ï¼")
                                
                                # æç¤ºé‡æ–°æœç´¢
                                st.info("ğŸ’¡ å»ºè®®é‡æ–°æœç´¢ä»¥æŸ¥çœ‹æœ€æ–°å†…å®¹")
                                
                            except Exception as e:
                                st.error(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
                    
                    with col_save2:
                        if st.button("ğŸ”„ æ’¤é”€ä¿®æ”¹", key=f"reset_edits_{selected_file}"):
                            st.rerun()
            
            if len(matches) > 50:
                st.info(f"ä»…æ˜¾ç¤ºå‰ 50 ä¸ªåŒ¹é…é¡¹ï¼Œå…±æœ‰ {len(matches)} ä¸ªåŒ¹é…é¡¹")
        
        # æ›¿æ¢åŠŸèƒ½
        st.header("ğŸ”„ æ‰¹é‡æ›¿æ¢åŠŸèƒ½")
        
        col1, col2 = st.columns(2)
        
        with col1:
            replace_term = st.text_input(
                "æ›¿æ¢ä¸º:",
                value=st.session_state.replace_term,
                placeholder="è¯·è¾“å…¥æ›¿æ¢åçš„è¯è¯­",
                help="å°†æœç´¢åˆ°çš„è¯è¯­æ›¿æ¢ä¸ºæ­¤è¯è¯­"
            )
            st.session_state.replace_term = replace_term
        
        with col2:
            backup_files = st.checkbox(
                "åˆ›å»ºå¤‡ä»½æ–‡ä»¶",
                value=True,
                help="æ›¿æ¢å‰è‡ªåŠ¨åˆ›å»ºå¤‡ä»½æ–‡ä»¶"
            )
        
        # æ›¿æ¢é¢„è§ˆ
        if search_term and replace_term:
            st.subheader("ğŸ” æ›¿æ¢é¢„è§ˆ")
            
            # æ˜¾ç¤ºæ›¿æ¢å‰åå¯¹æ¯”ç¤ºä¾‹
            example_before = f"è¿™æ˜¯åŒ…å« {search_term} çš„ç¤ºä¾‹æ–‡æœ¬"
            example_after = example_before.replace(search_term, f"**{replace_term}**")
            
            col_before, col_arrow, col_after = st.columns([1, 0.1, 1])
            
            with col_before:
                st.text_area("æ›¿æ¢å‰:", value=example_before, height=60, disabled=True)
            
            with col_arrow:
                st.markdown("<br><h2>â†’</h2>", unsafe_allow_html=True)
            
            with col_after:
                st.text_area("æ›¿æ¢å:", value=example_after, height=60, disabled=True)
        
        # é€‰æ‹©æ€§æ›¿æ¢åŠŸèƒ½
        st.subheader("ğŸ“‹ é€‰æ‹©è¦æ›¿æ¢çš„é¡¹ç›®")
        
        # åˆå§‹åŒ–é€‰æ‹©çŠ¶æ€
        if 'selected_replacements' not in st.session_state:
            st.session_state.selected_replacements = {}
        
        # å…¨é€‰/å…¨ä¸é€‰æŒ‰é’®
        col_select1, col_select2, col_select3 = st.columns([1, 1, 3])
        with col_select1:
            if st.button("âœ… å…¨é€‰", use_container_width=True):
                for file_path in search_tool.search_results.keys():
                    st.session_state.selected_replacements[file_path] = {
                        'selected': True,
                        'rows': 'all'
                    }
                st.rerun()
        
        with col_select2:
            if st.button("âŒ å…¨ä¸é€‰", use_container_width=True):
                st.session_state.selected_replacements = {}
                st.rerun()
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºé€‰æ‹©ç•Œé¢
        for file_path, file_data in search_tool.search_results.items():
            with st.expander(f"ğŸ“„ {Path(file_path).name} ({file_data['match_count']} å¤„åŒ¹é…)", expanded=False):
                # æ–‡ä»¶çº§åˆ«çš„é€‰æ‹©
                file_key = f"file_{file_path}"
                
                # åˆå§‹åŒ–è¯¥æ–‡ä»¶çš„é€‰æ‹©çŠ¶æ€
                if file_path not in st.session_state.selected_replacements:
                    st.session_state.selected_replacements[file_path] = {
                        'selected': False,
                        'rows': 'all',
                        'selected_rows': set()
                    }
                
                col_file1, col_file2 = st.columns([1, 3])
                
                with col_file1:
                    file_selected = st.checkbox(
                        "é€‰æ‹©æ­¤æ–‡ä»¶",
                        value=st.session_state.selected_replacements[file_path]['selected'],
                        key=f"cb_{file_key}"
                    )
                    st.session_state.selected_replacements[file_path]['selected'] = file_selected
                
                with col_file2:
                    if file_selected:
                        replace_mode = st.radio(
                            "æ›¿æ¢æ¨¡å¼:",
                            options=['all', 'selected'],
                            format_func=lambda x: "æ›¿æ¢æ‰€æœ‰åŒ¹é…é¡¹" if x == 'all' else "é€‰æ‹©ç‰¹å®šè¡Œ",
                            key=f"mode_{file_key}",
                            horizontal=True
                        )
                        st.session_state.selected_replacements[file_path]['rows'] = replace_mode
                        
                        # å¦‚æœé€‰æ‹©äº†ç‰¹å®šè¡Œæ¨¡å¼ï¼Œæ˜¾ç¤ºè¡Œé€‰æ‹©ç•Œé¢
                        if replace_mode == 'selected':
                            st.markdown("**é€‰æ‹©è¦æ›¿æ¢çš„è¡Œ:**")
                            
                            matches = file_data['matches']
                            # æŒ‰å·¥ä½œè¡¨åˆ†ç»„
                            sheets_data = {}
                            for match in matches:
                                sheet_name = match['sheet_name']
                                if sheet_name not in sheets_data:
                                    sheets_data[sheet_name] = []
                                sheets_data[sheet_name].append(match)
                            
                            # ä¸ºæ¯ä¸ªå·¥ä½œè¡¨æ˜¾ç¤ºè¡Œé€‰æ‹©
                            for sheet_name, sheet_matches in sheets_data.items():
                                st.markdown(f"*å·¥ä½œè¡¨: {sheet_name}*")
                                
                                # è·å–å”¯ä¸€çš„è¡Œå·
                                unique_rows = sorted(set(m['row'] for m in sheet_matches))
                                
                                cols = st.columns(5)
                                for idx, row_num in enumerate(unique_rows):
                                    with cols[idx % 5]:
                                        row_key = f"{file_path}_{sheet_name}_{row_num}"
                                        row_selected = st.checkbox(
                                            f"è¡Œ {row_num}",
                                            value=row_key in st.session_state.selected_replacements[file_path]['selected_rows'],
                                            key=f"row_{row_key}"
                                        )
                                        
                                        if row_selected:
                                            st.session_state.selected_replacements[file_path]['selected_rows'].add(row_key)
                                        elif row_key in st.session_state.selected_replacements[file_path]['selected_rows']:
                                            st.session_state.selected_replacements[file_path]['selected_rows'].remove(row_key)
        
        # æ˜¾ç¤ºæ›¿æ¢ç»Ÿè®¡
        st.subheader("ğŸ“Š æ›¿æ¢ç»Ÿè®¡")
        selected_files_count = sum(1 for f in st.session_state.selected_replacements.values() if f['selected'])
        total_selected_matches = 0
        
        for file_path, selection in st.session_state.selected_replacements.items():
            if selection['selected']:
                if selection['rows'] == 'all':
                    total_selected_matches += search_tool.search_results[file_path]['match_count']
                else:
                    # è®¡ç®—é€‰ä¸­çš„è¡Œæ•°
                    total_selected_matches += len(selection['selected_rows'])
        
        col_stat1, col_stat2 = st.columns(2)
        with col_stat1:
            st.metric("é€‰ä¸­çš„æ–‡ä»¶æ•°", selected_files_count)
        with col_stat2:
            st.metric("é¢„è®¡æ›¿æ¢é¡¹æ•°", total_selected_matches)
        
        # æ‰§è¡Œæ›¿æ¢æŒ‰é’®
        if st.button("ğŸ”„ æ‰§è¡Œæ‰¹é‡æ›¿æ¢", key="replace_btn", type="primary", use_container_width=True):
            if not replace_term:
                st.error("âŒ è¯·è¾“å…¥æ›¿æ¢è¯è¯­")
                return
            
            if selected_files_count == 0:
                st.error("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œæ›¿æ¢")
                return
            
            # å¦‚æœè¿˜æ²¡æœ‰ç¡®è®¤,æ˜¾ç¤ºç¡®è®¤å¤é€‰æ¡†
            if not st.session_state.replace_confirmed:
                st.warning(f"âš ï¸ æ­¤æ“ä½œå°†åœ¨ {selected_files_count} ä¸ªæ–‡ä»¶ä¸­æ‰§è¡Œçº¦ {total_selected_matches} å¤„æ›¿æ¢ï¼")
                st.session_state.show_confirm_checkbox = True
        
        # æ˜¾ç¤ºç¡®è®¤å¤é€‰æ¡†
        if st.session_state.show_confirm_checkbox:
            confirm_replace = st.checkbox("æˆ‘ç¡®è®¤è¦æ‰§è¡Œæ‰¹é‡æ›¿æ¢æ“ä½œ")
            
            if confirm_replace:
                st.session_state.replace_confirmed = True
                st.session_state.show_confirm_checkbox = False
                st.rerun()
        
        # å¦‚æœå·²ç»ç¡®è®¤,æ‰§è¡Œæ›¿æ¢æ“ä½œ
        if st.session_state.replace_confirmed:
            # æ‰§è¡Œé€‰æ‹©æ€§æ›¿æ¢
            with st.spinner("æ­£åœ¨æ‰§è¡Œæ›¿æ¢æ“ä½œ..."):
                replaced_files, total_replacements = selective_replace(
                    search_tool,
                    search_term,
                    replace_term,
                    st.session_state.selected_replacements,
                    backup_files
                )
            
            if replaced_files > 0:
                st.success(f"âœ… æ›¿æ¢å®Œæˆï¼åœ¨ {replaced_files} ä¸ªæ–‡ä»¶ä¸­å®Œæˆäº† {total_replacements} å¤„æ›¿æ¢")
                
                # é‡ç½®çŠ¶æ€
                st.session_state.replace_confirmed = False
                st.session_state.show_confirm_checkbox = False
                st.session_state.selected_replacements = {}
                
                # æ¸…ç©ºæœç´¢ç»“æœ,æç¤ºé‡æ–°æœç´¢
                search_tool.search_results = {}
                st.info("ğŸ’¡ æ›¿æ¢å®Œæˆï¼Œè¯·é‡æ–°æœç´¢ä»¥æŸ¥çœ‹æ›´æ–°åçš„å†…å®¹")
                
                # æ¸…ç©ºæœç´¢å’Œæ›¿æ¢è¯
                st.session_state.search_term = ""
                st.session_state.replace_term = ""
            else:
                st.error("âŒ æ›¿æ¢æ“ä½œå¤±è´¥")
                st.session_state.replace_confirmed = False
                st.session_state.show_confirm_checkbox = False
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ## ä½¿ç”¨è¯´æ˜
        
        ### åŸºæœ¬æµç¨‹ï¼š
        1. **è®¾ç½®æ–‡ä»¶å¤¹è·¯å¾„** - åœ¨ä¾§è¾¹æ è¾“å…¥åŒ…å«Excelæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
        2. **è¾“å…¥æœç´¢è¯è¯­** - åœ¨ä¸»ç•Œé¢è¾“å…¥è¦æœç´¢çš„è¯è¯­
        3. **è®¾ç½®æœç´¢é€‰é¡¹** - é€‰æ‹©æ˜¯å¦å¤§å°å†™æ•æ„Ÿã€å…¨è¯åŒ¹é…
        4. **å¼€å§‹æœç´¢** - ç‚¹å‡»"å¼€å§‹æœç´¢"æŒ‰é’®
        5. **æŸ¥çœ‹å’Œç¼–è¾‘ç»“æœ** - æµè§ˆæœç´¢ç»“æœï¼Œç›´æ¥åœ¨è¡¨æ ¼ä¸­ä¿®æ”¹å†…å®¹
        6. **ä¿å­˜å•ä¸ªæ–‡ä»¶ä¿®æ”¹** - åœ¨å¯ç¼–è¾‘è¡¨æ ¼ä¸­ä¿®æ”¹åç‚¹å‡»"ä¿å­˜ä¿®æ”¹åˆ°Excel"
        7. **å¿«é€Ÿæ“ä½œ** - ä½¿ç”¨"æ‰“å¼€æ–‡ä»¶å¤¹"æˆ–"æ‰“å¼€Excel"æŒ‰é’®å¿«é€Ÿè®¿é—®æ–‡ä»¶
        8. **æ‰¹é‡æ›¿æ¢** - ä½¿ç”¨æ‰¹é‡æ›¿æ¢åŠŸèƒ½å¯¹å¤šä¸ªæ–‡ä»¶æ‰§è¡Œç»Ÿä¸€æ›¿æ¢
        
        ### åŠŸèƒ½ç‰¹ç‚¹ï¼š
        - âœï¸ **ç›´æ¥ç¼–è¾‘** - åœ¨æœç´¢ç»“æœè¡¨æ ¼ä¸­ç›´æ¥ä¿®æ”¹å•å…ƒæ ¼å†…å®¹
        - ğŸ’¾ **å³æ—¶ä¿å­˜** - ä¿®æ”¹åç«‹å³ä¿å­˜åˆ°Excelæ–‡ä»¶
        - ğŸ” **å¤šçº¿ç¨‹æ‰¹é‡æœç´¢** - è‡ªåŠ¨ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿæœç´¢ï¼Œå……åˆ†åˆ©ç”¨CPUèµ„æº
        - ğŸ“Š **åŸè¡¨æ ¼å±•ç¤º** - ä»¥åŸå§‹è¡¨æ ¼å½¢å¼æ˜¾ç¤ºåŒ¹é…è¡Œçš„å®Œæ•´æ•°æ®
        - ğŸ“‚ **å¿«é€Ÿè®¿é—®** - ä¸€é”®æ‰“å¼€æ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹æˆ–ç›´æ¥æ‰“å¼€Excelæ–‡ä»¶
        - ğŸ¯ **é€‰æ‹©æ€§æ›¿æ¢** - å¯ä»¥é€‰æ‹©ç‰¹å®šæ–‡ä»¶ã€ç‰¹å®šè¡Œè¿›è¡Œæ‰¹é‡æ›¿æ¢
        - âš™ï¸ **çµæ´»é€‰é¡¹** - æ”¯æŒå¤§å°å†™æ•æ„Ÿå’Œå…¨è¯åŒ¹é…
        - ğŸ’¾ **è‡ªåŠ¨å¤‡ä»½** - æ‰¹é‡æ›¿æ¢å‰å¯è‡ªåŠ¨åˆ›å»ºå¤‡ä»½æ–‡ä»¶
        - ğŸ“ **å¤šæ ¼å¼æ”¯æŒ** - æ”¯æŒ .xlsx, .xls, .xlsm, .xlsb æ ¼å¼
        - âš¡ **å®æ—¶è¿›åº¦** - æ˜¾ç¤ºæœç´¢å’Œæ›¿æ¢çš„å®æ—¶è¿›åº¦
        
        ### ä¸¤ç§ä¿®æ”¹æ–¹å¼ï¼š
        1. **ç›´æ¥ç¼–è¾‘ï¼ˆæ¨èç”¨äºå°‘é‡ç²¾ç¡®ä¿®æ”¹ï¼‰**
           - åœ¨æœç´¢ç»“æœè¡¨æ ¼ä¸­ç›´æ¥ä¿®æ”¹å•å…ƒæ ¼
           - ç‚¹å‡»"ä¿å­˜ä¿®æ”¹åˆ°Excel"å³æ—¶ä¿å­˜
           - é€‚åˆä¿®æ”¹ä¸ªåˆ«å•å…ƒæ ¼å†…å®¹
        
        2. **æ‰¹é‡æ›¿æ¢ï¼ˆæ¨èç”¨äºå¤§é‡ç»Ÿä¸€æ›¿æ¢ï¼‰**
           - é€‰æ‹©è¦æ›¿æ¢çš„æ–‡ä»¶å’Œè¡Œ
           - æ‰§è¡Œç»Ÿä¸€çš„æŸ¥æ‰¾æ›¿æ¢æ“ä½œ
           - å¯åˆ›å»ºå¤‡ä»½æ–‡ä»¶
           - é€‚åˆå¤§è§„æ¨¡ç»Ÿä¸€ä¿®æ”¹
        
        ### æ³¨æ„äº‹é¡¹ï¼š
        - ç›´æ¥ç¼–è¾‘ä¼šç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè¯·è°¨æ…æ“ä½œ
        - æ‰¹é‡æ›¿æ¢æ“ä½œä¼šä¿®æ”¹åŸæ–‡ä»¶ï¼Œå»ºè®®å…ˆåˆ›å»ºå¤‡ä»½
        - å»ºè®®å…ˆåœ¨å°èŒƒå›´æµ‹è¯•
        - æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼è¯­æ³•ï¼ˆåœ¨æœç´¢è¯è¯­ä¸­ï¼‰
        - å¤§å‹æ–‡ä»¶å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å¤„ç†
        - ä¿®æ”¹ä¿å­˜åå»ºè®®é‡æ–°æœç´¢æŸ¥çœ‹æœ€æ–°å†…å®¹
        """)


def multithreaded_search(search_tool, search_term, case_sensitive, match_whole_word, progress_bar, status_text):
    """
    ä½¿ç”¨å¤šçº¿ç¨‹æœç´¢Excelæ–‡ä»¶
    
    Args:
        search_tool: ExcelSearchReplaceå·¥å…·å®ä¾‹
        search_term: æœç´¢è¯
        case_sensitive: æ˜¯å¦å¤§å°å†™æ•æ„Ÿ
        match_whole_word: æ˜¯å¦å…¨è¯åŒ¹é…
        progress_bar: è¿›åº¦æ¡å¯¹è±¡
        status_text: çŠ¶æ€æ–‡æœ¬å¯¹è±¡
    
    Returns:
        æ€»åŒ¹é…æ•°
    """
    import openpyxl
    import re
    
    # æ¸…ç©ºä¹‹å‰çš„æœç´¢ç»“æœ
    search_tool.search_results = {}
    
    if not search_tool.excel_files:
        return 0
    
    # å‡†å¤‡æœç´¢æ¨¡å¼
    if match_whole_word:
        pattern = r'\b' + re.escape(search_term) + r'\b'
    else:
        pattern = re.escape(search_term)
    
    flags = 0 if case_sensitive else re.IGNORECASE
    regex = re.compile(pattern, flags)
    
    # çº¿ç¨‹é”ï¼Œç”¨äºå®‰å…¨æ›´æ–°å…±äº«æ•°æ®
    lock = threading.Lock()
    total_matches = 0
    completed_files = 0
    total_files = len(search_tool.excel_files)
    
    def search_single_file(file_path):
        """æœç´¢å•ä¸ªExcelæ–‡ä»¶"""
        nonlocal total_matches, completed_files
        
        file_matches = []
        match_count = 0
        
        try:
            wb = openpyxl.load_workbook(file_path, read_only=True, data_only=True)
            
            for sheet_name in wb.sheetnames:
                sheet = wb[sheet_name]
                
                for row_idx, row in enumerate(sheet.iter_rows(values_only=False), start=1):
                    for col_idx, cell in enumerate(row, start=1):
                        if cell.value is not None:
                            cell_text = str(cell.value)
                            
                            # æœç´¢åŒ¹é…
                            matches = list(regex.finditer(cell_text))
                            
                            if matches:
                                for match in matches:
                                    file_matches.append({
                                        'sheet_name': sheet_name,
                                        'row': row_idx,
                                        'column': col_idx,
                                        'original_text': cell_text,
                                        'matched_text': match.group(),
                                        'start_pos': match.start(),
                                        'end_pos': match.end()
                                    })
                                    match_count += 1
            
            wb.close()
            
            # ä½¿ç”¨é”æ›´æ–°å…±äº«æ•°æ®
            if file_matches:
                with lock:
                    search_tool.search_results[str(file_path)] = {
                        'matches': file_matches,
                        'match_count': match_count
                    }
                    total_matches += match_count
            
        except Exception as e:
            # å¿½ç•¥æ— æ³•è¯»å–çš„æ–‡ä»¶
            pass
        
        # æ›´æ–°è¿›åº¦
        with lock:
            completed_files += 1
            progress = completed_files / total_files
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨æœç´¢... {completed_files}/{total_files} ä¸ªæ–‡ä»¶")
        
        return match_count
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæœç´¢
    # æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®çº¿ç¨‹æ•°ï¼Œæœ€å¤§ä¸º16
    max_workers = min(16, (os.cpu_count() or 4) * 2)
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(search_single_file, file_path): file_path 
                  for file_path in search_tool.excel_files}
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
                pass
    
    return total_matches


def get_row_data_as_list(file_path, sheet_name, row_num):
    """
    è·å–æŒ‡å®šExcelæ–‡ä»¶ä¸­æŸä¸€è¡Œçš„å®Œæ•´æ•°æ®ï¼ˆä»¥åˆ—è¡¨å½¢å¼è¿”å›ï¼‰
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åç§°
        row_num: è¡Œå·
    
    Returns:
        è¯¥è¡Œæ‰€æœ‰åˆ—çš„æ•°æ®åˆ—è¡¨
    """
    try:
        # è¯»å–Excelæ–‡ä»¶çš„æŒ‡å®šå·¥ä½œè¡¨
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)
        
        # è·å–æŒ‡å®šè¡Œçš„æ•°æ®ï¼ˆæ³¨æ„ï¼šrow_numæ˜¯1-basedï¼Œéœ€è¦è½¬æ¢ä¸º0-basedï¼‰
        if row_num <= len(df):
            row_data = df.iloc[row_num - 1]
            # å°†è¯¥è¡Œæ•°æ®è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä¿ç•™NaNæ˜¾ç¤ºä¸ºç©ºå­—ç¬¦ä¸²
            return [str(val) if pd.notna(val) else "" for val in row_data]
        else:
            return ["(è¡Œå·è¶…å‡ºèŒƒå›´)"]
    except Exception as e:
        return [f"(è¯»å–å¤±è´¥: {str(e)})"]


def selective_replace(search_tool, search_term, replace_term, selected_replacements, backup_files):
    """
    æ‰§è¡Œé€‰æ‹©æ€§æ›¿æ¢ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰
    
    Args:
        search_tool: ExcelSearchReplaceå·¥å…·å®ä¾‹
        search_term: æœç´¢è¯
        replace_term: æ›¿æ¢è¯
        selected_replacements: é€‰ä¸­çš„æ›¿æ¢é¡¹å­—å…¸
        backup_files: æ˜¯å¦å¤‡ä»½æ–‡ä»¶
    
    Returns:
        (æ›¿æ¢çš„æ–‡ä»¶æ•°, æ€»æ›¿æ¢æ¬¡æ•°)
    """
    import openpyxl
    import re
    from datetime import datetime
    import shutil
    
    # çº¿ç¨‹é”
    lock = threading.Lock()
    replaced_files = 0
    total_replacements = 0
    
    def replace_single_file(file_path, selection):
        """æ›¿æ¢å•ä¸ªæ–‡ä»¶"""
        nonlocal replaced_files, total_replacements
        
        try:
            if not selection['selected']:
                return
            
            # åˆ›å»ºå¤‡ä»½
            if backup_files:
                backup_path = f"{file_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                shutil.copy2(file_path, backup_path)
            
            # åŠ è½½å·¥ä½œç°¿
            wb = openpyxl.load_workbook(file_path)
            file_replaced = False
            file_replacement_count = 0
            
            # è·å–è¯¥æ–‡ä»¶çš„åŒ¹é…é¡¹
            matches = search_tool.search_results[file_path]['matches']
            
            # å¦‚æœæ˜¯å…¨éƒ¨æ›¿æ¢æ¨¡å¼
            if selection['rows'] == 'all':
                for match in matches:
                    sheet = wb[match['sheet_name']]
                    cell = sheet.cell(row=match['row'], column=match['column'])
                    
                    if cell.value:
                        # æ‰§è¡Œæ›¿æ¢
                        new_value = str(cell.value).replace(search_term, replace_term)
                        cell.value = new_value
                        file_replacement_count += 1
                        file_replaced = True
            
            # å¦‚æœæ˜¯é€‰æ‹©ç‰¹å®šè¡Œæ¨¡å¼
            else:
                selected_rows = selection['selected_rows']
                for match in matches:
                    row_key = f"{file_path}_{match['sheet_name']}_{match['row']}"
                    if row_key in selected_rows:
                        sheet = wb[match['sheet_name']]
                        cell = sheet.cell(row=match['row'], column=match['column'])
                        
                        if cell.value:
                            # æ‰§è¡Œæ›¿æ¢
                            new_value = str(cell.value).replace(search_term, replace_term)
                            cell.value = new_value
                            file_replacement_count += 1
                            file_replaced = True
            
            # ä¿å­˜æ–‡ä»¶
            if file_replaced:
                wb.save(file_path)
                with lock:
                    replaced_files += 1
                    total_replacements += file_replacement_count
            
            wb.close()
        
        except Exception as e:
            st.error(f"æ›¿æ¢æ–‡ä»¶ {Path(file_path).name} æ—¶å‡ºé”™: {str(e)}")
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ›¿æ¢
    max_workers = min(8, (os.cpu_count() or 4))
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(replace_single_file, file_path, selection): file_path 
                  for file_path, selection in selected_replacements.items()}
        
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                pass
    
    return replaced_files, total_replacements


def open_folder(file_path):
    """
    æ‰“å¼€æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
    """
    try:
        folder_path = os.path.dirname(os.path.abspath(file_path))
        system = platform.system()
        
        if system == "Windows":
            # Windowsç³»ç»Ÿ
            os.startfile(folder_path)
        elif system == "Darwin":
            # macOSç³»ç»Ÿ
            subprocess.run(["open", folder_path])
        else:
            # Linuxç³»ç»Ÿ
            subprocess.run(["xdg-open", folder_path])
        
        st.success(f"âœ… å·²æ‰“å¼€æ–‡ä»¶å¤¹: {folder_path}")
    except Exception as e:
        st.error(f"âŒ æ‰“å¼€æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")


def open_file(file_path):
    """
    æ‰“å¼€Excelæ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
    """
    try:
        system = platform.system()
        
        if system == "Windows":
            # Windowsç³»ç»Ÿ
            os.startfile(file_path)
        elif system == "Darwin":
            # macOSç³»ç»Ÿ
            subprocess.run(["open", file_path])
        else:
            # Linuxç³»ç»Ÿ
            subprocess.run(["xdg-open", file_path])
        
        st.success(f"âœ… å·²æ‰“å¼€æ–‡ä»¶: {Path(file_path).name}")
    except Exception as e:
        st.error(f"âŒ æ‰“å¼€æ–‡ä»¶å¤±è´¥: {str(e)}")

# é¡µé¢3: ç¿»è¯‘ç»“æœå¤„ç†
def translation_result_processor_page():
    st.title("ğŸ“Š AIç¿»è¯‘ç»“æœå¤„ç†å·¥å…·")
    st.markdown("### å°†AIè¿”å›çš„ç¿»è¯‘ç»“æœä¸åŸå§‹Excelæ–‡ä»¶è¿›è¡ŒåŒ¹é…ï¼Œç”ŸæˆåŒ…å«ç¿»è¯‘ç»“æœçš„Excelæ–‡ä»¶ã€‚")
    
    if 'result_translator' not in st.session_state:
        st.session_state.result_translator = MultiAPIExcelTranslator(
            api_key="", 
            api_provider="DeepSeek", 
            api_url=get_api_providers()["DeepSeek"]["url"], 
            model="deepseek-chat"
        )
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.header("ğŸ“ åŸå§‹Excelæ–‡ä»¶")
        
        original_excel_file = st.file_uploader(
            "ğŸ“„ ä¸Šä¼ åŸå§‹Excelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="result_original_excel"
        )
        
        original_df = None
        text_col = None
        
        if original_excel_file is not None:
            try:
                original_df = pd.read_excel(original_excel_file)
                # æ¸…ç†åˆ—åä¸­çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦
                original_df.columns = original_df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.result_original_df = original_df
                st.success(f"âœ… æˆåŠŸè¯»å–åŸå§‹æ–‡ä»¶ï¼Œå…± {len(original_df)} è¡Œæ•°æ®")
                
                with st.expander("ğŸ“Š åŸå§‹æ–‡ä»¶é¢„è§ˆ"):
                    st.dataframe(original_df.head(10))
                
                cols = original_df.columns.tolist()
                text_col = st.selectbox(
                    "ğŸ“ é€‰æ‹©æ–‡æœ¬åˆ—ï¼ˆä¸ç”Ÿæˆæç¤ºè¯æ—¶é€‰æ‹©çš„åˆ—ç›¸åŒï¼‰",
                    options=cols,
                    index=0,
                    key="result_text_col_select"
                )
                
                st.session_state.result_text_col = text_col
                
            except Exception as e:
                st.error(f"âŒ åŸå§‹æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    with col2:
        st.header("ğŸ“ AIç¿»è¯‘ç»“æœ")
        
        st.subheader("ğŸ“‹ è¾“å…¥AIç¿»è¯‘ç»“æœ")
        ai_result_text = st.text_area(
            "AIç¿»è¯‘ç»“æœæ–‡æœ¬",
            value=st.session_state.get('result_ai_result_text', ''),
            height=300,
            placeholder="""è¯·ç²˜è´´AIè¿”å›çš„ç¿»è¯‘ç»“æœæ–‡æœ¬ï¼Œæ”¯æŒå¤šç§è¡¨æ ¼æ ¼å¼ï¼š

æ ¼å¼1ï¼ˆæ ‡å‡†Markdownï¼‰ï¼š
| åŸæ–‡ï¼ˆä¸­æ–‡ï¼‰ | è‹±æ–‡ç¿»è¯‘ |
|-------------|----------|
| å–‚ï¼ä½ ä»¬å¹²ä»€ä¹ˆï¼ | Hey! What are you doing! |

æ ¼å¼2ï¼ˆç®€åŒ–æ ¼å¼ï¼‰ï¼š
| åŸæ–‡ï¼ˆä¸­æ–‡ï¼‰ | è‹±æ–‡ Translation |
| å–‚ï¼ä½ ä»¬å¹²ä»€ä¹ˆï¼ | Hey! What are you doing! |

æ ¼å¼3ï¼ˆæ— è¡¨å¤´ï¼‰ï¼š
| å–‚ï¼ä½ ä»¬å¹²ä»€ä¹ˆï¼ | Hey! What are you doing! |
| çªçªçªâ€”â€”å—¡ï¼â€”â€” | Vroom vroomâ€”Broom!â€” |""",
            key="result_ai_result_text_area"
        )
        st.session_state.result_ai_result_text = ai_result_text
        
        st.subheader("ğŸŒ ç›®æ ‡è¯­è¨€")
        target_language = st.selectbox(
            "é€‰æ‹©ç›®æ ‡è¯­è¨€",
            options=get_preset_languages(),
            index=0,
            key="result_target_language_select"
        )
        
        if target_language == "è‡ªå®šä¹‰":
            custom_language = st.text_input(
                "è¾“å…¥è‡ªå®šä¹‰è¯­è¨€",
                value=st.session_state.get('result_custom_language', ''),
                placeholder="ä¾‹å¦‚ï¼šä¿„æ–‡ã€è‘¡è„ç‰™æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ç­‰",
                key="result_custom_language_input"
            )
            target_language = custom_language if custom_language else target_language
        
        st.session_state.result_target_language = target_language
        st.info(f"ğŸ¯ ç›®æ ‡è¯­è¨€: {target_language}")
    
    if st.button("ğŸš€ å¤„ç†ç¿»è¯‘ç»“æœ", key="process_results_btn", use_container_width=True):
        if original_df is None or text_col is None:
            st.error("âŒ è¯·å…ˆä¸Šä¼ åŸå§‹Excelæ–‡ä»¶å¹¶é€‰æ‹©æ–‡æœ¬åˆ—ã€‚")
            return
        
        if not ai_result_text:
            st.error("âŒ è¯·è¾“å…¥AIç¿»è¯‘ç»“æœæ–‡æœ¬ã€‚")
            return
        
        if not target_language:
            st.error("âŒ è¯·å…ˆé€‰æ‹©ç›®æ ‡è¯­è¨€ã€‚")
            return
        
        with st.spinner("æ­£åœ¨è§£æAIç¿»è¯‘ç»“æœ..."):
            translations = parse_ai_translation_result(ai_result_text)
        
        if not translations:
            st.error("âŒ æ— æ³•è§£æAIç¿»è¯‘ç»“æœï¼Œè¯·æ£€æŸ¥æ–‡æœ¬æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
            return
        
        st.success(f"âœ… æˆåŠŸè§£æ {len(translations)} æ¡ç¿»è¯‘ç»“æœ")
        
        with st.expander("ğŸ“Š è§£æç»“æœé¢„è§ˆ"):
            if translations:
                preview_data = []
                for i, (original, translation) in enumerate(list(translations.items())[:10]):
                    preview_data.append({
                        "åºå·": i + 1,
                        "åŸæ–‡": original,
                        "ç¿»è¯‘ç»“æœ": translation
                    })
                preview_df = pd.DataFrame(preview_data)
                st.dataframe(preview_df)
            else:
                st.warning("æœªè§£æåˆ°ä»»ä½•ç¿»è¯‘ç»“æœ")
        
        with st.spinner("æ­£åœ¨åˆå¹¶ç¿»è¯‘ç»“æœ..."):
            result_df, matched_count, unmatched_texts = merge_translations_with_excel(
                original_df, text_col, translations, target_language
            )
        
        st.success(f"âœ… æˆåŠŸåŒ¹é… {matched_count}/{len(original_df)} æ¡è®°å½•")
        
        if unmatched_texts:
            st.warning(f"âš ï¸ æœ‰ {len(unmatched_texts)} æ¡è®°å½•æœªèƒ½åŒ¹é…")
        
        with st.expander("ğŸ“Š åˆå¹¶ç»“æœé¢„è§ˆ"):
            st.dataframe(result_df.head(10))
        
        st.session_state.result_df = result_df
        st.session_state.matched_count = matched_count
        st.session_state.unmatched_count = len(unmatched_texts)
        st.session_state.original_filename = original_excel_file.name if original_excel_file else "translation_results"
        
        st.success("âœ… ç¿»è¯‘ç»“æœå¤„ç†å®Œæˆï¼")
    
    if st.session_state.get('result_df') is not None:
        result_df = st.session_state.result_df
        matched_count = st.session_state.matched_count
        unmatched_count = st.session_state.unmatched_count
        target_language = st.session_state.get('result_target_language', 'è‹±æ–‡')
        original_filename = st.session_state.get('original_filename', 'translation_results')
        
        st.header("ğŸ“¥ ä¸‹è½½ç»“æœ")
        
        if original_filename and original_filename != "translation_results":
            original_name = Path(original_filename).stem
            output_filename = f"{original_name}_{target_language}ori.xlsx"
        else:
            output_filename = f"translation_results_{target_language}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        excel_buffer = io.BytesIO()
        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            result_df.to_excel(writer, index=False, sheet_name='ç¿»è¯‘ç»“æœ')
        
        excel_buffer.seek(0)
        
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½ç¿»è¯‘ç»“æœExcelæ–‡ä»¶ ({matched_count}/{len(result_df)} æ¡åŒ¹é…)",
            data=excel_buffer,
            file_name=output_filename,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        
        st.info(f"**ç»Ÿè®¡ä¿¡æ¯ï¼š** æ€»è®°å½•æ•°: {len(result_df)} | æˆåŠŸåŒ¹é…: {matched_count} | æœªåŒ¹é…: {unmatched_count} | åŒ¹é…ç‡: {matched_count/len(result_df)*100:.1f}%")
def find_excel_files(folder_path):
    """æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰Excelæ–‡ä»¶"""
    excel_files = []
    folder_path = Path(folder_path)
    
    if not folder_path.exists():
        return False, "æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨"
    
    # æ”¯æŒçš„Excelæ–‡ä»¶æ‰©å±•å
    excel_extensions = ['*.xlsx', '*.xls', '*.xlsm', '*.xlsb']
    
    for ext in excel_extensions:
        excel_files.extend(folder_path.rglob(ext))
    
    return True, excel_files


def excel_comparison_page():
    st.title("ğŸ” Excelè¡¨æ ¼å¯¹æ¯”å·¥å…·")
    st.markdown("### æ¯”è¾ƒä¸¤ä¸ªç›¸ä¼¼Excelè¡¨æ ¼ï¼Œæ‰¾å‡ºå·®å¼‚å’Œæ”¹åŠ¨")

    st.info("ğŸ’¡ æ­¤åŠŸèƒ½é€‚ç”¨äºæ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬ç›¸ä¼¼çš„Excelæ–‡ä»¶ï¼Œæ‰¾å‡ºè¢«ä¿®æ”¹çš„å†…å®¹")

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ“„ åŸå§‹è¡¨æ ¼ (ç‰ˆæœ¬A)")
        file_a = st.file_uploader(
            "ä¸Šä¼ åŸå§‹Excelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="comparison_file_a"
        )

        if file_a is not None:
            try:
                df_a = pd.read_excel(file_a)
                st.success(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶A: {len(df_a)} è¡Œ, {len(df_a.columns)} åˆ—")

                with st.expander("ğŸ“Š æ–‡ä»¶Aé¢„è§ˆ"):
                    st.dataframe(df_a.head(10))

            except Exception as e:
                st.error(f"âŒ è¯»å–æ–‡ä»¶Aå¤±è´¥: {e}")

    with col2:
        st.subheader("ğŸ“„ ä¿®æ”¹åè¡¨æ ¼ (ç‰ˆæœ¬B)")
        file_b = st.file_uploader(
            "ä¸Šä¼ ä¿®æ”¹åçš„Excelæ–‡ä»¶",
            type=['xlsx', 'xls'],
            key="comparison_file_b"
        )

        if file_b is not None:
            try:
                df_b = pd.read_excel(file_b)
                st.success(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶B: {len(df_b)} è¡Œ, {len(df_b.columns)} åˆ—")

                with st.expander("ğŸ“Š æ–‡ä»¶Bé¢„è§ˆ"):
                    st.dataframe(df_b.head(10))

            except Exception as e:
                st.error(f"âŒ è¯»å–æ–‡ä»¶Bå¤±è´¥: {e}")

    # æ¯”è¾ƒè®¾ç½®
    st.markdown("---")
    st.subheader("âš™ï¸ æ¯”è¾ƒè®¾ç½®")

    col1, col2, col3 = st.columns(3)

    with col1:
        # é€‰æ‹©å…³é”®åˆ—ï¼ˆç”¨äºè¡ŒåŒ¹é…ï¼‰
        key_column = st.text_input(
            "å…³é”®åˆ—åï¼ˆç”¨äºè¡ŒåŒ¹é…ï¼‰:",
            placeholder="ä¾‹å¦‚: IDã€åºå·ç­‰",
            help="ç”¨äºåŒ¹é…ä¸¤ä¸ªè¡¨æ ¼ä¸­å¯¹åº”è¡Œçš„åˆ—åï¼Œç•™ç©ºåˆ™æŒ‰è¡Œå·åŒ¹é…"
        )

    with col2:
        # æ¯”è¾ƒæ¨¡å¼
        compare_mode = st.selectbox(
            "æ¯”è¾ƒæ¨¡å¼:",
            options=["ç²¾ç¡®åŒ¹é…", "æ¨¡ç³ŠåŒ¹é…", "ä»…æ¯”è¾ƒæ–‡æœ¬å†…å®¹"],
            index=0,
            help="ç²¾ç¡®åŒ¹é…ï¼šå®Œå…¨ä¸€è‡´ï¼›æ¨¡ç³ŠåŒ¹é…ï¼šå…è®¸å¾®å°å·®å¼‚ï¼›ä»…æ¯”è¾ƒæ–‡æœ¬ï¼šå¿½ç•¥æ ¼å¼"
        )

    with col3:
        # æ•æ„Ÿåº¦è®¾ç½®
        sensitivity = st.slider(
            "å·®å¼‚æ•æ„Ÿåº¦:",
            min_value=1,
            max_value=10,
            value=5,
            help="æ•°å€¼è¶Šé«˜ï¼Œå¯¹å¾®å°å·®å¼‚è¶Šæ•æ„Ÿ"
        )

    # é«˜çº§é€‰é¡¹
    with st.expander("ğŸ”§ é«˜çº§é€‰é¡¹"):
        col1, col2 = st.columns(2)

        with col1:
            ignore_case = st.checkbox("å¿½ç•¥å¤§å°å†™", value=True)
            ignore_whitespace = st.checkbox("å¿½ç•¥ç©ºç™½å­—ç¬¦", value=True)
            show_unchanged = st.checkbox("æ˜¾ç¤ºæœªæ›´æ”¹çš„è¡Œ", value=False)

        with col2:
            highlight_changes = st.checkbox("é«˜äº®æ˜¾ç¤ºæ›´æ”¹", value=True)
            include_additions = st.checkbox("æ£€æµ‹æ–°å¢è¡Œ", value=True)
            include_deletions = st.checkbox("æ£€æµ‹åˆ é™¤è¡Œ", value=True)

    # æ‰§è¡Œæ¯”è¾ƒ
    if st.button("ğŸš€ å¼€å§‹æ¯”è¾ƒ", type="primary", use_container_width=True):
        if file_a is None or file_b is None:
            st.error("âŒ è¯·å…ˆä¸Šä¼ ä¸¤ä¸ªExcelæ–‡ä»¶")
            return

        try:
            # è¯»å–æ•°æ®
            df_a = pd.read_excel(file_a)
            df_b = pd.read_excel(file_b)

            # æ‰§è¡Œæ¯”è¾ƒ
            with st.spinner("ğŸ” æ­£åœ¨æ¯”è¾ƒä¸¤ä¸ªè¡¨æ ¼..."):
                comparison_results = compare_dataframes_simple(
                    df_a, df_b, key_column, compare_mode, sensitivity,
                    ignore_case, ignore_whitespace, include_additions, include_deletions
                )

            # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
            display_comparison_results_simple(
                comparison_results, highlight_changes, show_unchanged
            )

        except Exception as e:
            st.error(f"âŒ æ¯”è¾ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            st.error(traceback.format_exc())


def compare_dataframes_simple(df_a, df_b, key_column=None, compare_mode="ç²¾ç¡®åŒ¹é…",
                              sensitivity=5, ignore_case=True, ignore_whitespace=True,
                              include_additions=True, include_deletions=True):
    """
    ç®€åŒ–çš„DataFrameæ¯”è¾ƒå‡½æ•°
    """
    results = {
        'added_rows': [],
        'deleted_rows': [],
        'modified_rows': [],
        'modified_cells': [],
        'summary': {
            'total_rows_a': len(df_a),
            'total_rows_b': len(df_b),
            'added_count': 0,
            'deleted_count': 0,
            'modified_count': 0,
            'similarity_score': 0
        }
    }

    # é¢„å¤„ç†æ•°æ®
    df_a_clean = preprocess_dataframe_simple(df_a, ignore_case, ignore_whitespace)
    df_b_clean = preprocess_dataframe_simple(df_b, ignore_case, ignore_whitespace)

    # å¦‚æœæœ‰å…³é”®åˆ—ï¼Œä½¿ç”¨å…³é”®åˆ—è¿›è¡Œè¡ŒåŒ¹é…
    if key_column and key_column in df_a.columns and key_column in df_b.columns:
        # ä½¿ç”¨å…³é”®åˆ—åŒ¹é…è¡Œ
        a_keys = df_a[key_column].astype(str).tolist()
        b_keys = df_b[key_column].astype(str).tolist()

        # æ‰¾å‡ºæ–°å¢å’Œåˆ é™¤çš„è¡Œ
        if include_additions:
            for i, key in enumerate(b_keys):
                if key not in a_keys:
                    results['added_rows'].append({
                        'key': key,
                        'row_index_b': i,
                        'row_data': df_b.iloc[i].to_dict()
                    })

        if include_deletions:
            for i, key in enumerate(a_keys):
                if key not in b_keys:
                    results['deleted_rows'].append({
                        'key': key,
                        'row_index_a': i,
                        'row_data': df_a.iloc[i].to_dict()
                    })

        # æ¯”è¾ƒå…±åŒçš„è¡Œ
        common_keys = set(a_keys) & set(b_keys)
        for key in common_keys:
            idx_a = a_keys.index(key)
            idx_b = b_keys.index(key)

            row_a = df_a_clean.iloc[idx_a]
            row_b = df_b_clean.iloc[idx_b]

            # æ¯”è¾ƒè¡Œå†…å®¹
            changes = compare_rows_simple(row_a, row_b, df_a.columns.tolist(),
                                          compare_mode, sensitivity)

            if changes:
                results['modified_rows'].append({
                    'key': key,
                    'row_index_a': idx_a,
                    'row_index_b': idx_b,
                    'row_data_a': df_a.iloc[idx_a].to_dict(),
                    'row_data_b': df_b.iloc[idx_b].to_dict(),
                    'changes': changes,
                    'change_count': len(changes)
                })

                # è®°å½•ä¿®æ”¹çš„å•å…ƒæ ¼
                for change in changes:
                    results['modified_cells'].append({
                        'key': key,
                        'row_index_a': idx_a,
                        'row_index_b': idx_b,
                        'column': change['column'],
                        'value_a': change['value_a'],
                        'value_b': change['value_b'],
                        'change_type': change['change_type']
                    })

    else:
        # æ²¡æœ‰å…³é”®åˆ—ï¼ŒæŒ‰è¡Œå·åŒ¹é…
        max_rows = min(len(df_a), len(df_b))

        for i in range(max_rows):
            row_a = df_a_clean.iloc[i]
            row_b = df_b_clean.iloc[i]

            # æ¯”è¾ƒè¡Œå†…å®¹
            changes = compare_rows_simple(row_a, row_b, df_a.columns.tolist(),
                                          compare_mode, sensitivity)

            if changes:
                results['modified_rows'].append({
                    'key': f"è¡Œ{i + 1}",
                    'row_index_a': i,
                    'row_index_b': i,
                    'row_data_a': df_a.iloc[i].to_dict(),
                    'row_data_b': df_b.iloc[i].to_dict(),
                    'changes': changes,
                    'change_count': len(changes)
                })

                for change in changes:
                    results['modified_cells'].append({
                        'key': f"è¡Œ{i + 1}",
                        'row_index_a': i,
                        'row_index_b': i,
                        'column': change['column'],
                        'value_a': change['value_a'],
                        'value_b': change['value_b'],
                        'change_type': change['change_type']
                    })

        # å¤„ç†æ–°å¢/åˆ é™¤çš„è¡Œï¼ˆæŒ‰è¡Œå·ï¼‰
        if include_additions and len(df_b) > len(df_a):
            for i in range(len(df_a), len(df_b)):
                results['added_rows'].append({
                    'key': f"æ–°å¢è¡Œ{i + 1}",
                    'row_index_b': i,
                    'row_data': df_b.iloc[i].to_dict()
                })

        if include_deletions and len(df_a) > len(df_b):
            for i in range(len(df_b), len(df_a)):
                results['deleted_rows'].append({
                    'key': f"åˆ é™¤è¡Œ{i + 1}",
                    'row_index_a': i,
                    'row_data': df_a.iloc[i].to_dict()
                })

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    results['summary']['added_count'] = len(results['added_rows'])
    results['summary']['deleted_count'] = len(results['deleted_rows'])
    results['summary']['modified_count'] = len(results['modified_rows'])

    # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
    total_cells = results['summary']['total_rows_a'] * len(df_a.columns) if len(df_a.columns) > 0 else 0
    if total_cells > 0:
        changed_cells = len(results['modified_cells'])
        similarity = 1 - (changed_cells / total_cells)
        results['summary']['similarity_score'] = round(similarity * 100, 2)

    return results


def preprocess_dataframe_simple(df, ignore_case=True, ignore_whitespace=True):
    """
    ç®€åŒ–çš„DataFrameé¢„å¤„ç†å‡½æ•°
    """
    df_clean = df.copy()

    # å¤„ç†NaNå€¼
    df_clean = df_clean.fillna('')

    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹è¿›è¡Œæ¯”è¾ƒ
    for col in df_clean.columns:
        df_clean[col] = df_clean[col].astype(str)

        if ignore_case:
            df_clean[col] = df_clean[col].str.lower()

        if ignore_whitespace:
            df_clean[col] = df_clean[col].str.strip()
            df_clean[col] = df_clean[col].str.replace(r'\s+', ' ', regex=True)

    return df_clean


def compare_rows_simple(row_a, row_b, columns, compare_mode="ç²¾ç¡®åŒ¹é…", sensitivity=5):
    """
    ç®€åŒ–çš„è¡Œæ¯”è¾ƒå‡½æ•°
    """
    changes = []

    for col in columns:
        val_a = str(row_a[col]) if pd.notna(row_a[col]) else ""
        val_b = str(row_b[col]) if pd.notna(row_b[col]) else ""

        # ç©ºå€¼å¤„ç†
        if val_a == "" and val_b == "":
            continue

        change_type = "æœªå˜åŒ–"

        if compare_mode == "ç²¾ç¡®åŒ¹é…":
            if val_a != val_b:
                change_type = "ä¿®æ”¹"
        elif compare_mode == "æ¨¡ç³ŠåŒ¹é…":
            similarity = calculate_similarity(val_a, val_b)
            threshold = sensitivity / 10.0  # å°†æ•æ„Ÿåº¦è½¬æ¢ä¸º0-1çš„é˜ˆå€¼
            if similarity < threshold:
                change_type = "ä¿®æ”¹"
        elif compare_mode == "ä»…æ¯”è¾ƒæ–‡æœ¬å†…å®¹":
            # ç§»é™¤æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªæ¯”è¾ƒæ–‡æœ¬å†…å®¹
            text_a = re.sub(r'[^a-zA-Z\u4e00-\u9fa5]', '', val_a)
            text_b = re.sub(r'[^a-zA-Z\u4e00-\u9fa5]', '', val_b)
            if text_a != text_b:
                change_type = "ä¿®æ”¹"

        if change_type == "ä¿®æ”¹":
            changes.append({
                'column': col,
                'value_a': val_a,
                'value_b': val_b,
                'change_type': change_type,
                'similarity': calculate_similarity(val_a, val_b) if compare_mode == "æ¨¡ç³ŠåŒ¹é…" else None
            })

    return changes


def calculate_similarity(str1, str2):
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
    """
    if not str1 and not str2:
        return 1.0
    if not str1 or not str2:
        return 0.0

    return difflib.SequenceMatcher(None, str1, str2).ratio()


def display_comparison_results_simple(results, highlight_changes=True, show_unchanged=False):
    """
    ç®€åŒ–çš„æ¯”è¾ƒç»“æœæ˜¾ç¤ºå‡½æ•°ï¼Œé¿å…ä½¿ç”¨å›¾è¡¨åŠŸèƒ½
    """
    st.markdown("---")
    st.header("ğŸ“Š æ¯”è¾ƒç»“æœ")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    summary = results['summary']
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("æ–‡ä»¶Aè¡Œæ•°", summary['total_rows_a'])
    with col2:
        st.metric("æ–‡ä»¶Bè¡Œæ•°", summary['total_rows_b'])
    with col3:
        st.metric("ä¿®æ”¹è¡Œæ•°", summary['modified_count'])
    with col4:
        st.metric("ç›¸ä¼¼åº¦", f"{summary['similarity_score']}%")

    # æ˜¾ç¤ºæ–°å¢è¡Œ
    if results['added_rows']:
        st.subheader("ğŸ†• æ–°å¢è¡Œ")
        added_data = []
        for row in results['added_rows']:
            row_data = {'å…³é”®å€¼': row['key'], 'è¡Œå·(B)': row['row_index_b'] + 1}
            row_data.update(row['row_data'])
            added_data.append(row_data)

        added_df = pd.DataFrame(added_data)
        st.dataframe(added_df, use_container_width=True)

    # æ˜¾ç¤ºåˆ é™¤è¡Œ
    if results['deleted_rows']:
        st.subheader("ğŸ—‘ï¸ åˆ é™¤è¡Œ")
        deleted_data = []
        for row in results['deleted_rows']:
            row_data = {'å…³é”®å€¼': row['key'], 'è¡Œå·(A)': row['row_index_a'] + 1}
            row_data.update(row['row_data'])
            deleted_data.append(row_data)

        deleted_df = pd.DataFrame(deleted_data)
        st.dataframe(deleted_df, use_container_width=True)

    # æ˜¾ç¤ºä¿®æ”¹çš„è¡Œ
    if results['modified_rows']:
        st.subheader("âœï¸ ä¿®æ”¹çš„è¡Œ")

        for mod_row in results['modified_rows']:
            with st.expander(f"ğŸ” {mod_row['key']} - {mod_row['change_count']} å¤„ä¿®æ”¹", expanded=True):
                # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
                comparison_data = []

                # è·å–æ‰€æœ‰åˆ—
                all_columns = set(mod_row['row_data_a'].keys()) | set(mod_row['row_data_b'].keys())

                for col in sorted(all_columns):
                    val_a = mod_row['row_data_a'].get(col, '')
                    val_b = mod_row['row_data_b'].get(col, '')

                    # æ£€æŸ¥æ­¤åˆ—æ˜¯å¦æœ‰ä¿®æ”¹
                    is_changed = False
                    for change in mod_row['changes']:
                        if change['column'] == col:
                            is_changed = True
                            break

                    if is_changed or show_unchanged:
                        # å‡†å¤‡æ˜¾ç¤ºå€¼ï¼ˆé«˜äº®ä¿®æ”¹ï¼‰
                        if highlight_changes and is_changed:
                            display_a = f"**{val_a}**" if val_a else ""
                            display_b = f"**{val_b}**" if val_b else ""
                        else:
                            display_a = val_a
                            display_b = val_b

                        comparison_data.append({
                            'åˆ—å': col,
                            'æ–‡ä»¶Aå€¼': display_a,
                            'æ–‡ä»¶Bå€¼': display_b,
                            'çŠ¶æ€': 'âœ… æœªä¿®æ”¹' if not is_changed else 'âŒ å·²ä¿®æ”¹'
                        })

                # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
                comp_df = pd.DataFrame(comparison_data)
                st.dataframe(comp_df, use_container_width=True)

                # æ˜¾ç¤ºè¯¦ç»†ä¿®æ”¹
                st.write("**è¯¦ç»†ä¿®æ”¹:**")
                for change in mod_row['changes']:
                    col1, col2 = st.columns(2)
                    with col1:
                        st.text_area(
                            f"æ–‡ä»¶A - {change['column']}",
                            value=change['value_a'],
                            height=50,
                            key=f"a_{mod_row['key']}_{change['column']}"
                        )
                    with col2:
                        st.text_area(
                            f"æ–‡ä»¶B - {change['column']}",
                            value=change['value_b'],
                            height=50,
                            key=f"b_{mod_row['key']}_{change['column']}"
                        )

    # æ˜¾ç¤ºä¿®æ”¹ç»Ÿè®¡ï¼ˆä½¿ç”¨è¡¨æ ¼è€Œä¸æ˜¯å›¾è¡¨ï¼‰
    if results['modified_cells']:
        st.subheader("ğŸ“ˆ ä¿®æ”¹ç»Ÿè®¡")

        # æŒ‰åˆ—ç»Ÿè®¡ä¿®æ”¹æ¬¡æ•°
        col_changes = {}
        for cell in results['modified_cells']:
            col = cell['column']
            if col not in col_changes:
                col_changes[col] = 0
            col_changes[col] += 1

        # ä½¿ç”¨è¡¨æ ¼æ˜¾ç¤ºä¿®æ”¹ç»Ÿè®¡ï¼Œé¿å…ä½¿ç”¨å›¾è¡¨
        if col_changes:
            stat_data = []
            for col, count in col_changes.items():
                stat_data.append({
                    'åˆ—å': col,
                    'ä¿®æ”¹æ¬¡æ•°': count
                })

            stat_df = pd.DataFrame(stat_data).sort_values('ä¿®æ”¹æ¬¡æ•°', ascending=False)
            st.dataframe(stat_df, use_container_width=True)

            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºä¿®æ”¹æ¯”ä¾‹
            st.write("**ä¿®æ”¹æ¯”ä¾‹:**")
            for _, row in stat_df.iterrows():
                col_name = row['åˆ—å']
                count = row['ä¿®æ”¹æ¬¡æ•°']
                max_count = max(col_changes.values())
                percentage = (count / max_count) * 100

                st.write(f"{col_name}: {count} æ¬¡ä¿®æ”¹")
                st.progress(int(percentage))

    # æä¾›ç»“æœä¸‹è½½
    st.subheader("ğŸ’¾ ä¸‹è½½æ¯”è¾ƒç»“æœ")

    # å‡†å¤‡ä¸‹è½½æ•°æ®
    download_data = []
    for cell in results['modified_cells']:
        download_data.append({
            'å…³é”®å€¼': cell['key'],
            'è¡Œå·(A)': cell.get('row_index_a', '') + 1,
            'è¡Œå·(B)': cell.get('row_index_b', '') + 1,
            'åˆ—å': cell['column'],
            'æ–‡ä»¶Aå€¼': cell['value_a'],
            'æ–‡ä»¶Bå€¼': cell['value_b'],
            'ä¿®æ”¹ç±»å‹': cell['change_type']
        })

    if download_data:
        download_df = pd.DataFrame(download_data)
        csv_data = download_df.to_csv(index=False).encode('utf-8-sig')

        st.download_button(
            label="ğŸ“¥ ä¸‹è½½å·®å¼‚æŠ¥å‘Š(CSV)",
            data=csv_data,
            file_name=f"excel_comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )
    else:
        st.info("ğŸ“ æ²¡æœ‰æ£€æµ‹åˆ°å·®å¼‚ï¼Œæ— éœ€ä¸‹è½½æŠ¥å‘Š")
# å…¨å±€é”ç”¨äºçº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ›´æ–°
progress_lock = Lock()

def similar(a, b):
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦"""
    return SequenceMatcher(None, str(a), str(b)).ratio()

def load_single_file(file_path):
    """åŠ è½½å•ä¸ªæ–‡ä»¶ï¼ˆExcelæˆ–CSVï¼‰"""
    try:
        if file_path.suffix.lower() in ['.xlsx', '.xls', '.xlsm']:
            # è¯»å–Excelæ–‡ä»¶çš„æ‰€æœ‰sheet
            excel_file = pd.read_excel(file_path, sheet_name=None)
            results = {}
            for sheet_name, df in excel_file.items():
                if not df.empty:
                    key = f"{file_path.name} - {sheet_name}"
                    results[key] = {
                        'dataframe': df,
                        'file_path': file_path,
                        'sheet_name': sheet_name,
                        'file_type': 'excel'
                    }
            return results
        elif file_path.suffix.lower() == '.csv':
            # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼è¯»å–CSV
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is not None and not df.empty:
                return {
                    file_path.name: {
                        'dataframe': df,
                        'file_path': file_path,
                        'sheet_name': 'CSV',
                        'file_type': 'csv'
                    }
                }
    except Exception as e:
        st.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
    
    return {}

def load_all_files_parallel(folder_path, max_workers=4):
    """å¹¶è¡ŒåŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰Excelå’ŒCSVæ–‡ä»¶"""
    all_files = {}
    folder_path = Path(folder_path)
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶è·¯å¾„
    file_paths = []
    for pattern in ['*.xlsx', '*.xls', '*.xlsm', '*.csv']:
        file_paths.extend(folder_path.rglob(pattern))
    
    if not file_paths:
        return all_files
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŠ è½½æ–‡ä»¶
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_path = {executor.submit(load_single_file, path): path for path in file_paths}
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_path):
            try:
                result = future.result()
                all_files.update(result)
            except Exception as e:
                path = future_to_path[future]
                st.warning(f"å¤„ç†æ–‡ä»¶ {path} æ—¶å‡ºé”™: {str(e)}")
    
    return all_files

def load_source_files_parallel(folder_path, max_workers=4):
    """å¹¶è¡ŒåŠ è½½æºæ–‡ä»¶å¤¹ä¸­çš„Excelå’ŒCSVæ–‡ä»¶"""
    source_files = {}
    folder_path = Path(folder_path)
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶è·¯å¾„
    file_paths = []
    for pattern in ['*.xlsx', '*.xls', '*.xlsm', '*.csv']:
        file_paths.extend(folder_path.glob(pattern))
    
    if not file_paths:
        return source_files
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒåŠ è½½æ–‡ä»¶
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_path = {executor.submit(load_single_file, path): path for path in file_paths}
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_path):
            try:
                result = future.result()
                source_files.update(result)
            except Exception as e:
                path = future_to_path[future]
                st.error(f"å¤„ç†æ–‡ä»¶ {path} æ—¶å‡ºé”™: {str(e)}")
    
    return source_files

def find_matching_text(search_text, files_dict, source_col, target_col, match_strategy, similarity_threshold):
    """åœ¨æ–‡ä»¶å­—å…¸ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æœ¬"""
    if pd.isna(search_text) or search_text == '':
        return None, None, 0
        
    search_text = str(search_text).strip()
    
    best_match = None
    best_similarity = 0
    best_source = None
    
    for file_info in files_dict.values():
        df = file_info['dataframe']
        
        # æ£€æŸ¥æ‰€éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
        if source_col in df.columns and target_col in df.columns:
            # æ ¹æ®åŒ¹é…ç­–ç•¥è¿›è¡ŒåŒ¹é…
            if match_strategy == "ç²¾ç¡®åŒ¹é…":
                # ç²¾ç¡®åŒ¹é…
                matches = df[df[source_col].astype(str).str.strip() == search_text]
                if not matches.empty:
                    return matches[target_col].iloc[0], matches[source_col].iloc[0], 1.0
            else:
                # æ¨¡ç³ŠåŒ¹é…
                for idx, row in df.iterrows():
                    source_text = str(row[source_col])
                    if pd.isna(source_text) or source_text == '':
                        continue
                    
                    if match_strategy == "åŒ…å«åŒ¹é…":
                        # åŒ…å«åŒ¹é…
                        if search_text in source_text or source_text in search_text:
                            similarity = similar(search_text, source_text)
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_match = row[target_col]
                                best_source = source_text
                    else:  # ç›¸ä¼¼åº¦åŒ¹é…
                        # è®¡ç®—ç›¸ä¼¼åº¦
                        similarity = similar(search_text, source_text)
                        if similarity > best_similarity and similarity >= similarity_threshold:
                            best_similarity = similarity
                            best_match = row[target_col]
                            best_source = source_text
    
    # å¯¹äºæ¨¡ç³ŠåŒ¹é…ï¼Œè¿”å›æœ€ä½³åŒ¹é…ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
    if match_strategy != "ç²¾ç¡®åŒ¹é…" and best_match is not None:
        return best_match, best_source, best_similarity
    
    return None, None, 0

def process_single_row(args):
    """å¤„ç†å•è¡Œæ•°æ®çš„åŒ¹é…"""
    index, row, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled = args
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å·²å¡«å……çš„è¡Œ
    if skip_filled and not pd.isna(row.get(folder1_fill_col, None)) and str(row[folder1_fill_col]).strip() != '':
        return index, None, None, None, 0, "è·³è¿‡å·²å¡«å……"
    
    search_text = row[folder1_match_col]
    
    # è·³è¿‡ç©ºå€¼
    if pd.isna(search_text) or search_text == '':
        return index, None, None, None, 0, "ç©ºå€¼"
    
    # æŸ¥æ‰¾åŒ¹é…
    matched_text, matched_source, similarity = find_matching_text(
        search_text, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold
    )
    
    match_status = "åŒ¹é…æˆåŠŸ" if matched_text is not None else "æœªåŒ¹é…"
    
    return index, matched_text, matched_source, search_text, similarity, match_status

def process_single_file(args):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„åŒ¹é…"""
    filename, file_info, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled, thread_id = args
    
    df = file_info['dataframe'].copy()
    file_matches = 0
    file_total = 0
    file_skipped = 0
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
    if folder1_match_col not in df.columns:
        return filename, None, {"error": f"æ–‡ä»¶ {filename} ä¸­æ‰¾ä¸åˆ°åˆ— '{folder1_match_col}'"}
        
    if folder1_fill_col not in df.columns:
        return filename, None, {"error": f"æ–‡ä»¶ {filename} ä¸­æ‰¾ä¸åˆ°åˆ— '{folder1_fill_col}'"}
    
    # å‡†å¤‡å¤„ç†æ•°æ®
    rows_to_process = []
    for index, row in df.iterrows():
        file_total += 1
        rows_to_process.append((index, row, folder1_match_col, folder1_fill_col, folder2_files, folder2_source_col, folder2_target_col, match_strategy, similarity_threshold, skip_filled))
    
    # å¹¶è¡Œå¤„ç†è¡Œæ•°æ®
    matched_results = {}
    match_details = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        future_to_index = {executor.submit(process_single_row, args): args[0] for args in rows_to_process}
        
        for future in concurrent.futures.as_completed(future_to_index):
            try:
                index, matched_text, matched_source, search_text, similarity, match_status = future.result()
                
                # è®°å½•åŒ¹é…è¯¦æƒ…
                match_details.append({
                    'index': index,
                    'search_text': search_text,
                    'matched_text': matched_text,
                    'matched_source': matched_source,
                    'similarity': similarity,
                    'status': match_status
                })
                
                if match_status == "è·³è¿‡å·²å¡«å……":
                    file_skipped += 1
                elif matched_text is not None:
                    matched_results[index] = matched_text
                    file_matches += 1
            except Exception as e:
                index = future_to_index[future]
                st.warning(f"å¤„ç†æ–‡ä»¶ {filename} çš„ç¬¬ {index} è¡Œæ—¶å‡ºé”™: {str(e)}")
    
    # åº”ç”¨åŒ¹é…ç»“æœ
    for index, matched_text in matched_results.items():
        df.at[index, folder1_fill_col] = matched_text
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'total_rows': file_total,
        'matched_rows': file_matches,
        'unmatched_rows': file_total - file_matches - file_skipped,
        'skipped_rows': file_skipped,
        'match_details': match_details
    }
    
    return filename, df, report

def process_file_matching_parallel(folder1_path, folder2_path, folder1_match_col, folder1_fill_col, 
                                  folder2_source_col, folder2_target_col, match_strategy, 
                                  similarity_threshold, skip_filled, max_workers=4):
    """å¹¶è¡Œå¤„ç†æ–‡ä»¶åŒ¹é…"""
    
    # åŠ è½½æ–‡ä»¶å¤¹1ä¸­çš„æ–‡ä»¶
    st.info("æ­£åœ¨åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶...")
    folder1_files = load_source_files_parallel(folder1_path, max_workers)
    
    if not folder1_files:
        st.error("åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæˆ–CSVæ–‡ä»¶")
        return None, None
    
    # åŠ è½½æ–‡ä»¶å¤¹2ä¸­çš„æ–‡ä»¶
    st.info("æ­£åœ¨åŠ è½½ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶...")
    folder2_files = load_all_files_parallel(folder2_path, max_workers)
    
    if not folder2_files:
        st.error("åœ¨ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæˆ–CSVæ–‡ä»¶")
        return None, None
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶ä¿¡æ¯
    st.success(f"åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(folder1_files)} ä¸ªæ–‡ä»¶")
    st.success(f"åœ¨ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(folder2_files)} ä¸ªæ–‡ä»¶")
    
    # å¤„ç†åŒ¹é…
    st.info("å¼€å§‹åŒ¹é…å¤„ç†...")
    results = {}
    match_report = {
        'total_files': len(folder1_files),
        'total_rows': 0,
        'matched_rows': 0,
        'unmatched_rows': 0,
        'skipped_rows': 0,
        'file_details': {}
    }
    
    # å‡†å¤‡å¤„ç†æ•°æ®
    files_to_process = []
    for i, (filename, file_info) in enumerate(folder1_files.items()):
        files_to_process.append((
            filename, file_info, folder1_match_col, folder1_fill_col,
            folder2_files, folder2_source_col, folder2_target_col, 
            match_strategy, similarity_threshold, skip_filled, i % max_workers
        ))
    
    # å¹¶è¡Œå¤„ç†æ–‡ä»¶
    progress_bar = st.progress(0)
    processed_count = 0
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_filename = {executor.submit(process_single_file, args): args[0] for args in files_to_process}
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_filename):
            try:
                filename, processed_df, report = future.result()
                if processed_df is not None:
                    results[filename] = processed_df
                    match_report['file_details'][filename] = report
                    match_report['total_rows'] += report['total_rows']
                    match_report['matched_rows'] += report['matched_rows']
                    match_report['unmatched_rows'] += report['unmatched_rows']
                    match_report['skipped_rows'] += report['skipped_rows']
                elif "error" in report:
                    st.error(report["error"])
            except Exception as e:
                filename = future_to_filename[future]
                st.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
            
            # æ›´æ–°è¿›åº¦
            with progress_lock:
                processed_count += 1
                progress_bar.progress(processed_count / len(files_to_process))
    
    progress_bar.empty()
    return results, match_report

def save_processed_files(processed_files):
    """ä¿å­˜å¤„ç†åçš„æ–‡ä»¶åˆ°ZIPåŒ…"""
    zip_buffer = BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for filename, df in processed_files.items():
            # æ ¹æ®åŸæ–‡ä»¶ç±»å‹ä¿å­˜
            if filename.lower().endswith('.csv'):
                # ä¿å­˜ä¸ºCSV
                csv_buffer = BytesIO()
                df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                csv_buffer.seek(0)
                zip_file.writestr(filename, csv_buffer.getvalue())
            else:
                # ä¿å­˜ä¸ºExcel
                excel_buffer = BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
                    df.to_excel(writer, index=False, sheet_name='Sheet1')
                excel_buffer.seek(0)
                zip_file.writestr(filename, excel_buffer.getvalue())
    
    zip_buffer.seek(0)
    return zip_buffer

def excel_matchpro_page():
    st.set_page_config(
        page_title="Excel/CSVæ–‡ä»¶åŒ¹é…å·¥å…·(å¢å¼ºç‰ˆ)",
        page_icon="âš¡",
        layout="wide"
    )
    
    st.title("âš¡ Excel/CSVæ–‡ä»¶åŒ¹é…å·¥å…·(å¢å¼ºç‰ˆ)")
    st.markdown("""
    è¿™ä¸ªå·¥å…·ä½¿ç”¨å¤šçº¿ç¨‹æŠ€æœ¯åŠ é€Ÿå¤„ç†ï¼Œå¯ä»¥å¿«é€ŸåŒ¹é…ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­çš„Excelå’ŒCSVæ–‡ä»¶å†…å®¹ã€‚  
    æ”¯æŒç²¾ç¡®åŒ¹é…ã€åŒ…å«åŒ¹é…å’Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œå¯è·³è¿‡å·²æœ‰ç¿»è¯‘æ–‡æœ¬çš„è¡Œã€‚
    """)
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.header("é…ç½®å‚æ•°")
    
    with st.sidebar.expander("æ–‡ä»¶å¤¹è®¾ç½®", expanded=True):
        folder1_path = st.text_input("ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå›ºå®šæ ¼å¼æ–‡ä»¶ï¼‰", value="./folder1")
        folder2_path = st.text_input("ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç¿»è¯‘æ–‡ä»¶ï¼‰", value="./folder2")
        max_workers = st.slider("çº¿ç¨‹æ•°", min_value=1, max_value=16, value=4, step=1)
    
    with st.sidebar.expander("åŒ¹é…ç­–ç•¥è®¾ç½®", expanded=True):
        match_strategy = st.selectbox(
            "åŒ¹é…ç­–ç•¥",
            ["ç²¾ç¡®åŒ¹é…", "åŒ…å«åŒ¹é…", "ç›¸ä¼¼åº¦åŒ¹é…"],
            help="ç²¾ç¡®åŒ¹é…: å®Œå…¨ç›¸åŒçš„æ–‡æœ¬; åŒ…å«åŒ¹é…: æ–‡æœ¬äº’ç›¸åŒ…å«; ç›¸ä¼¼åº¦åŒ¹é…: åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦"
        )
        
        similarity_threshold = st.slider(
            "ç›¸ä¼¼åº¦é˜ˆå€¼(ä»…å¯¹ç›¸ä¼¼åº¦åŒ¹é…æœ‰æ•ˆ)",
            min_value=0.1,
            max_value=1.0,
            value=0.8,
            step=0.05,
            help="ç›¸ä¼¼åº¦é«˜äºæ­¤é˜ˆå€¼çš„æ–‡æœ¬å°†è¢«è§†ä¸ºåŒ¹é…"
        )
        
        skip_filled = st.checkbox(
            "è·³è¿‡å·²æœ‰ç¿»è¯‘æ–‡æœ¬çš„è¡Œ",
            value=True,
            help="å¦‚æœç›®æ ‡åˆ—å·²æœ‰å†…å®¹ï¼Œåˆ™è·³è¿‡è¯¥è¡Œä¸è¿›è¡ŒåŒ¹é…"
        )
    
    with st.sidebar.expander("åˆ—æ˜ å°„è®¾ç½®", expanded=True):
        st.markdown("**ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹åˆ—è®¾ç½®**")
        folder1_match_col = st.text_input("åŒ¹é…åˆ—åï¼ˆç”¨äºæŸ¥æ‰¾çš„åˆ—ï¼‰", value="ä¸­æ–‡æ–‡æœ¬")
        folder1_fill_col = st.text_input("å¡«å……åˆ—åï¼ˆè¦å¡«å…¥ç¿»è¯‘çš„åˆ—ï¼‰", value="è‹±æ–‡æ–‡æœ¬")
        
        st.markdown("**ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹åˆ—è®¾ç½®**")
        folder2_source_col = st.text_input("åŸæ–‡åˆ—å", value="åŸæ–‡")
        folder2_target_col = st.text_input("ç¿»è¯‘åˆ—å", value="ç¿»è¯‘ç»“æœ")
    
    # ä¸»ç•Œé¢
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if st.button("å¼€å§‹å¤„ç†", type="primary", use_container_width=True):
            if not folder1_path or not folder2_path:
                st.error("è¯·å¡«å†™ä¸¤ä¸ªæ–‡ä»¶å¤¹çš„è·¯å¾„")
                return
                
            if not os.path.exists(folder1_path):
                st.error(f"ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder1_path}")
                return
                
            if not os.path.exists(folder2_path):
                st.error(f"ç¬¬äºŒä¸ªæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder2_path}")
                return
            
            # å¤„ç†åŒ¹é…
            start_time = time.time()
            with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶åŒ¹é…..."):
                processed_files, match_report = process_file_matching_parallel(
                    folder1_path, folder2_path, 
                    folder1_match_col, folder1_fill_col,
                    folder2_source_col, folder2_target_col,
                    match_strategy, similarity_threshold, skip_filled, max_workers
                )
            
            end_time = time.time()
            
            if processed_files is not None:
                # æ˜¾ç¤ºæŠ¥å‘Š
                st.success(f"å¤„ç†å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
                
                # æ±‡æ€»æŠ¥å‘Š
                st.subheader("åŒ¹é…æŠ¥å‘Š")
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("æ€»æ–‡ä»¶æ•°", match_report['total_files'])
                    st.metric("æ€»è¡Œæ•°", match_report['total_rows'])
                
                with col2:
                    st.metric("åŒ¹é…è¡Œæ•°", match_report['matched_rows'])
                    match_rate = (match_report['matched_rows'] / (match_report['total_rows'] - match_report['skipped_rows']) * 100) if (match_report['total_rows'] - match_report['skipped_rows']) > 0 else 0
                    st.metric("åŒ¹é…ç‡", f"{match_rate:.1f}%")
                
                with col3:
                    st.metric("æœªåŒ¹é…è¡Œæ•°", match_report['unmatched_rows'])
                    st.metric("è·³è¿‡çš„è¡Œæ•°", match_report['skipped_rows'])
                
                with col4:
                    st.metric("çº¿ç¨‹æ•°", max_workers)
                    st.metric("å¤„ç†æ—¶é—´", f"{end_time - start_time:.2f}s")
                
                # è¯¦ç»†æŠ¥å‘Š
                with st.expander("è¯¦ç»†æ–‡ä»¶æŠ¥å‘Š"):
                    for filename, details in match_report['file_details'].items():
                        if 'error' not in details:
                            file_match_rate = (details['matched_rows'] / (details['total_rows'] - details['skipped_rows']) * 100) if (details['total_rows'] - details['skipped_rows']) > 0 else 0
                            st.write(f"**{filename}**: {details['matched_rows']}/{details['total_rows']} åŒ¹é… ({file_match_rate:.1f}%)ï¼Œè·³è¿‡ {details['skipped_rows']} è¡Œ")
                
                # åŒ¹é…è¯¦æƒ…è¡¨æ ¼
                with st.expander("åŒ¹é…è¯¦æƒ…"):
                    all_match_details = []
                    for filename, details in match_report['file_details'].items():
                        if 'error' not in details and 'match_details' in details:
                            for match_detail in details['match_details']:
                                match_detail['filename'] = filename
                                all_match_details.append(match_detail)
                    
                    if all_match_details:
                        details_df = pd.DataFrame(all_match_details)
                        # åªæ˜¾ç¤ºéƒ¨åˆ†åˆ—ä»¥ä¿æŒç®€æ´
                        display_df = details_df[['filename', 'search_text', 'matched_source', 'similarity', 'status']].copy()
                        display_df = display_df.rename(columns={
                            'filename': 'æ–‡ä»¶å',
                            'search_text': 'æœç´¢æ–‡æœ¬',
                            'matched_source': 'åŒ¹é…åˆ°çš„åŸæ–‡',
                            'similarity': 'ç›¸ä¼¼åº¦',
                            'status': 'çŠ¶æ€'
                        })
                        st.dataframe(display_df)
                    else:
                        st.info("æ— åŒ¹é…è¯¦æƒ…æ•°æ®")
                
                # é¢„è§ˆå¤„ç†åçš„æ•°æ®
                with st.expander("é¢„è§ˆå¤„ç†åçš„æ•°æ®"):
                    selected_file = st.selectbox("é€‰æ‹©è¦é¢„è§ˆçš„æ–‡ä»¶", list(processed_files.keys()))
                    if selected_file:
                        st.dataframe(processed_files[selected_file].head(10))
                
                # ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶
                st.subheader("ä¸‹è½½å¤„ç†ç»“æœ")
                zip_buffer = save_processed_files(processed_files)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰å¤„ç†åçš„æ–‡ä»¶ (ZIP)",
                    data=zip_buffer.getvalue(),
                    file_name="processed_files.zip",
                    mime="application/zip",
                    use_container_width=True
                )
    
    with col2:
        st.markdown("### ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        1. **è®¾ç½®æ–‡ä»¶å¤¹è·¯å¾„**: è¾“å…¥ä¸¤ä¸ªæ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„
        2. **é…ç½®åŒ¹é…ç­–ç•¥**: é€‰æ‹©é€‚åˆçš„åŒ¹é…æ–¹å¼å’Œå‚æ•°
        3. **é…ç½®åˆ—æ˜ å°„**: è®¾ç½®æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶çš„åˆ—åå¯¹åº”å…³ç³»
        4. **è°ƒæ•´çº¿ç¨‹æ•°**: æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´çº¿ç¨‹æ•°ä»¥è·å¾—æœ€ä½³æ€§èƒ½
        5. **å¼€å§‹å¤„ç†**: ç‚¹å‡»æŒ‰é’®å¼€å§‹åŒ¹é…è¿‡ç¨‹
        6. **ä¸‹è½½ç»“æœ**: å¤„ç†å®Œæˆåä¸‹è½½ZIPåŒ…
        """)
        
        st.markdown("### åŒ¹é…ç­–ç•¥è¯´æ˜")
        st.markdown("""
        - **ç²¾ç¡®åŒ¹é…**: å®Œå…¨ç›¸åŒçš„æ–‡æœ¬æ‰ä¼šåŒ¹é…
        - **åŒ…å«åŒ¹é…**: æ–‡æœ¬äº’ç›¸åŒ…å«å³å¯åŒ¹é…(å¦‚"å…³å¡1"å’Œ"å…³å¡")
        - **ç›¸ä¼¼åº¦åŒ¹é…**: åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•ï¼Œå¯å¤„ç†é”™åˆ«å­—
        """)
        
        st.markdown("### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        st.markdown("""
        - Excel: .xlsx, .xls, .xlsm
        - CSV: .csv
        """)
        
        st.markdown("### æ€§èƒ½ä¼˜åŒ–")
        st.markdown("""
        - ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ–‡ä»¶
        - è·³è¿‡å·²æœ‰ç¿»è¯‘æ–‡æœ¬çš„è¡Œ
        - æ”¯æŒæ¨¡ç³ŠåŒ¹é…å’Œç›¸ä¼¼åº¦è®¡ç®—
        - å®æ—¶è¿›åº¦æ˜¾ç¤ºå’Œè¯¦ç»†æŠ¥å‘Š
        """)
def search_in_dataframe(df, col_name, target_values, keyword, case_sensitive=False, match_whole_word=False):
    """åœ¨DataFrameä¸­æœç´¢æ»¡è¶³æ¡ä»¶çš„è¡Œ"""
    matches = []
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    if col_name not in df.columns:
        return matches, f"åˆ— '{col_name}' ä¸å­˜åœ¨"
    
    # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å‡ºæŒ‡å®šåˆ—åŒ…å«ç›®æ ‡å€¼çš„è¡Œ
    # ç¡®ä¿æ¯”è¾ƒæ—¶ä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹
    filtered_df = df[df[col_name].astype(str).isin([str(v) for v in target_values])]
    
    if len(filtered_df) == 0:
        return matches, "æœªæ‰¾åˆ°åŒ…å«æŒ‡å®šç›®æ ‡å€¼çš„è¡Œ"
    
    # å¦‚æœæ²¡æœ‰æä¾›å…³é”®è¯ï¼Œè¿”å›æ‰€æœ‰åŒ¹é…è¡Œ
    if not keyword or not keyword.strip():
        for idx, row in filtered_df.iterrows():
            matches.append({
                'row_index': idx,
                'row_data': row.to_dict(),
                'matched_column': col_name,
                'matched_value': row[col_name],
                'keyword_found': False,
                'keyword_matches': [],
                'match_count': 0
            })
        return matches, f"æ‰¾åˆ° {len(matches)} è¡ŒåŒ…å«ç›®æ ‡å€¼ï¼Œä½†æœªæœç´¢å…³é”®è¯"
    
    # ç¬¬äºŒæ­¥ï¼šåœ¨è¿‡æ»¤åçš„è¡Œä¸­æœç´¢å…³é”®è¯
    keyword = keyword.strip()
    flags = 0 if case_sensitive else re.IGNORECASE
    
    # æ„å»ºæœç´¢æ¨¡å¼
    if match_whole_word:
        pattern = r'\b' + re.escape(keyword) + r'\b'
    else:
        pattern = re.escape(keyword)
    
    keyword_matches = 0
    
    for idx, row in filtered_df.iterrows():
        row_matches = []
        
        # æœç´¢è¡Œçš„æ¯ä¸€åˆ—
        for col_idx, (col, cell_value) in enumerate(row.items()):
            if pd.isna(cell_value):
                continue
                
            cell_str = str(cell_value)
            cell_matches = list(re.finditer(pattern, cell_str, flags))
            
            for match in cell_matches:
                row_matches.append({
                    'column': col,
                    'original_value': cell_str,
                    'match_text': match.group(),
                    'start_pos': match.start(),
                    'end_pos': match.end(),
                    'replaced_value': cell_str[:match.start()] + f"**[{match.group()}]**" + cell_str[match.end():]
                })
        
        if row_matches:
            keyword_matches += 1
            matches.append({
                'row_index': idx,
                'row_data': row.to_dict(),
                'matched_column': col_name,
                'matched_value': row[col_name],
                'keyword_found': True,
                'keyword_matches': row_matches,
                'match_count': len(row_matches)
            })
    
    return matches, f"åœ¨ {len(filtered_df)} è¡Œç›®æ ‡è¡Œä¸­æ‰¾åˆ° {keyword_matches} è¡ŒåŒ…å«å…³é”®è¯"
def highlight_keyword(text, keyword, case_sensitive=False):
    """é«˜äº®æ˜¾ç¤ºå…³é”®è¯"""
    if not text or not keyword:
        return text
    
    flags = 0 if case_sensitive else re.IGNORECASE
    pattern = re.escape(keyword)
    
    if case_sensitive:
        highlighted = re.sub(f'({pattern})', r'**\1**', text)
    else:
        highlighted = re.sub(f'({pattern})', r'**\1**', text, flags=re.IGNORECASE)
    
    return highlighted

def replace_in_excel(file_path, replacements, backup=True):
    """åœ¨Excelæ–‡ä»¶ä¸­æ‰§è¡Œæ›¿æ¢æ“ä½œ"""
    try:
        file_path = Path(file_path)
        
        # å¤‡ä»½åŸæ–‡ä»¶
        if backup:
            backup_path = file_path.parent / f"{file_path.stem}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}{file_path.suffix}"
            shutil.copy2(file_path, backup_path)
            st.info(f"ğŸ“ å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_path.name}")
        
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path, engine='openpyxl')
        
        # æ‰§è¡Œæ›¿æ¢
        replaced_count = 0
        for replacement in replacements:
            row_idx = replacement['row_index']
            col_name = replacement['column']
            old_text = replacement['original_value']
            new_text = replacement['new_value']
            
            if row_idx < len(df) and col_name in df.columns:
                # è·å–å½“å‰å•å…ƒæ ¼å€¼
                current_value = df.at[row_idx, col_name]
                if pd.isna(current_value):
                    current_value = ""
                
                current_str = str(current_value)
                
                # æ‰§è¡Œæ›¿æ¢
                if replacement.get('replace_all', False):
                    # æ›¿æ¢æ‰€æœ‰å‡ºç°çš„å…³é”®è¯
                    flags = 0 if replacement.get('case_sensitive', False) else re.IGNORECASE
                    pattern = re.escape(replacement['search_keyword'])
                    if replacement.get('match_whole_word', False):
                        pattern = r'\b' + pattern + r'\b'
                    
                    new_value = re.sub(pattern, replacement['replace_keyword'], current_str, flags=flags)
                else:
                    # æ›¿æ¢ç‰¹å®šä½ç½®çš„åŒ¹é…
                    start_pos = replacement['start_pos']
                    end_pos = replacement['end_pos']
                    new_value = current_str[:start_pos] + replacement['replace_keyword'] + current_str[end_pos:]
                
                df.at[row_idx, col_name] = new_value
                replaced_count += 1
        
        # ä¿å­˜æ–‡ä»¶
        df.to_excel(file_path, index=False, engine='openpyxl')
        
        return True, f"âœ… æˆåŠŸæ›¿æ¢ {replaced_count} å¤„å†…å®¹"
        
    except Exception as e:
        return False, f"âŒ æ›¿æ¢å¤±è´¥: {str(e)}"
import tempfile


# ============================================================================
# å·¥å…·å‡½æ•° - Niconico
# ============================================================================

def find_yt_dlp():
    """æŸ¥æ‰¾yt-dlpå¯æ‰§è¡Œæ–‡ä»¶"""
    if sys.platform.startswith('win'):
        candidates = ["yt-dlp.exe", "yt-dlp"]
    else:
        candidates = ["./yt-dlp", "yt-dlp"]
    
    for candidate in candidates:
        if os.path.exists(candidate):
            return candidate
    return "yt-dlp"  # ä¾èµ–äºPATH

def normalize_url(url):
    """å°†éæ ‡å‡†çš„Niconico URLæ ¼å¼è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
    return url.replace("www.video.nicovideo.jp", "www.nicovideo.jp")

def extract_watch_id(url):
    """ä»Niconicoè§†é¢‘URLä¸­æå–watch ID (sm/nmå·)"""
    match = re.search(r'(sm|nm)\d+', url)
    if match:
        return match.group(0)
    return "unknown_id"

def extract_bilibili_id(url):
    """ä»Bilibiliè§†é¢‘URLä¸­æå–video ID (BVå·æˆ–avå·)"""
    # å°è¯•æå–BVå·
    bv_match = re.search(r'BV[a-zA-Z0-9]+', url)
    if bv_match:
        return bv_match.group(0)
    
    # å°è¯•æå–avå·
    av_match = re.search(r'av(\d+)', url)
    if av_match:
        return f"av{av_match.group(1)}"
    
    return "unknown_id"

def run_yt_dlp_to_get_json(url, output_filename_base="danmaku"):
    """è¿è¡Œyt-dlpå‘½ä»¤æ¥æŠ“å–å¼¹å¹•æ•°æ®å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
    yt_dlp_path = find_yt_dlp()
    
    command = [
        yt_dlp_path,
        "--skip-download",
        "--write-sub",
        "--all-subs",
        "--sub-format", "json",
        "--output", f"{output_filename_base}.%(ext)s",
        url
    ]
    
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        
        json_filename = f"{output_filename_base}.comments.json"
        if os.path.exists(json_filename):
            return json_filename
        else:
            return None
            
    except subprocess.CalledProcessError as e:
        st.error(f"yt-dlpæ‰§è¡Œå¤±è´¥: {e.stderr}")
        return None
    except FileNotFoundError:
        st.error(f"æ‰¾ä¸åˆ°yt-dlpå¯æ‰§è¡Œæ–‡ä»¶ã€‚è¯·ç¡®ä¿yt-dlpå·²å®‰è£…æˆ–åœ¨PATHä¸­ã€‚")
        return None

def process_niconico_json_to_dataframe(json_path):
    """è¯»å–yt-dlpç”Ÿæˆçš„JSONæ–‡ä»¶ï¼Œå¤„ç†Niconicoå¼¹å¹•æ•°æ®"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        st.error(f"JSONæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return None

    danmaku_list = []
    for comment in data:
        vpos_ms = comment.get("vposMs", 0)
        time_sec = vpos_ms / 1000
        video_time = time.strftime('%H:%M:%S', time.gmtime(time_sec))
        
        posted_at_str = comment.get("postedAt")
        try:
            posted_at = datetime.fromisoformat(posted_at_str)
            send_time = posted_at.strftime('%Y-%m-%d %H:%M:%S')
        except:
            send_time = posted_at_str
            
        commands = " ".join(comment.get("commands", []))
        
        danmaku_info = {
            "å¼¹å¹•å†…å®¹": comment.get("body"),
            "è§†é¢‘æ—¶é—´": video_time,
            "æ—¶é—´(ç§’)": time_sec,
            "æ ¼å¼/é¢œè‰²": commands,
            "ç”¨æˆ·ID": comment.get("userId"),
            "å‘é€æ—¶é—´": send_time,
            "ç¼–å·": comment.get("no"),
        }
        danmaku_list.append(danmaku_info)
        
    if not danmaku_list:
        return None
        
    df = pd.DataFrame(danmaku_list)
    df = df[['ç¼–å·', 'è§†é¢‘æ—¶é—´', 'æ—¶é—´(ç§’)', 'å¼¹å¹•å†…å®¹', 'æ ¼å¼/é¢œè‰²', 'ç”¨æˆ·ID', 'å‘é€æ—¶é—´']]
    return df

def scrape_niconico_danmaku(url):
    """æŠ“å–Niconicoå¼¹å¹•"""
    normalized_url = normalize_url(url)
    watch_id = extract_watch_id(normalized_url)
    
    with st.spinner(f"æ­£åœ¨æŠ“å–Niconicoè§†é¢‘ {watch_id} çš„å¼¹å¹•..."):
        json_path = run_yt_dlp_to_get_json(normalized_url, output_filename_base=watch_id)
        
        if json_path:
            df = process_niconico_json_to_dataframe(json_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(json_path)
            except OSError:
                pass
            
            return df, watch_id
        else:
            return None, watch_id

# ============================================================================
# å·¥å…·å‡½æ•° - Bilibili
# ============================================================================

def scrape_bilibili_danmaku(url, cookies_file=None):
    """æŠ“å–Bilibiliå¼¹å¹•"""
    video_id = extract_bilibili_id(url)
    
    with st.spinner(f"æ­£åœ¨æŠ“å–Bilibiliè§†é¢‘ {video_id} çš„å¼¹å¹•..."):
        # ä½¿ç”¨yt-dlpæŠ“å–Bilibiliå¼¹å¹•
        yt_dlp_path = find_yt_dlp()
        
        command = [
            yt_dlp_path,
            "--skip-download",
            "--write-sub",
            "--all-subs",
            "--sub-format", "json",
            "--output", f"{video_id}.%(ext)s",
        ]
        
        # å¦‚æœæä¾›äº†Cookieæ–‡ä»¶ï¼Œæ·»åŠ åˆ°å‘½ä»¤ä¸­
        if cookies_file and os.path.exists(cookies_file):
            command.extend(["--cookies", cookies_file])
        
        command.append(url)
        
        try:
            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=60)
            
            json_filename = f"{video_id}.comments.json"
            if os.path.exists(json_filename):
                try:
                    with open(json_filename, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                except (FileNotFoundError, json.JSONDecodeError) as e:
                    st.error(f"JSONæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
                    return None, video_id
                
                danmaku_list = []
                for comment in data:
                    # Bilibiliçš„å¼¹å¹•æ ¼å¼ä¸Niconicoç•¥æœ‰ä¸åŒ
                    danmaku_info = {
                        "å¼¹å¹•å†…å®¹": comment.get("body", comment.get("text", "")),
                        "å‘é€æ—¶é—´": comment.get("postedAt", comment.get("timestamp", "")),
                        "ç”¨æˆ·ID": comment.get("userId", comment.get("author", "")),
                    }
                    danmaku_list.append(danmaku_info)
                
                if danmaku_list:
                    df = pd.DataFrame(danmaku_list)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.remove(json_filename)
                    except OSError:
                        pass
                    
                    return df, video_id
                else:
                    st.warning("æœªæ‰¾åˆ°å¼¹å¹•æ•°æ®")
                    return None, video_id
            else:
                st.error(f"yt-dlpæœªç”Ÿæˆé¢„æœŸçš„æ–‡ä»¶")
                return None, video_id
                
        except subprocess.CalledProcessError as e:
            error_msg = e.stderr if e.stderr else str(e)
            if "412" in error_msg or "Precondition Failed" in error_msg:
                st.error(
                    "âŒ Bilibili APIé€Ÿç‡é™åˆ¶ï¼ˆHTTP 412é”™è¯¯ï¼‰\n\n"
                    "è¿™é€šå¸¸æ˜¯å› ä¸ºï¼š\n"
                    "1. æ²¡æœ‰æä¾›æœ‰æ•ˆçš„Cookieè®¤è¯\n"
                    "2. è¯¥IPåœ°å€çš„è¯·æ±‚å·²è¾¾åˆ°é™åˆ¶\n\n"
                    "**è§£å†³æ–¹æ¡ˆï¼š**\n"
                    "è¯·åœ¨å·¦ä¾§æ ä¸Šä¼ æ‚¨çš„Bilibili Cookieæ–‡ä»¶ï¼Œç„¶åé‡è¯•ã€‚"
                )
            else:
                st.error(f"yt-dlpæ‰§è¡Œå¤±è´¥: {error_msg}")
            return None, video_id
        except subprocess.TimeoutExpired:
            st.error("âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
            return None, video_id
        except FileNotFoundError:
            st.error(f"æ‰¾ä¸åˆ°yt-dlpå¯æ‰§è¡Œæ–‡ä»¶")
            return None, video_id

# ============================================================================
# UIå¸ƒå±€
# ============================================================================

def danmu_page():
    # æ ‡é¢˜å’Œæè¿°
    st.markdown("""
    # ğŸ¬ å¼¹å¹•æŠ“å–å·¥å…·
    
    æ”¯æŒä» **Niconico** å’Œ **Bilibili** æŠ“å–è§†é¢‘å¼¹å¹•ï¼Œå¹¶å¯¼å‡ºä¸º Excel æ–‡ä»¶ã€‚
    """)
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®1")
        platform = st.radio(
            "é€‰æ‹©è§†é¢‘å¹³å°",
            options=["Niconico", "Bilibili"],
            help="é€‰æ‹©æ‚¨è¦æŠ“å–å¼¹å¹•çš„è§†é¢‘å¹³å°",
            key="video_pla_selector"
        )
        
        st.divider()
        
        # Bilibili Cookieé…ç½®
        bilibili_cookies_file = None
        if platform == "Bilibili":
            st.subheader("ğŸ” Bilibili Cookieé…ç½®")
            st.markdown(
                """Bilibiliéœ€è¦Cookieè®¤è¯ä»¥é¿å…é€Ÿç‡é™åˆ¶ã€‚\n\n
                **è·å–Cookieçš„æ–¹æ³•ï¼š**
                1. æ‰“å¼€æµè§ˆå™¨è®¿é—® https://www.bilibili.com
                2. ç™»å½•æ‚¨çš„è´¦å·
                3. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…· â†’ Application â†’ Cookies
                4. å¤åˆ¶æ‰€æœ‰Cookieå†…å®¹åˆ°æ–‡æœ¬æ–‡ä»¶
                5. ä¸Šä¼ è¯¥æ–‡ä»¶
                """
            )
            
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ Cookieæ–‡ä»¶",
                type=["txt"],
                help="ä¸Šä¼ ä»æµè§ˆå™¨å¯¼å‡ºçš„Cookieæ–‡ä»¶"
            )
            
            if uploaded_file is not None:
                # ä¿å­˜ä¸Šä¼ çš„Cookieæ–‡ä»¶
                cookies_content = uploaded_file.read().decode('utf-8')
                bilibili_cookies_file = "temp_cookies.txt"
                with open(bilibili_cookies_file, 'w', encoding='utf-8') as f:
                    f.write(cookies_content)
                st.success("âœ… Cookieæ–‡ä»¶å·²åŠ è½½")
            else:
                st.warning("âš ï¸ æœªä¸Šä¼ Cookieæ–‡ä»¶ï¼Œå¯èƒ½å¯¼è‡´é€Ÿç‡é™åˆ¶é”™è¯¯")
        
        st.divider()
        
        st.markdown("""
        ### ğŸ“Œ ä½¿ç”¨è¯´æ˜
        
        **Niconico:**
        - è¾“å…¥æ ¼å¼: `https://www.nicovideo.jp/watch/sm500873`
        - æˆ–: `https://www.video.nicovideo.jp/watch/sm500873` (ä¼šè‡ªåŠ¨è½¬æ¢)
        
        **Bilibili:**
        - è¾“å…¥æ ¼å¼: `https://www.bilibili.com/video/BV1xx411c7mD`
        - æˆ–: `https://www.bilibili.com/video/av123456789`
        - å»ºè®®ä¸Šä¼ Cookieæ–‡ä»¶ä»¥é¿å…é€Ÿç‡é™åˆ¶
        """)
    
    # ä¸»å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader(f"è¾“å…¥{platform}è§†é¢‘é“¾æ¥")
        video_url = st.text_input(
            "è§†é¢‘é“¾æ¥",
            placeholder=f"è¯·è¾“å…¥{platform}è§†é¢‘é“¾æ¥...",
            label_visibility="collapsed"
        )
    
    with col2:
        st.subheader("æ“ä½œ")
        scrape_button = st.button(
            "ğŸ” å¼€å§‹æŠ“å–",
            use_container_width=True,
            type="primary"
        )
    
    st.divider()
    
    # å¤„ç†æŠ“å–è¯·æ±‚
    if scrape_button:
        if not video_url.strip():
            st.error("âŒ è¯·è¾“å…¥è§†é¢‘é“¾æ¥")
        else:
            if platform == "Niconico":
                df, video_id = scrape_niconico_danmaku(video_url)
            else:  # Bilibili
                df, video_id = scrape_bilibili_danmaku(video_url, cookies_file=bilibili_cookies_file)
            
            if df is not None and len(df) > 0:
                st.success(f"âœ… æˆåŠŸæŠ“å– {len(df)} æ¡å¼¹å¹•ï¼")
                
                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                st.subheader("ğŸ“Š å¼¹å¹•æ•°æ®é¢„è§ˆ")
                st.dataframe(df, use_container_width=True, height=400)
                
                # å¯¼å‡ºé€‰é¡¹
                st.subheader("ğŸ’¾ å¯¼å‡ºé€‰é¡¹")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    # Excelå¯¼å‡º
                    excel_buffer = pd.ExcelWriter(
                        f"danmaku_{video_id}.xlsx",
                        engine='openpyxl'
                    )
                    df.to_excel(excel_buffer, index=False, sheet_name='å¼¹å¹•æ•°æ®')
                    excel_buffer.close()
                    
                    with open(f"danmaku_{video_id}.xlsx", 'rb') as f:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½ Excel",
                            data=f.read(),
                            file_name=f"danmaku_{video_id}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.remove(f"danmaku_{video_id}.xlsx")
                    except OSError:
                        pass
                    
                    # æ¸…ç†ä¸´æ—¶Cookieæ–‡ä»¶
                    if bilibili_cookies_file and os.path.exists(bilibili_cookies_file):
                        try:
                            os.remove(bilibili_cookies_file)
                        except OSError:
                            pass
                
                with col2:
                    # CSVå¯¼å‡º
                    csv_buffer = df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ CSV",
                        data=csv_buffer,
                        file_name=f"danmaku_{video_id}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                
                with col3:
                    # JSONå¯¼å‡º
                    json_buffer = df.to_json(orient='records', force_ascii=False)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ JSON",
                        data=json_buffer,
                        file_name=f"danmaku_{video_id}.json",
                        mime="application/json",
                        use_container_width=True
                    )
                
                # ç»Ÿè®¡ä¿¡æ¯
                st.subheader("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("æ€»å¼¹å¹•æ•°", len(df))
                
                with col2:
                    if 'ç”¨æˆ·ID' in df.columns:
                        unique_users = df['ç”¨æˆ·ID'].nunique()
                        st.metric("ç‹¬ç«‹ç”¨æˆ·æ•°", unique_users)
                
                with col3:
                    if 'å¼¹å¹•å†…å®¹' in df.columns:
                        avg_length = df['å¼¹å¹•å†…å®¹'].str.len().mean()
                        st.metric("å¹³å‡å¼¹å¹•é•¿åº¦", f"{avg_length:.1f} å­—ç¬¦")
            
            elif df is not None and len(df) == 0:
                st.warning("âš ï¸ æœªæ‰¾åˆ°å¼¹å¹•æ•°æ®ï¼Œè¯·æ£€æŸ¥è§†é¢‘é“¾æ¥æ˜¯å¦æ­£ç¡®")
            else:
                st.error("âŒ æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è§†é¢‘é“¾æ¥æˆ–ç½‘ç»œè¿æ¥")
    
    # é¡µè„š
    st.divider()
    st.markdown("""
    ---
    **å¼¹å¹•æŠ“å–å·¥å…·** | åŸºäº Streamlit å’Œ yt-dlp
    
    ğŸ’¡ **æç¤º:**
    - æŸäº›è§†é¢‘çš„å¼¹å¹•å¯èƒ½éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®
    - å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥
    - æ”¯æŒçš„è§†é¢‘å¹³å°ï¼šNiconicoã€Bilibili
    """)

def excel_sreplace_page():
    # åˆ›å»ºä¾§è¾¹æ ç”¨äºè¾“å…¥å‚æ•°
    st.sidebar.header("ğŸ”§ æœç´¢å‚æ•°è®¾ç½®")
    
    # è·å–æ–‡ä»¶å¤¹è·¯å¾„
    folder_path = st.sidebar.text_input(
        "ğŸ“ è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„:",
        placeholder="ä¾‹å¦‚: C:/Users/ç”¨æˆ·å/Documents/Excelæ–‡ä»¶",
        help="è¯·è¾“å…¥åŒ…å«Excelæ–‡ä»¶çš„æ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„"
    )
    
    # æœç´¢å‚æ•°è®¾ç½®
    col_name = st.sidebar.text_input(
        "ğŸ“Š è¦æœç´¢çš„åˆ—å:",
        value="è§’è‰²å",
        placeholder="ä¾‹å¦‚: è§’è‰²å",
        help="è¯·è¾“å…¥è¦æœç´¢çš„Excelåˆ—åç§°"
    )
    
    # ç›®æ ‡å€¼è¾“å…¥ï¼ˆæ”¯æŒå¤šä¸ªå€¼ï¼‰
    target_values_input = st.sidebar.text_input(
        "ğŸ¯ åˆ—ç›®æ ‡å€¼ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰:",
        value="ç­é•¿,ç­é•¿å¤§äºº",
        placeholder="ä¾‹å¦‚: ç­é•¿,ç­é•¿å¤§äºº",
        help="è¯·è¾“å…¥è¦åœ¨æŒ‡å®šåˆ—ä¸­æŸ¥æ‰¾çš„å€¼ï¼Œå¤šä¸ªå€¼ç”¨é€—å·åˆ†éš”"
    )
    
    # è¦æœç´¢çš„å…³é”®è¯
    search_keyword = st.sidebar.text_input(
        "ğŸ”¤ è¦æŸ¥æ‰¾çš„å…³é”®è¯YYY:",
        value="ç§",
        placeholder="è¯·è¾“å…¥è¦åœ¨è¡Œä¸­æŸ¥æ‰¾çš„å…³é”®è¯",
        help="åœ¨æ»¡è¶³æ¡ä»¶çš„è¡Œä¸­æŸ¥æ‰¾æ­¤å…³é”®è¯"
    )
    
    # é«˜çº§é€‰é¡¹
    st.sidebar.markdown("---")
    st.sidebar.subheader("âš™ï¸ é«˜çº§é€‰é¡¹")
    
    case_sensitive = st.sidebar.checkbox(
        "åŒºåˆ†å¤§å°å†™",
        value=False,
        help="å‹¾é€‰åæœç´¢æ—¶åŒºåˆ†è‹±æ–‡å¤§å°å†™"
    )
    
    match_whole_word = st.sidebar.checkbox(
        "å…¨è¯åŒ¹é…",
        value=False,
        help="å‹¾é€‰ååªåŒ¹é…å®Œæ•´çš„è¯è¯­"
    )
    
    # å¤„ç†ç›®æ ‡å€¼ï¼ˆåˆ†å‰²é€—å·åˆ†éš”çš„å€¼ï¼‰
    target_values = [v.strip() for v in target_values_input.split(',') if v.strip()]
    
    # æ£€æŸ¥è¾“å…¥å‚æ•°
    if not folder_path:
        st.warning("âš ï¸ è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„")
        return
    
    if not col_name:
        st.warning("âš ï¸ è¯·è¾“å…¥è¦æœç´¢çš„åˆ—å")
        return
    
    if not target_values:
        st.warning("âš ï¸ è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªç›®æ ‡å€¼")
        return
    
    # æŸ¥æ‰¾Excelæ–‡ä»¶
    success, result = find_excel_files(folder_path)
    
    if not success:
        st.error(f"âŒ {result}")
        return
    
    excel_files = result
    
    if not excel_files:
        st.warning("âš ï¸ åœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°Excelæ–‡ä»¶")
        return
    
    st.success(f"âœ… æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨
    with st.expander("ğŸ“ æ‰¾åˆ°çš„Excelæ–‡ä»¶"):
        for i, file_path in enumerate(excel_files[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            st.write(f"{i+1}. {file_path.name}")
        
        if len(excel_files) > 10:
            st.info(f"... è¿˜æœ‰ {len(excel_files) - 10} ä¸ªæ–‡ä»¶")
    
    # æ‰§è¡Œæœç´¢
    if st.button("ğŸš€ å¼€å§‹æœç´¢", type="primary", use_container_width=True):
        if not search_keyword.strip():
            st.warning("âš ï¸ å…³é”®è¯YYYä¸ºç©ºï¼Œå°†åªæ˜¾ç¤ºåŒ…å«ç›®æ ‡å€¼çš„è¡Œ")
        
        all_matches = []
        files_with_matches = 0
        total_matches = 0
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, file_path in enumerate(excel_files):
            progress = (i + 1) / len(excel_files)
            progress_bar.progress(progress)
            status_text.text(f"ğŸ” æ­£åœ¨å¤„ç†æ–‡ä»¶ {i+1}/{len(excel_files)}: {file_path.name}")
            
            try:
                # è¯»å–Excelæ–‡ä»¶
                df = pd.read_excel(file_path, engine='openpyxl')
                
                # æœç´¢æ•°æ®
                matches, message = search_in_dataframe(
                    df, col_name, target_values, search_keyword, 
                    case_sensitive, match_whole_word
                )
                
                if matches:
                    files_with_matches += 1
                    total_matches += len(matches)
                    
                    for match in matches:
                        match['file_path'] = str(file_path)
                        match['file_name'] = file_path.name
                        all_matches.append(match)
                
                # çŸ­æš‚å»¶è¿Ÿä»¥ä¾¿æ˜¾ç¤ºè¿›åº¦
                import time
                time.sleep(0.1)
                
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
        
        progress_bar.progress(1.0)
        status_text.text(f"âœ… æœç´¢å®Œæˆï¼")
        
        # ä¿å­˜æœç´¢ç»“æœåˆ°session state
        st.session_state.search_results = all_matches
        st.session_state.search_keyword = search_keyword
        st.session_state.case_sensitive = case_sensitive
        st.session_state.match_whole_word = match_whole_word
        
        # æ˜¾ç¤ºæœç´¢ç»“æœ
        st.header("ğŸ“Š æœç´¢ç»“æœ")
        
        if total_matches == 0:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„è¡Œ")
            
            # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ‰«ææ–‡ä»¶æ•°", len(excel_files))
            with col2:
                st.metric("åŒ…å«åŒ¹é…çš„æ–‡ä»¶", files_with_matches)
            with col3:
                st.metric("æ€»åŒ¹é…è¡Œæ•°", total_matches)
        else:
            st.success(f"âœ… åœ¨ {files_with_matches} ä¸ªæ–‡ä»¶ä¸­æ‰¾åˆ° {total_matches} è¡ŒåŒ¹é…ç»“æœ")
            
            # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ‰«ææ–‡ä»¶æ•°", len(excel_files))
            with col2:
                st.metric("åŒ…å«åŒ¹é…çš„æ–‡ä»¶", files_with_matches)
            with col3:
                st.metric("æ€»åŒ¹é…è¡Œæ•°", total_matches)
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºç»“æœ
            files_group = {}
            for match in all_matches:
                file_name = match['file_name']
                if file_name not in files_group:
                    files_group[file_name] = []
                files_group[file_name].append(match)
            
            # æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
            for file_name, file_matches in files_group.items():
                with st.expander(f"ğŸ“„ {file_name} ({len(file_matches)} è¡ŒåŒ¹é…)", expanded=True):
                    st.write(f"**æ–‡ä»¶è·¯å¾„:** {file_matches[0]['file_path']}")
                    
                    # åˆ›å»ºç»“æœæ˜¾ç¤ºè¡¨æ ¼
                    display_data = []
                    for i, match in enumerate(file_matches):
                        row_data = match['row_data']
                        
                        # å‡†å¤‡è¡Œæ˜¾ç¤ºæ•°æ®
                        row_display = {}
                        for col_name_display, cell_value in row_data.items():
                            if pd.isna(cell_value):
                                display_value = ""
                            else:
                                cell_str = str(cell_value)
                                
                                # é«˜äº®å…³é”®è¯
                                if search_keyword and search_keyword.strip():
                                    highlighted = highlight_keyword(
                                        cell_str, search_keyword, case_sensitive
                                    )
                                    row_display[col_name_display] = highlighted
                                else:
                                    row_display[col_name_display] = cell_str
                        
                        display_data.append({
                            'åºå·': i + 1,
                            'åŒ¹é…åˆ—å€¼': match['matched_value'],
                            'å…³é”®è¯åŒ¹é…æ•°': match.get('match_count', 0) if match.get('keyword_found', False) else 'æ— ',
                            **row_display
                        })
                    
                    # è½¬æ¢ä¸ºDataFrameæ˜¾ç¤º
                    if display_data:
                        # è·å–æ‰€æœ‰åˆ—å
                        all_columns = set()
                        for item in display_data:
                            all_columns.update(item.keys())
                        
                        # åˆ›å»ºè§„èŒƒçš„æ˜¾ç¤ºé¡ºåº
                        base_columns = ['åºå·', 'åŒ¹é…åˆ—å€¼', 'å…³é”®è¯åŒ¹é…æ•°']
                        other_columns = [col for col in all_columns if col not in base_columns]
                        display_columns = base_columns + sorted(other_columns)
                        
                        # åˆ›å»ºæ˜¾ç¤ºDataFrame
                        display_df = pd.DataFrame(display_data)
                        
                        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
                        for col in display_columns:
                            if col not in display_df.columns:
                                display_df[col] = ""
                        
                        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
                        display_df = display_df[display_columns]
                        
                        # æ˜¾ç¤ºè¡¨æ ¼
                        st.dataframe(display_df, use_container_width=True)
                    
                    # æ˜¾ç¤ºåŒ¹é…è¯¦æƒ…
                    if search_keyword and search_keyword.strip():
                        st.subheader("ğŸ” åŒ¹é…è¯¦æƒ…")
                        for i, match in enumerate(file_matches):
                            if match.get('keyword_found', False) and match.get('keyword_matches'):
                                with st.expander(f"åŒ¹é…è¯¦æƒ… - è¡Œ {i+1}"):
                                    st.write(f"**æ–‡ä»¶:** {file_name}")
                                    st.write(f"**è¡Œç´¢å¼•:** {match['row_index'] + 2}")  # +2 å› ä¸ºExcelä»1å¼€å§‹ä¸”æœ‰æ ‡é¢˜
                                    st.write(f"**åœ¨åˆ— '{col_name}' ä¸­æ‰¾åˆ°å€¼:** {match['matched_value']}")
                                    st.write(f"**å…³é”®è¯åŒ¹é…ä½ç½®:**")
                                    
                                    for kw_match in match['keyword_matches']:
                                        col_name_kw = kw_match['column']
                                        cell_value = kw_match['original_value']
                                        match_text = kw_match['match_text']
                                        start_pos = kw_match['start_pos']
                                        end_pos = kw_match['end_pos']
                                        
                                        # æ˜¾ç¤ºä¸Šä¸‹æ–‡
                                        context_start = max(0, start_pos - 20)
                                        context_end = min(len(cell_value), end_pos + 20)
                                        context = cell_value[context_start:context_end]
                                        
                                        # é«˜äº®æ˜¾ç¤º
                                        if context_start > 0:
                                            context = "..." + context
                                        if context_end < len(cell_value):
                                            context = context + "..."
                                        
                                        highlighted_context = highlight_keyword(
                                            context, search_keyword, case_sensitive
                                        )
                                        
                                        st.write(f"**åˆ— '{col_name_kw}':** {highlighted_context}")
            
            # æä¾›ç»“æœä¸‹è½½
            if all_matches:
                st.header("ğŸ’¾ ä¸‹è½½æœç´¢ç»“æœ")
                
                # å‡†å¤‡ä¸‹è½½æ•°æ®
                download_data = []
                for match in all_matches:
                    row_data = {
                        'æ–‡ä»¶è·¯å¾„': match['file_path'],
                        'æ–‡ä»¶åç§°': match['file_name'],
                        'è¡Œç´¢å¼•': match['row_index'] + 2,  # è½¬æ¢ä¸ºExcelè¡Œå·
                        'åŒ¹é…åˆ—': match['matched_column'],
                        'åŒ¹é…åˆ—å€¼': match['matched_value'],
                        'æ˜¯å¦æ‰¾åˆ°å…³é”®è¯': 'æ˜¯' if match.get('keyword_found', False) else 'å¦',
                        'å…³é”®è¯åŒ¹é…æ•°': match.get('match_count', 0)
                    }
                    
                    # æ·»åŠ æ‰€æœ‰åˆ—æ•°æ®
                    for col_name_dl, cell_value in match['row_data'].items():
                        row_data[col_name_dl] = cell_value if not pd.isna(cell_value) else ""
                    
                    download_data.append(row_data)
                
                download_df = pd.DataFrame(download_data)
                
                # æä¾›ä¸‹è½½
                csv_data = download_df.to_csv(index=False).encode('utf-8-sig')
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æœç´¢ç»“æœ(CSV)",
                    data=csv_data,
                    file_name=f"excel_search_results.csv",
                    mime="text/csv",
                    use_container_width=True
                )

    # æ›¿æ¢åŠŸèƒ½ç•Œé¢
    if st.session_state.get('search_results'):
        st.markdown("---")
        st.header("ğŸ”„ æ›¿æ¢åŠŸèƒ½")
        
        # è·å–æœç´¢ç»“æœ
        search_results = st.session_state.search_results
        search_keyword = st.session_state.search_keyword
        case_sensitive = st.session_state.case_sensitive
        match_whole_word = st.session_state.match_whole_word
        
        # æ›¿æ¢è®¾ç½®
        col1, col2 = st.columns(2)
        
        with col1:
            replace_keyword = st.text_input(
                "ğŸ”„ æ›¿æ¢ä¸º:",
                value="åƒ•",
                placeholder="è¯·è¾“å…¥æ›¿æ¢åçš„è¯è¯­",
                help="å°†æŸ¥æ‰¾åˆ°çš„å…³é”®è¯æ›¿æ¢ä¸ºæ­¤è¯è¯­"
            )
        
        with col2:
            replace_mode = st.radio(
                "æ›¿æ¢æ¨¡å¼:",
                ["æ›¿æ¢æ‰€æœ‰åŒ¹é…", "ä»…æ›¿æ¢é€‰ä¸­é¡¹"],
                help="é€‰æ‹©æ›¿æ¢å…¨éƒ¨åŒ¹é…é¡¹è¿˜æ˜¯ä»…æ›¿æ¢é€‰ä¸­çš„åŒ¹é…é¡¹",
                key = "tihuanA"
            )
            
            create_backup = st.checkbox(
                "åˆ›å»ºå¤‡ä»½æ–‡ä»¶",
                value=True,
                help="æ›¿æ¢å‰è‡ªåŠ¨åˆ›å»ºå¤‡ä»½æ–‡ä»¶"
            )
        
        # æ˜¾ç¤ºæ›¿æ¢é¢„è§ˆ
        st.subheader("ğŸ‘ï¸ æ›¿æ¢é¢„è§ˆ")
        
        # æ”¶é›†æ‰€æœ‰åŒ¹é…é¡¹ç”¨äºæ›¿æ¢
        all_replacements = []
        files_to_replace = set()
        
        for match in search_results:
            if match.get('keyword_found', False) and match.get('keyword_matches'):
                file_path = match['file_path']
                files_to_replace.add(file_path)
                
                for kw_match in match['keyword_matches']:
                    replacement_info = {
                        'file_path': file_path,
                        'file_name': match['file_name'],
                        'row_index': match['row_index'],
                        'column': kw_match['column'],
                        'original_value': kw_match['original_value'],
                        'search_keyword': search_keyword,
                        'replace_keyword': replace_keyword,
                        'start_pos': kw_match['start_pos'],
                        'end_pos': kw_match['end_pos'],
                        'case_sensitive': case_sensitive,
                        'match_whole_word': match_whole_word,
                        'replace_all': (replace_mode == "æ›¿æ¢æ‰€æœ‰åŒ¹é…")
                    }
                    
                    # è®¡ç®—æ›¿æ¢åçš„å€¼
                    if replacement_info['replace_all']:
                        flags = 0 if case_sensitive else re.IGNORECASE
                        pattern = re.escape(search_keyword)
                        if match_whole_word:
                            pattern = r'\b' + pattern + r'\b'
                        
                        new_value = re.sub(pattern, replace_keyword, kw_match['original_value'], flags=flags)
                    else:
                        new_value = kw_match['original_value'][:kw_match['start_pos']] + replace_keyword + kw_match['original_value'][kw_match['end_pos']:]
                    
                    replacement_info['new_value'] = new_value
                    all_replacements.append(replacement_info)
        
        # æ˜¾ç¤ºæ›¿æ¢é¢„è§ˆ
        if all_replacements:
            st.info(f"ğŸ“Š å…±æ‰¾åˆ° {len(all_replacements)} å¤„å¯æ›¿æ¢å†…å®¹ï¼Œæ¶‰åŠ {len(files_to_replace)} ä¸ªæ–‡ä»¶")
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºé¢„è§ˆ
            for file_path in files_to_replace:
                file_replacements = [r for r in all_replacements if r['file_path'] == file_path]
                
                with st.expander(f"ğŸ“„ {Path(file_path).name} - {len(file_replacements)} å¤„æ›¿æ¢"):
                    preview_data = []
                    
                    for i, replacement in enumerate(file_replacements[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                        original_text = replacement['original_value']
                        new_text = replacement['new_value']
                        
                        # é«˜äº®æ˜¾ç¤ºå˜åŒ–
                        highlighted_original = highlight_keyword(original_text, search_keyword, case_sensitive)
                        highlighted_new = highlight_keyword(new_text, replace_keyword, case_sensitive)
                        
                        preview_data.append({
                            'åºå·': i + 1,
                            'è¡Œå·': replacement['row_index'] + 2,
                            'åˆ—å': replacement['column'],
                            'åŸå†…å®¹': highlighted_original,
                            'æ–°å†…å®¹': highlighted_new,
                            'å˜åŒ–': "âœ… æœ‰å˜åŒ–" if original_text != new_text else "âš ï¸ æ— å˜åŒ–"
                        })
                    
                    if preview_data:
                        preview_df = pd.DataFrame(preview_data)
                        st.dataframe(preview_df, use_container_width=True)
                    
                    if len(file_replacements) > 10:
                        st.info(f"... è¿˜æœ‰ {len(file_replacements) - 10} å¤„æ›¿æ¢æœªæ˜¾ç¤º")
            
            # æ›¿æ¢ç¡®è®¤
            st.subheader("âœ… æ›¿æ¢ç¡®è®¤")
            
            st.warning("âš ï¸ æ­¤æ“ä½œå°†ä¿®æ”¹åŸå§‹Excelæ–‡ä»¶ï¼è¯·ç¡®è®¤ä»¥ä¸‹ä¿¡æ¯ï¼š")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»æ›¿æ¢æ•°", len(all_replacements))
            with col2:
                st.metric("æ¶‰åŠæ–‡ä»¶æ•°", len(files_to_replace))
            with col3:
                st.metric("å¤‡ä»½æ–‡ä»¶", "ä¼šåˆ›å»º" if create_backup else "ä¸åˆ›å»º")
            
            # äºŒæ¬¡ç¡®è®¤
            confirm_replace = st.checkbox("æˆ‘ç¡®è®¤è¦æ‰§è¡Œæ›¿æ¢æ“ä½œï¼Œç†è§£æ­¤æ“ä½œä¼šä¿®æ”¹åŸå§‹æ–‡ä»¶")
            
            if confirm_replace:
                if st.button("ğŸ”„ æ‰§è¡Œæ›¿æ¢", type="primary", use_container_width=True):
                    # æŒ‰æ–‡ä»¶åˆ†ç»„æ‰§è¡Œæ›¿æ¢
                    total_replaced = 0
                    success_files = 0
                    
                    for file_path in files_to_replace:
                        file_replacements = [r for r in all_replacements if r['file_path'] == file_path]
                        
                        with st.spinner(f"æ­£åœ¨æ›¿æ¢æ–‡ä»¶ {Path(file_path).name}..."):
                            success, message = replace_in_excel(file_path, file_replacements, create_backup)
                            
                            if success:
                                success_files += 1
                                total_replaced += len(file_replacements)
                                st.success(f"âœ… {Path(file_path).name}: {message}")
                            else:
                                st.error(f"âŒ {Path(file_path).name}: {message}")
                    
                    st.success(f"ğŸ‰ æ›¿æ¢å®Œæˆï¼æˆåŠŸå¤„ç† {success_files}/{len(files_to_replace)} ä¸ªæ–‡ä»¶ï¼Œå…±æ›¿æ¢ {total_replaced} å¤„å†…å®¹")
                    
                    # æ¸…ç©ºæœç´¢ç»“æœï¼Œæç¤ºé‡æ–°æœç´¢
                    st.session_state.search_results = None
                    st.info("ğŸ’¡ æ›¿æ¢å®Œæˆï¼Œè¯·é‡æ–°æœç´¢ä»¥æŸ¥çœ‹æ›´æ–°åçš„å†…å®¹")
        else:
            st.warning("æœªæ‰¾åˆ°å¯æ›¿æ¢çš„å†…å®¹")
def term_lookup_page():
    st.title("ğŸ” æœ¯è¯­æŸ¥è¯¢")
    st.markdown("### æ™ºèƒ½æœç´¢æœ¯è¯­åº“ä¸è§’è‰²æ•°æ®åº“ | by Jacky_9S")
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'lookup_translator' not in st.session_state:
        st.session_state.lookup_translator = MultiAPIExcelTranslator(
            api_key="", 
            api_provider="DeepSeek", 
            api_url=get_api_providers()["DeepSeek"]["url"], 
            model="deepseek-chat"
        )
    
    if 'lookup_term_base_loaded' not in st.session_state:
        st.session_state.lookup_term_base_loaded = False
    
    if 'lookup_role_base_loaded' not in st.session_state:
        st.session_state.lookup_role_base_loaded = False
    
    if 'lookup_term_target_cols' not in st.session_state:
        st.session_state.lookup_term_target_cols = []
    
    if 'lookup_term_df' not in st.session_state:
        st.session_state.lookup_term_df = None
    
    if 'lookup_role_df' not in st.session_state:
        st.session_state.lookup_role_df = None
    
    if 'lookup_term_source_col' not in st.session_state:
        st.session_state.lookup_term_source_col = None
    
    if 'lookup_role_name_col' not in st.session_state:
        st.session_state.lookup_role_name_col = None
    
    if 'lookup_role_personality_col' not in st.session_state:
        st.session_state.lookup_role_personality_col = None
    
    translator = st.session_state.lookup_translator
    
    # å¿«é€Ÿå¯¼å…¥åŒºåŸŸ
    st.header("ğŸš€ å¿«é€Ÿå¯¼å…¥")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", use_container_width=True):
            # æ¸…ç©ºæ‰€æœ‰æ•°æ®
            translator.term_base_list = []
            translator.role_personality_dict = {}
            st.session_state.lookup_term_base_loaded = False
            st.session_state.lookup_role_base_loaded = False
            st.session_state.lookup_term_target_cols = []
            st.session_state.lookup_term_df = None
            st.session_state.lookup_role_df = None
            st.session_state.lookup_term_source_col = None
            st.session_state.lookup_role_name_col = None
            st.session_state.lookup_role_personality_col = None
            st.success("âœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç©º!")
            st.rerun()
    
    st.markdown("---")
    
    # æ‰¹é‡ä¸Šä¼ åŒºåŸŸ
    st.header("ğŸ“¤ æ‰¹é‡ä¸Šä¼ ")
    batch_files = st.file_uploader(
        "ğŸ“ åŒæ—¶é€‰æ‹©æœ¯è¯­åº“å’Œè§’è‰²æ¡£æ¡ˆæ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰",
        type=['xlsx', 'xls'],
        accept_multiple_files=True,
        key="lookup_batch_files"
    )
    
    if batch_files:
        st.info(f"å·²é€‰æ‹© {len(batch_files)} ä¸ªæ–‡ä»¶")
        for i, file in enumerate(batch_files):
            st.write(f"{i+1}. {file.name}")
        
        if st.button("ğŸ”„ å¤„ç†æ‰¹é‡ä¸Šä¼ çš„æ–‡ä»¶", key="process_batch_files"):
            term_file = None
            role_file = None
            
            # è¯†åˆ«æ–‡ä»¶ç±»å‹
            for file in batch_files:
                file_name = file.name.lower()
                if 'æœ¯è¯­åº“' in file_name or 'term' in file_name or 'æœ¯è¯­' in file_name:
                    term_file = file
                elif 'è§’è‰²æ¡£æ¡ˆ' in file_name or 'role' in file_name or 'è§’è‰²' in file_name:
                    role_file = file
            
            # å¤„ç†è¯†åˆ«åˆ°çš„æ–‡ä»¶
            if term_file:
                try:
                    df = pd.read_excel(term_file)
                    df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                    st.session_state.lookup_term_df = df
                    st.success(f"âœ… å·²è¯†åˆ«å¹¶åŠ è½½æœ¯è¯­åº“æ–‡ä»¶: {term_file.name}")
                except Exception as e:
                    st.error(f"âŒ æœ¯è¯­åº“æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            
            if role_file:
                try:
                    df = pd.read_excel(role_file)
                    df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                    st.session_state.lookup_role_df = df
                    st.success(f"âœ… å·²è¯†åˆ«å¹¶åŠ è½½è§’è‰²æ¡£æ¡ˆæ–‡ä»¶: {role_file.name}")
                except Exception as e:
                    st.error(f"âŒ è§’è‰²æ¡£æ¡ˆæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            
            if not term_file and not role_file:
                st.warning("âš ï¸ æœªèƒ½è¯†åˆ«æœ¯è¯­åº“æˆ–è§’è‰²æ¡£æ¡ˆæ–‡ä»¶")
                st.info("ğŸ’¡ æç¤ºï¼šæ–‡ä»¶ååº”åŒ…å«'æœ¯è¯­åº“'æˆ–'è§’è‰²æ¡£æ¡ˆ'ç­‰å…³é”®è¯")
    
    st.markdown("---")
    
    # æœ¯è¯­åº“å’Œè§’è‰²åº“ä¸Šä¼ åŒºåŸŸ
    col1, col2 = st.columns(2)
    
    with col1:
        st.header("ğŸ“š æœ¯è¯­åº“ä¸Šä¼ ")
        
        uploaded_term_base = st.file_uploader(
            "ğŸ“ é€‰æ‹©æœ¯è¯­åº“æ–‡ä»¶ï¼ˆExcelï¼‰",
            type=['xlsx', 'xls'],
            key="lookup_term_base_uploader"
        )
        
        if uploaded_term_base is not None:
            try:
                df = pd.read_excel(uploaded_term_base)
                df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.lookup_term_df = df
                st.success(f"âœ… æˆåŠŸè¯»å–æœ¯è¯­åº“ï¼Œå…± {len(df)} æ¡è®°å½•")
                
                with st.expander("ğŸ“Š æœ¯è¯­åº“é¢„è§ˆ"):
                    st.dataframe(df.head(10))
            except Exception as e:
                st.error(f"âŒ å¤„ç†æœ¯è¯­åº“æ–‡ä»¶å¤±è´¥: {e}")
        
        # å¦‚æœæœ¯è¯­åº“å·²åŠ è½½ï¼Œæ˜¾ç¤ºåˆ—é€‰æ‹©
        if st.session_state.lookup_term_df is not None:
            df = st.session_state.lookup_term_df
            cols = df.columns.tolist()
            
            st.subheader("ğŸ“ é€‰æ‹©åˆ—")
            source_col_name = st.selectbox(
                "é€‰æ‹©ä¸­æ–‡åˆ—",
                options=cols,
                index=0,
                key="lookup_term_source_col_select"
            )
            
            target_cols_names = st.multiselect(
                "é€‰æ‹©ç¿»è¯‘åˆ—ï¼ˆå¯å¤šé€‰ï¼‰",
                options=cols,
                default=[cols[1]] if len(cols) > 1 else [],
                key="lookup_term_target_cols_select"
            )
            
            if st.button("ğŸ“¥ åŠ è½½æœ¯è¯­åº“", key="lookup_load_term_base"):
                if not target_cols_names:
                    st.error("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç¿»è¯‘åˆ—")
                else:
                    # å­˜å‚¨åˆ—åå’Œç´¢å¼•
                    st.session_state.lookup_term_source_col = source_col_name
                    st.session_state.lookup_term_target_cols = target_cols_names
                    
                    # æ„å»ºæœ¯è¯­åˆ—è¡¨ï¼ˆç”¨äºå…¼å®¹åŸæœ‰çš„æŸ¥æ‰¾é€»è¾‘ï¼‰
                    term_list = []
                    for _, row in df.iterrows():
                        source = row[source_col_name]
                        if pd.isna(source):
                            continue
                        source = str(source).strip()
                        if not source:
                            continue
                        
                        for target_col in target_cols_names:
                            target = row[target_col]
                            if pd.isna(target) or str(target).strip() in ['', '-']:
                                continue
                            target = str(target).strip()
                            
                            term_list.append({
                                'source': source,
                                'target': target,
                                'target_col': target_col
                            })
                    
                    translator.term_base_list = term_list
                    st.session_state.lookup_term_base_loaded = True
                    st.success(f"âœ… æœ¯è¯­åº“åŠ è½½æˆåŠŸï¼šå…± {len(df)} æ¡è®°å½•ï¼Œ{len(term_list)} ä¸ªæœ¯è¯­-ç¿»è¯‘å¯¹")
                    st.rerun()
        
        if st.session_state.lookup_term_base_loaded:
            st.metric("ğŸ“Š å·²åŠ è½½æœ¯è¯­æ•°", len(translator.term_base_list))
    
    with col2:
        st.header("ğŸ‘¤ è§’è‰²æ€§æ ¼åº“ä¸Šä¼ ")
        
        uploaded_role_base = st.file_uploader(
            "ğŸ“ é€‰æ‹©è§’è‰²æ€§æ ¼åº“æ–‡ä»¶ï¼ˆExcelï¼‰",
            type=['xlsx', 'xls'],
            key="lookup_role_base_uploader"
        )
        
        if uploaded_role_base is not None:
            try:
                df = pd.read_excel(uploaded_role_base)
                df.columns = df.columns.str.strip().str.replace('\n', '').str.replace('\r', '')
                st.session_state.lookup_role_df = df
                st.success(f"âœ… æˆåŠŸè¯»å–è§’è‰²æ€§æ ¼åº“ï¼Œå…± {len(df)} æ¡è®°å½•")
                
                with st.expander("ğŸ“Š è§’è‰²æ€§æ ¼åº“é¢„è§ˆ"):
                    st.dataframe(df.head(10))
            except Exception as e:
                st.error(f"âŒ å¤„ç†è§’è‰²æ€§æ ¼åº“æ–‡ä»¶å¤±è´¥: {e}")
        
        # å¦‚æœè§’è‰²åº“å·²åŠ è½½ï¼Œæ˜¾ç¤ºåˆ—é€‰æ‹©
        if st.session_state.lookup_role_df is not None:
            df = st.session_state.lookup_role_df
            cols = df.columns.tolist()
            
            st.subheader("ğŸ“ é€‰æ‹©åˆ—")
            role_name_col = st.selectbox(
                "é€‰æ‹©è§’è‰²åç§°åˆ—",
                options=cols,
                index=0,
                key="lookup_role_name_col_select"
            )
            
            role_personality_col = st.selectbox(
                "é€‰æ‹©æ€§æ ¼æè¿°åˆ—",
                options=cols,
                index=min(1, len(cols)-1) if len(cols) > 1 else 0,
                key="lookup_role_personality_col_select"
            )
            
            if st.button("ğŸ“¥ åŠ è½½è§’è‰²æ€§æ ¼åº“", key="lookup_load_role_base"):
                st.session_state.lookup_role_name_col = role_name_col
                st.session_state.lookup_role_personality_col = role_personality_col
                
                # åŠ è½½åˆ°translator
                if translator.load_role_personality(df, role_name_col, role_personality_col):
                    st.session_state.lookup_role_base_loaded = True
                    st.success(f"âœ… è§’è‰²æ€§æ ¼åº“åŠ è½½æˆåŠŸï¼šå…± {len(translator.role_personality_dict)} ä¸ªè§’è‰²")
                    st.rerun()
        
        if st.session_state.lookup_role_base_loaded:
            st.metric("ğŸ“Š å·²åŠ è½½è§’è‰²æ•°", len(translator.role_personality_dict))
    
    st.markdown("---")
    
    # æ™ºèƒ½æŸ¥è¯¢åŒºåŸŸ
    st.header("ğŸ” æ™ºèƒ½æŸ¥è¯¢")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        query_input = st.text_input(
            "è¾“å…¥æŸ¥è¯¢è¯è¯­",
            placeholder="ä¾‹å¦‚ï¼šåƒã€æ°å…‹ã€ä¸è¦ã€ç¾å‘³ç­‰",
            key="lookup_query_input"
        )
    
    with col2:
        st.write("")
        st.write("")
        search_button = st.button("ğŸš€ å¼€å§‹æŸ¥è¯¢", use_container_width=True, key="lookup_search_button")
    
    match_mode = st.radio(
        "åŒ¹é…æ¨¡å¼",
        ["æ¨¡ç³ŠåŒ¹é…", "ç²¾ç¡®åŒ¹é…"],
        horizontal=True,
        key="lookup_match_mode"
    )
    
    # æ‰§è¡ŒæŸ¥è¯¢
    if search_button or (query_input and len(query_input) > 0):
        if not query_input:
            st.warning("âš ï¸ è¯·è¾“å…¥æŸ¥è¯¢è¯è¯­")
        elif not st.session_state.lookup_term_base_loaded and not st.session_state.lookup_role_base_loaded:
            st.warning("âš ï¸ è¯·å…ˆåŠ è½½æœ¯è¯­åº“æˆ–è§’è‰²æ€§æ ¼åº“")
        else:
            with st.spinner("ğŸ” æ­£åœ¨æŸ¥è¯¢..."):
                time.sleep(0.3)  # æ¨¡æ‹ŸåŠ è½½
                
                st.subheader("ğŸ“Š æŸ¥è¯¢ç»“æœ")
                
                total_matches = 0
                
                # æŸ¥è¯¢æœ¯è¯­åº“
                if st.session_state.lookup_term_base_loaded and st.session_state.lookup_term_df is not None:
                    st.markdown("### ğŸ“š æœ¯è¯­åº“åŒ¹é…ç»“æœ")
                    
                    df = st.session_state.lookup_term_df
                    source_col = st.session_state.lookup_term_source_col
                    target_cols = st.session_state.lookup_term_target_cols
                    
                    # æ”¶é›†åŒ¹é…çš„è¡Œ
                    matched_rows = []
                    for idx, row in df.iterrows():
                        source_text = row[source_col]
                        if pd.isna(source_text):
                            continue
                        
                        source_text = str(source_text)
                        
                        # æ£€æŸ¥æºåˆ—æ˜¯å¦åŒ¹é…
                        if match_mode == "æ¨¡ç³ŠåŒ¹é…":
                            match = query_input in source_text
                        else:
                            match = query_input == source_text
                        
                        # å¦‚æœæºåˆ—ä¸åŒ¹é…ï¼Œæ£€æŸ¥ç¿»è¯‘åˆ—
                        if not match:
                            for target_col in target_cols:
                                target_text = row[target_col]
                                if pd.isna(target_text) or str(target_text).strip() in ['', '-']:
                                    continue
                                target_text = str(target_text)
                                
                                if match_mode == "æ¨¡ç³ŠåŒ¹é…":
                                    if query_input in target_text:
                                        match = True
                                        break
                                else:
                                    if query_input == target_text:
                                        match = True
                                        break
                        
                        if match:
                            matched_rows.append(row)
                    
                    # åˆå¹¶ç›¸åŒè¯æ¡çš„ç¿»è¯‘
                    merged_terms = {}
                    for row in matched_rows:
                        source_text = str(row[source_col])
                        if source_text not in merged_terms:
                            merged_terms[source_text] = {col: [] for col in target_cols}
                        
                        for target_col in target_cols:
                            value = row[target_col]
                            if pd.isna(value) or str(value).strip() in ['', '-']:
                                continue
                            value = str(value)
                            if value not in merged_terms[source_text][target_col]:
                                merged_terms[source_text][target_col].append(value)
                    
                    if merged_terms:
                        st.success(f"âœ… æ‰¾åˆ° {len(merged_terms)} ä¸ªä¸åŒè¯æ¡ï¼Œå…± {len(matched_rows)} æ¡è®°å½•")
                        total_matches += len(merged_terms)
                        
                        for source_text, translations in merged_terms.items():
                            with st.container():
                                st.markdown(f"#### ğŸ”¸ {source_text}")
                                
                                cols_display = st.columns(len(target_cols))
                                for i, target_col in enumerate(target_cols):
                                    with cols_display[i]:
                                        values = translations[target_col]
                                        display_value = ', '.join(values) if values else '-'
                                        st.info(f"**{target_col}:** {display_value}")
                                
                                st.markdown("---")
                    else:
                        st.warning(f"âš ï¸ æœ¯è¯­åº“ä¸­æœªæ‰¾åˆ°åŒ…å«ã€Œ{query_input}ã€çš„è¯æ¡")
                
                # æŸ¥è¯¢è§’è‰²æ€§æ ¼åº“
                if st.session_state.lookup_role_base_loaded and st.session_state.lookup_role_df is not None:
                    st.markdown("### ğŸ‘¤ è§’è‰²æ€§æ ¼åº“åŒ¹é…ç»“æœ")
                    
                    df = st.session_state.lookup_role_df
                    role_name_col = st.session_state.lookup_role_name_col
                    role_personality_col = st.session_state.lookup_role_personality_col
                    
                    # æ”¶é›†åŒ¹é…çš„è§’è‰²
                    matched_roles = []
                    for idx, row in df.iterrows():
                        role_name = row[role_name_col]
                        if pd.isna(role_name):
                            continue
                        
                        role_name = str(role_name)
                        
                        if match_mode == "æ¨¡ç³ŠåŒ¹é…":
                            match = query_input in role_name
                        else:
                            match = query_input == role_name
                        
                        if match:
                            matched_roles.append(row)
                    
                    if matched_roles:
                        st.success(f"âœ… æ‰¾åˆ° {len(matched_roles)} ä¸ªåŒ¹é…è§’è‰²")
                        total_matches += len(matched_roles)
                        
                        for row in matched_roles:
                            role_name = str(row[role_name_col])
                            personality = str(row[role_personality_col]) if not pd.isna(row[role_personality_col]) else 'æ— æ€§æ ¼æè¿°'
                            
                            with st.container():
                                st.markdown(f"#### ğŸ‘¤ {role_name}")
                                
                                # åœ¨æœ¯è¯­åº“ä¸­æŸ¥æ‰¾è¯¥è§’è‰²åçš„ç¿»è¯‘
                                if st.session_state.lookup_term_base_loaded and st.session_state.lookup_term_df is not None:
                                    term_df = st.session_state.lookup_term_df
                                    source_col = st.session_state.lookup_term_source_col
                                    target_cols = st.session_state.lookup_term_target_cols
                                    
                                    role_term_matches = term_df[term_df[source_col] == role_name]
                                    
                                    if not role_term_matches.empty:
                                        # åˆå¹¶ç¿»è¯‘
                                        merged_translations = {col: [] for col in target_cols}
                                        
                                        for _, term_row in role_term_matches.iterrows():
                                            for target_col in target_cols:
                                                value = term_row[target_col]
                                                if pd.isna(value) or str(value).strip() in ['', '-']:
                                                    continue
                                                value = str(value)
                                                if value not in merged_translations[target_col]:
                                                    merged_translations[target_col].append(value)
                                        
                                        # æ˜¾ç¤ºç¿»è¯‘
                                        cols_display = st.columns(len(target_cols))
                                        for i, target_col in enumerate(target_cols):
                                            with cols_display[i]:
                                                values = merged_translations[target_col]
                                                display_value = ', '.join(values) if values else '-'
                                                st.info(f"**{target_col}:** {display_value}")
                                
                                # æ˜¾ç¤ºæ€§æ ¼æè¿°
                                st.success(f"**ğŸ’¬ æ€§æ ¼æè¿°:** {personality}")
                                st.markdown("---")
                    else:
                        st.warning(f"âš ï¸ è§’è‰²æ€§æ ¼åº“ä¸­æœªæ‰¾åˆ°åŒ…å«ã€Œ{query_input}ã€çš„è§’è‰²")
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                st.info(f"ğŸ” æŸ¥è¯¢è¯ï¼š**{query_input}** | åŒ¹é…æ¨¡å¼ï¼š**{match_mode}** | å…±æ‰¾åˆ°ï¼š**{total_matches}** æ¡ç»“æœ")
def excel_ABC_page():
    """Excelæ‰¹é‡å¤„ç†å·¥å…·ä¸»å‡½æ•°"""
    
    st.title("ğŸ“Š Excelæ‰¹é‡å¤„ç†å·¥å…·")
    
    # åˆå§‹åŒ–session state
    if 'excel_files' not in st.session_state:
        st.session_state.excel_files = []
    if 'dataframes' not in st.session_state:
        st.session_state.dataframes = {}
    
    # è¾…åŠ©å‡½æ•°
    def load_excel_file(file_path):
        """åŠ è½½Excelæ–‡ä»¶"""
        try:
            df = pd.read_excel(file_path)
            return df
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None
    
    def check_condition(value, keywords, match_mode):
        """æ£€æŸ¥å€¼æ˜¯å¦æ»¡è¶³å…³é”®è¯æ¡ä»¶"""
        if not keywords:
            return True
        
        value_str = str(value).lower()
        keywords_list = [kw.strip().lower() for kw in keywords if kw.strip()]
        
        if not keywords_list:
            return True
        
        if match_mode == "åŒæ—¶åŒ…å«æ‰€æœ‰å…³é”®è¯":
            return all(kw in value_str for kw in keywords_list)
        else:  # åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯
            return any(kw in value_str for kw in keywords_list)
    
    def process_dataframe(df, col1, col2, keywords, match_mode, operation, params):
        """å¤„ç†æ•°æ®æ¡†"""
        df_copy = df.copy()
        modified_count = 0
        
        for idx, row in df_copy.iterrows():
            if check_condition(row[col1], keywords, match_mode):
                if operation == "åˆ é™¤å€¼":
                    target_value = params.get('target_value', '')
                    if target_value:
                        cell_value = str(row[col2])
                        if target_value in cell_value:
                            df_copy.at[idx, col2] = cell_value.replace(target_value, '')
                            modified_count += 1
                        
                elif operation == "æ›¿æ¢å€¼":
                    old_value = params.get('old_value', '')
                    new_value = params.get('new_value', '')
                    if old_value:
                        cell_value = str(row[col2])
                        if old_value in cell_value:
                            df_copy.at[idx, col2] = cell_value.replace(old_value, new_value)
                            modified_count += 1
                        
                elif operation == "ä¿®æ”¹ä¸­é—´å€¼":
                    value_a = params.get('value_a', '')
                    value_c = params.get('value_c', '')
                    new_value = params.get('new_value', '')
                    
                    cell_value = str(row[col2])
                    if value_a and value_c and value_a in cell_value and value_c in cell_value:
                        pos_a = cell_value.find(value_a)
                        pos_c = cell_value.find(value_c, pos_a + len(value_a))
                        
                        if pos_c > pos_a:
                            before = cell_value[:pos_a + len(value_a)]
                            after = cell_value[pos_c:]
                            df_copy.at[idx, col2] = before + new_value + after
                            modified_count += 1
        
        return df_copy, modified_count
    
    # ä¸»ç•Œé¢
    # 1. æ–‡ä»¶ä¸Šä¼ 
    st.header("1ï¸âƒ£ ä¸Šä¼ æ–‡ä»¶å¤¹")
    uploaded_files = st.file_uploader(
        "é€‰æ‹©Excelæ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰", 
        type=['xlsx', 'xls'], 
        accept_multiple_files=True,
        key="excel_uploader"
    )
    
    if uploaded_files:
        st.session_state.excel_files = []
        st.session_state.dataframes = {}
        
        for uploaded_file in uploaded_files:
            temp_path = f"temp_{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            df = load_excel_file(temp_path)
            if df is not None:
                st.session_state.excel_files.append(uploaded_file.name)
                st.session_state.dataframes[uploaded_file.name] = df
            
            os.remove(temp_path)
        
        st.success(f"å·²åŠ è½½ {len(st.session_state.excel_files)} ä¸ªExcelæ–‡ä»¶")
        
        with st.expander("æŸ¥çœ‹å·²åŠ è½½çš„æ–‡ä»¶"):
            for file_name in st.session_state.excel_files:
                st.write(f"- {file_name}")
    
    # 2. å¤„ç†è®¾ç½®
    if st.session_state.excel_files:
        st.header("2ï¸âƒ£ é…ç½®å¤„ç†è§„åˆ™")
        
        # é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶æ¥é¢„è§ˆåˆ—å
        sample_file = st.session_state.excel_files[0]
        sample_df = st.session_state.dataframes[sample_file]
        columns = list(sample_df.columns)
        
        col_left, col_right = st.columns(2)
        
        with col_left:
            st.subheader("é€‰æ‹©åˆ—")
            col1 = st.selectbox("ç¬¬ä¸€åˆ—ï¼ˆæ¡ä»¶åˆ—ï¼‰", columns, key="col1")
            col2 = st.selectbox("ç¬¬äºŒåˆ—ï¼ˆæ“ä½œåˆ—ï¼‰", columns, key="col2")
        
        with col_right:
            st.subheader("æ¡ä»¶è®¾ç½®")
            keywords_input = st.text_area(
                "å…³é”®è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œç•™ç©ºåˆ™å¤„ç†æ‰€æœ‰è¡Œï¼‰",
                height=100,
                placeholder="è¾“å…¥å…³é”®è¯\nå¯è¾“å…¥å¤šä¸ª\næ¯è¡Œä¸€ä¸ª",
                key="keywords_input"
            )
            keywords = [kw.strip() for kw in keywords_input.split('\n') if kw.strip()]
            
            match_mode = st.radio(
                "åŒ¹é…æ¨¡å¼",
                ["åŒæ—¶åŒ…å«æ‰€æœ‰å…³é”®è¯", "åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯"],
                disabled=len(keywords) == 0,
                key="match_mode"
            )
        
        st.divider()
        
        # 3. æ“ä½œé€‰æ‹©
        st.subheader("é€‰æ‹©æ“ä½œ")
        operation = st.radio(
            "æ“ä½œç±»å‹",
            ["åˆ é™¤å€¼", "æ›¿æ¢å€¼", "ä¿®æ”¹ä¸­é—´å€¼"],
            key="operation"
        )
        
        params = {}
        
        if operation == "åˆ é™¤å€¼":
            st.info("ğŸ’¡ åˆ é™¤ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­åŒ…å«çš„æŒ‡å®šå†…å®¹")
            params['target_value'] = st.text_input(
                "è¦åˆ é™¤çš„å†…å®¹", 
                key="delete_value", 
                placeholder="ä¾‹å¦‚ï¼šåˆ é™¤'å¸…'ï¼Œåˆ™'æˆ‘å¥½å¸…'å˜ä¸º'æˆ‘å¥½'"
            )
            
        elif operation == "æ›¿æ¢å€¼":
            st.info("ğŸ’¡ å°†ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­çš„æŸä¸ªå†…å®¹æ›¿æ¢ä¸ºæ–°å†…å®¹")
            col_a, col_b = st.columns(2)
            with col_a:
                params['old_value'] = st.text_input(
                    "è¦æ›¿æ¢çš„å†…å®¹", 
                    key="old_value",
                    placeholder="ä¾‹å¦‚ï¼šå¸…"
                )
            with col_b:
                params['new_value'] = st.text_input(
                    "æ›¿æ¢ä¸º", 
                    key="new_value",
                    placeholder="ä¾‹å¦‚ï¼šä¸‘"
                )
                
        elif operation == "ä¿®æ”¹ä¸­é—´å€¼":
            st.info("ğŸ’¡ ä¿®æ”¹å¤¹åœ¨ä¸¤ä¸ªå€¼ä¹‹é—´çš„å†…å®¹")
            col_a, col_b, col_c = st.columns(3)
            with col_a:
                params['value_a'] = st.text_input("èµ·å§‹å€¼ A", key="value_a")
            with col_b:
                params['value_c'] = st.text_input("ç»“æŸå€¼ C", key="value_c")
            with col_c:
                params['new_value'] = st.text_input("æ–°çš„ä¸­é—´å€¼", key="middle_new_value")
        
        st.divider()
        
        # 4. é¢„è§ˆå’Œæ‰§è¡Œ
        st.header("3ï¸âƒ£ é¢„è§ˆå’Œæ‰§è¡Œ")
        
        col_preview, col_execute = st.columns(2)
        
        with col_preview:
            if st.button("ğŸ” é¢„è§ˆæ•ˆæœï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰", type="secondary", use_container_width=True):
                preview_df = st.session_state.dataframes[sample_file].copy()
                processed_df, count = process_dataframe(
                    preview_df, col1, col2, keywords, match_mode, operation, params
                )
                
                st.success(f"é¢„è§ˆå®Œæˆï¼å…±ä¿®æ”¹ {count} è¡Œæ•°æ®")
                
                col_before, col_after = st.columns(2)
                with col_before:
                    st.write("**å¤„ç†å‰**")
                    st.dataframe(preview_df[[col1, col2]].head(20), use_container_width=True)
                with col_after:
                    st.write("**å¤„ç†å**")
                    st.dataframe(processed_df[[col1, col2]].head(20), use_container_width=True)
        
        with col_execute:
            if st.button("âœ… æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶", type="primary", use_container_width=True):
                with st.spinner("æ­£åœ¨å¤„ç†..."):
                    results = {}
                    total_modified = 0
                    
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    for idx, file_name in enumerate(st.session_state.excel_files):
                        status_text.text(f"æ­£åœ¨å¤„ç†: {file_name}")
                        df = st.session_state.dataframes[file_name]
                        processed_df, count = process_dataframe(
                            df, col1, col2, keywords, match_mode, operation, params
                        )
                        results[file_name] = processed_df
                        total_modified += count
                        progress_bar.progress((idx + 1) / len(st.session_state.excel_files))
                    
                    status_text.empty()
                    progress_bar.empty()
                    
                    st.success(f"âœ… å¤„ç†å®Œæˆï¼å…±ä¿®æ”¹ {total_modified} è¡Œæ•°æ®")
                    
                    # åˆ›å»ºä¸‹è½½åŒ…
                    zip_buffer = BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for file_name, df in results.items():
                            excel_buffer = BytesIO()
                            df.to_excel(excel_buffer, index=False)
                            zip_file.writestr(f"processed_{file_name}", excel_buffer.getvalue())
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶ï¼ˆZIPï¼‰",
                        data=zip_buffer.getvalue(),
                        file_name="processed_excel_files.zip",
                        mime="application/zip",
                        use_container_width=True
                    )
    
    else:
        st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ Excelæ–‡ä»¶")
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### åŠŸèƒ½è¯´æ˜
        
        1. **ä¸Šä¼ æ–‡ä»¶**ï¼šé€‰æ‹©å¤šä¸ªExcelæ–‡ä»¶ï¼ˆæ”¯æŒ.xlsxå’Œ.xlsæ ¼å¼ï¼‰
        
        2. **é€‰æ‹©åˆ—**ï¼š
           - ç¬¬ä¸€åˆ—ï¼šæ¡ä»¶åˆ—ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ç¬¦åˆå¤„ç†æ¡ä»¶
           - ç¬¬äºŒåˆ—ï¼šæ“ä½œåˆ—ï¼Œå¯¹ç¬¦åˆæ¡ä»¶çš„è¡Œè¿›è¡Œæ“ä½œ
        
        3. **è®¾ç½®æ¡ä»¶**ï¼š
           - è¾“å…¥å…³é”®è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰
           - ç•™ç©ºè¡¨ç¤ºå¤„ç†æ‰€æœ‰è¡Œ
           - é€‰æ‹©åŒ¹é…æ¨¡å¼ï¼šåŒæ—¶åŒ…å«æ‰€æœ‰å…³é”®è¯ æˆ– åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯
        
        4. **é€‰æ‹©æ“ä½œ**ï¼š
           - **åˆ é™¤å€¼**ï¼šåˆ é™¤ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­åŒ…å«çš„æŒ‡å®šå†…å®¹ï¼ˆä¾‹å¦‚ï¼šåˆ é™¤"å¸…"ï¼Œ"æˆ‘å¥½å¸…"å˜ä¸º"æˆ‘å¥½"ï¼‰
           - **æ›¿æ¢å€¼**ï¼šå°†ç¬¬äºŒåˆ—æ–‡æœ¬ä¸­çš„æŸä¸ªå†…å®¹æ›¿æ¢ä¸ºæ–°å†…å®¹ï¼ˆä¾‹å¦‚ï¼š"å¸…"æ›¿æ¢ä¸º"ä¸‘"ï¼Œ"æˆ‘å¥½å¸…"å˜ä¸º"æˆ‘å¥½ä¸‘"ï¼‰
           - **ä¿®æ”¹ä¸­é—´å€¼**ï¼šä¿®æ”¹å¤¹åœ¨Aå’ŒCä¹‹é—´çš„Bå€¼ï¼ˆä¾‹å¦‚ï¼šA="<"ï¼ŒC=">"ï¼Œå°†"<æ—§å€¼>"æ”¹ä¸º"<æ–°å€¼>"ï¼‰
        
        5. **é¢„è§ˆå’Œæ‰§è¡Œ**ï¼š
           - å…ˆé¢„è§ˆç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å¤„ç†æ•ˆæœ
           - ç¡®è®¤æ— è¯¯åæ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶
           - ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶å‹ç¼©åŒ…
        
        ### ä½¿ç”¨ç¤ºä¾‹
        
        **ç¤ºä¾‹1ï¼šåˆ é™¤æŒ‡å®šå†…å®¹**
        - æ¡ä»¶ï¼šç¬¬ä¸€åˆ—åŒ…å«"äº§å“"
        - æ“ä½œï¼šåˆ é™¤ç¬¬äºŒåˆ—ä¸­çš„"æ—§ç‰ˆ"
        - ç»“æœï¼š"æ—§ç‰ˆäº§å“è¯´æ˜" â†’ "äº§å“è¯´æ˜"
        
        **ç¤ºä¾‹2ï¼šæ›¿æ¢å†…å®¹**
        - æ¡ä»¶ï¼šç¬¬ä¸€åˆ—åŒ…å«"è¯„ä»·"
        - æ“ä½œï¼šå°†"å¸…"æ›¿æ¢ä¸º"ä¸‘"
        - ç»“æœï¼š"è¿™ä¸ªäººå¥½å¸…" â†’ "è¿™ä¸ªäººå¥½ä¸‘"
        
        **ç¤ºä¾‹3ï¼šä¿®æ”¹ä¸­é—´å€¼**
        - æ¡ä»¶ï¼šç¬¬ä¸€åˆ—åŒ…å«"æ ‡ç­¾"
        - æ“ä½œï¼šA="ã€"ï¼ŒC="ã€‘"ï¼Œæ–°å€¼="å·²å¤„ç†"
        - ç»“æœï¼š"ã€å¾…å¤„ç†ã€‘ä»»åŠ¡" â†’ "ã€å·²å¤„ç†ã€‘ä»»åŠ¡"
        """)
    
# ä¸»ç¨‹åº
def jacky_page():
    st.header("ä½œè€…ä¸»é¡µ")
    col1,col2,col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ“– æ‰“å¼€ä½œè€…ä¸»é¡µ", use_container_width=True):
            st.markdown("[ä½œè€…ä¸»é¡µ](https://jackyjay.cn)")
        if st.button("ğŸ” æ‰“å¼€ç™¾åº¦", use_container_width=True):
            st.markdown("[ç™¾åº¦](https://www.baidu.com)")
    
    with col2:
        if st.button("ğŸ“š æ‰“å¼€GitHub", use_container_width=True):
            st.markdown("[GitHub](https://github.com)")
        if st.button("ğŸ’¬ æ‰“å¼€Stack Overflow", use_container_width=True):
            st.markdown("[Stack Overflow](https://stackoverflow.com)")
    
    with col3:
        if st.button("ğŸ“Š æ‰“å¼€Streamlitæ–‡æ¡£", use_container_width=True):
            st.markdown("[Streamlitæ–‡æ¡£](https://docs.streamlit.io)")
        if st.button("ğŸ¼ æ‰“å¼€Pandasæ–‡æ¡£", use_container_width=True):
            st.markdown("[Pandasæ–‡æ¡£](https://pandas.pydata.org/docs)")
def main():
    st.set_page_config(
        page_title="API_AI_Excelç¿»è¯‘åˆ†æå·¥å…·_Jacky",
        page_icon="ğŸ®",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ä¾§è¾¹æ é¡µé¢é€‰æ‹©
    st.sidebar.title("ğŸ® å¤šAPI Excelæ™ºèƒ½ç¿»è¯‘å·¥å…·")
    st.sidebar.markdown("---")
    
    page = st.sidebar.radio(
        "é€‰æ‹©åŠŸèƒ½é¡µé¢",
        [
            "ğŸ“ æç¤ºè¯ç”Ÿæˆå™¨",
            "ğŸ“Š ç¿»è¯‘ç»“æœå¤„ç†",
            "ğŸ”„ æ‰¹é‡ç¿»è¯‘å·¥å…·",
            "æœ¯è¯­æŸ¥æ‰¾",
            "excelæŸ¥æ‰¾æ›¿æ¢",
            "excelé«˜çº§æ›¿æ¢",
            "Jackyçš„ä¸»é¡µ",
            "ğŸ” Excelè¡¨æ ¼å¯¹æ¯”",
            "ğŸ” ExcelABCæ“ä½œ",
            "ğŸ” æŠ“å¼¹å¹•ï¼ˆåªæ”¯æŒnikoniko)",
            "blblè§†é¢‘å¼¹å¹•è¯„è®ºä¸‹è½½",
            "æ–‡ä»¶å¤¹å•å‘åŒ¹é…ç¨‹åº",
            "æ¨¡æ¿ä¸€é”®åŒ¹é…"
        ],
        index=0
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("""
    ### ğŸ“– ä½¿ç”¨è¯´æ˜
    
    **æç¤ºè¯ç”Ÿæˆå™¨ï¼š**
    - ä¸Šä¼ å¾…ç¿»è¯‘æ–‡æœ¬
    - åŠ è½½æœ¯è¯­åº“å’Œæ€§æ ¼åº“
    - ç”Ÿæˆç¿»è¯‘æç¤ºè¯
    - å¤åˆ¶ç»™AIè¿›è¡Œç¿»è¯‘
    
    **ç¿»è¯‘ç»“æœå¤„ç†ï¼š**
    - ä¸Šä¼ åŸå§‹Excelæ–‡ä»¶
    - ç²˜è´´AIç¿»è¯‘ç»“æœ
    - è‡ªåŠ¨åŒ¹é…åˆå¹¶
    - ä¸‹è½½å®Œæ•´ç»“æœ
    
    **æ‰¹é‡ç¿»è¯‘å·¥å…·ï¼š**
    - é…ç½®APIå¯†é’¥
    - ä¸Šä¼ æ–‡ä»¶å’Œæœ¯è¯­åº“
    - è‡ªåŠ¨æ‰¹é‡ç¿»è¯‘
    - æ”¯æŒé‡è¯•æœºåˆ¶
    
    ### âš™ï¸ ç‰ˆæœ¬ä¿¡æ¯
    ç‰ˆæœ¬: v2.0 åˆå¹¶ç‰ˆ
    ä½œè€…: Jacky_9S
    """)
    
    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºä¸åŒé¡µé¢
    if page == "ğŸ“ æç¤ºè¯ç”Ÿæˆå™¨":
        prompt_generator_page()
    elif page == "ğŸ“Š ç¿»è¯‘ç»“æœå¤„ç†":
        translation_result_processor_page()
    elif page == "ğŸ”„ æ‰¹é‡ç¿»è¯‘å·¥å…·":
        batch_translation_page()
    elif page == 'æœ¯è¯­æŸ¥æ‰¾':
        term_lookup_page()
    elif page == "excelæŸ¥æ‰¾æ›¿æ¢":
        excel_replace_page()
    elif page == "excelé«˜çº§æ›¿æ¢":
        excel_sreplace_page()
    elif page == "Jackyçš„ä¸»é¡µ":
        jacky_page()
    elif page == "ğŸ” Excelè¡¨æ ¼å¯¹æ¯”":  # æ–°å¢çš„é¡µé¢
        excel_comparison_page()
    elif page == "ğŸ” ExcelABCæ“ä½œ":  # æ–°å¢çš„é¡µé¢
        excel_ABC_page()
    elif page == "ğŸ” æŠ“å¼¹å¹•ï¼ˆåªæ”¯æŒnikoniko)":  # æ–°å¢çš„é¡µé¢
        danmu_page()
    elif page == "blblè§†é¢‘å¼¹å¹•è¯„è®ºä¸‹è½½":  # æ–°å¢çš„é¡µé¢
        ytdlp_downloader_app()
    elif page == "æ–‡ä»¶å¤¹å•å‘åŒ¹é…ç¨‹åº":  # æ–°å¢çš„é¡µé¢
        excel_matchpro_page()
    elif page == "æ¨¡æ¿ä¸€é”®åŒ¹é…":  # æ–°å¢çš„é¡µé¢
        grand_match()
if __name__ == "__main__":
    # ç¡®ä¿ jieba åº“å·²å®‰è£…
    try:
        import jieba
    except ImportError:
        print("jieba åº“æœªå®‰è£…ï¼Œæ­£åœ¨å°è¯•å®‰è£…...")
        os.system(f"{sys.executable} -m pip install jieba")
        import jieba
    main()
